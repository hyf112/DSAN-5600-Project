[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "COVID-19, caused by the SARS-CoV-2 virus, was first identified in December 2019 in Wuhan, China. It quickly spread globally, leading the World Health Organization (WHO) to declare it a pandemic in March 2020. The virus primarily spreads through respiratory droplets and close contact, causing symptoms like fever, cough, and difficulty breathing. Severe cases can lead to pneumonia and death, especially in older adults and those with underlying health conditions."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "Data-Sources.html",
    "href": "Data-Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The Our World in Data Coronavirus Pandemic (COVID-19) dataset is an open and comprehensive resource for data related to the COVID-19 pandemic. The dataset provides extensive data on COVID-19 cases, deaths, testing, vaccinations, and various other metrics. It covers a global scope, with data from nearly every country in the world. The information is compiled from various reputable sources, including the World Health Organization (WHO), European Centre for Disease Prevention and Control (ECDC), and numerous other national and international health agencies. The dataset is updated regularly, often daily, to provide the most current information available on the pandemic’s status across the globe.\n\n\n\nClick on the logo to access the sample data of US!"
  },
  {
    "objectID": "Data-Sources.html#the-global-terrorism-database-gtd-by-university-of-maryland",
    "href": "Data-Sources.html#the-global-terrorism-database-gtd-by-university-of-maryland",
    "title": "Data Sources",
    "section": "The Global Terrorism Database™ (GTD) by University of Maryland",
    "text": "The Global Terrorism Database™ (GTD) by University of Maryland\nAn open-source database containing information on terrorist events around the world from 1970 through 2020 (with annual updates planned for the future). Unlike many other event databases, the GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 200,000 cases.\n\n\n\nClick on the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#sipri-military-expenditure-database",
    "href": "Data-Sources.html#sipri-military-expenditure-database",
    "title": "Data Sources",
    "section": "SIPRI Military Expenditure Database",
    "text": "SIPRI Military Expenditure Database\nThe SIPRI Military Expenditure Database contains consistent time series on the military spending of countries for the period 1949–2021. The database is updated annually, which may include updates to data for any of the years included in the database. The main purpose of the data on military expenditure is to provide an easily identifiable measure of the scale of resources absorbed by the military. Military expenditure is an input measure which is not directly related to the ‘output’ of military activities, such as military capability or military security. Military expenditure data measured in constant dollars is a trend indicator of the volume of resources used for military activities, which allow comparisons to be made over time for individual countries and between countries.\n\n\n\nClick on the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#department-of-homeland-security-dhs",
    "href": "Data-Sources.html#department-of-homeland-security-dhs",
    "title": "Data Sources",
    "section": "Department of Homeland Security (DHS)",
    "text": "Department of Homeland Security (DHS)\nThe United States Department of Homeland Security (DHS) is the U.S. federal executive department responsible for public security, roughly comparable to the interior or home ministries of other countries. Its stated missions involve anti-terrorism, border security, immigration and customs, cyber security, and disaster prevention and management. Collected data comprises of the following US Visa holders from the years 1998 to 2021:\n\nTemporary Business Visitors (B-1, WB, CNMI, GMB)\nTemporary Visitors for Pleasure - Tourism/Vacation or Medical Treatment (B-2, WT, CNMI, GMT)\nStudents (F-1, F-2, M-1, M-2)\n\n\n\n\nClick on the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#quantmod-r-package",
    "href": "Data-Sources.html#quantmod-r-package",
    "title": "Data Sources",
    "section": "Quantmod R Package",
    "text": "Quantmod R Package\nThe quantmod package for R is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The getSymbols() function in the package aids in collecting stock price data of US companies and indexes. As previously mentioned, the historical stock prices of Lockheed Martin and the Dow Jones U.S. Travel & Tourism Index were chosen for this analysis.\nSee data collection code and time-series plots below:\n\nVisualizing Lockheed Martin’s and Raytheon Tech’s Stock Prices and Dow Jones U.S. Travel & Tourism Index\n::: panel-tabset #### Lockheed Martin\n\nRaytheon Technologies (formerly American Appliance Company)\n\n\nCode\n#g2 &lt;- ggplot(rtx, aes(x=date, y=RTX)) +\n\n\n\n\nDow Jones U.S. Travel & Tourism Index"
  },
  {
    "objectID": "Data-Sources.html#section-code",
    "href": "Data-Sources.html#section-code",
    "title": "Data Sources",
    "section": "Section Code",
    "text": "Section Code\nCode for this section can be found here"
  },
  {
    "objectID": "Data-Sources.html#references",
    "href": "Data-Sources.html#references",
    "title": "Data Sources",
    "section": "References",
    "text": "References\n“Download the Global Terrorism Database.” Global Terrorism Database, 2001. https://www.start.umd.edu/gtd/contact/download.\n“SIPRI Military Expenditure Database.” SIPRI MILEX. STOCKHOLM INTERNATIONAL PEACE RESEARCH INSTITUTE, 1966. https://milex.sipri.org/sipri.\n“Nonimmigrant Admissions.” Nonimmigrant Admissions | Homeland Security. Department of Homeland Security, n.d. https://www.dhs.gov/immigration-statistics/nonimmigrant."
  },
  {
    "objectID": "Data-Sources.html#unhcr",
    "href": "Data-Sources.html#unhcr",
    "title": "Data Sources",
    "section": "UNHCR",
    "text": "UNHCR\nUNHCR containing information on terrorist events around the world from 1970 through 2020 (with annual updates planned for the future). Unlike many other event databases, the GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 200,000 cases.\n\n\n\nClick on the logo to access the data!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tegveer Ghura",
    "section": "Education",
    "text": "Education\n\nBoston University | Boston, MA\n\nB.A (Cum Laude) Economics | Sept 2017 - May 2021\n\nGeorgetown University | Washington, DC\n\nM.S Data Science & Analytics | Aug 2022 - May 2024 (anticipated)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Tegveer Ghura",
    "section": "Experience",
    "text": "Experience\n\ntechCliks | Product Data Engineer Intern | May 2023 - Aug 2023\nPloomber.io | Data Science Intern | May 2023 - Aug 2023\nMetricStream, Inc. | Finance Associate | Sept 2021 - Apr 2022\nMetricStream, Inc. | Finance Intern | June 2021 - Aug 2022\nUnimoni Financial Services Ltd. | Project Intern | Jul 2019 - Aug 2019"
  },
  {
    "objectID": "index.html#summary-temporal-analysis-of-terrorism-and-tourism-in-the-united-states",
    "href": "index.html#summary-temporal-analysis-of-terrorism-and-tourism-in-the-united-states",
    "title": "Introduction",
    "section": "",
    "text": "Warning\n\n\n\nTrigger warning: The following content contains descriptions of violent acts and extremism related to terrorism, which may be disturbing or triggering for some readers.\n\n\n\n\n\n\n\n\nBrowser Viewing Experience\n\n\n\nTo get the best viewing experience of this website, it is recommended to use Safari, which has a stronger support for webGL, compared to Google Chrome.\n\n\nTerrorism is a puzzling and gripping phenomenon. Its relationship with tourism is intricate and multi-dimensional. Interestingly, international terrorism and tourism share common traits, such as being transnational in nature, involving citizens from different nations, and utilizing travel and communication technologies. The impact of terrorist attacks extends to several other industries related to tourism, including airlines, hotels, restaurants, and tourist-oriented shops and services [@baker].\nReceipts from international tourism in destinations around the world grew by 4% in 2012 reaching USD 1,075 billion. This growth is equal to a 4% increase in international tourist arrivals over the previous year which reached 1,035 million in 2012. An additional USD 219 billion was recorded in receipts from international passenger transport, bringing total exports generated by international tourism in 2012 to US$ 1.3 trillion [@unwto].\nThe Global Terrorism Database™ (GTD) [@GTD] defines a terrorist attack as the threatened or actual use of illegal force and violence by a nonstate actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation. In practice this means in order to consider an incident for inclusion in the GTD [@GTD], all three of the following attributes must be present:\n\nThe incident must be intentional – the result of a conscious calculation on the part of a perpetrator.\nThe incident must entail some level of violence or immediate threat of violence, including property violence, as well as violence against people.\nThe perpetrators of the incidents must be sub-national actors. The database does not include acts of state terrorism.\n\nIn addition, at least two of the following three criteria must be present for an incident to be included in the GTD [@GTD]:\n\nCriterion 1: The act must be aimed at attaining a political, economic, religious, or social goal. In terms of economic goals, the exclusive pursuit of profit does not satisfy this criterion. It must involve the pursuit of more profound, systemic economic change.\nCriterion 2: There must be evidence of an intention to coerce, intimidate, or convey some other message to a larger audience (or audiences) than the immediate victims. It is the act taken as a totality that is considered, irrespective if every individual involved in carrying out the act was aware of this intention. As long as any of the planners or decision-makers behind the attack intended to coerce, intimidate or publicize, the intentionality criterion is met.\nCriterion 3: The action must be outside the context of legitimate warfare activities. That is, the act must be outside the parameters permitted by international humanitarian law, insofar as it targets non-combatants"
  },
  {
    "objectID": "index.html#public-us-sentiment",
    "href": "index.html#public-us-sentiment",
    "title": "Introduction",
    "section": "Public US Sentiment",
    "text": "Public US Sentiment\nThe September 11 attacks, commonly known as 9/11, on the World Trade Center in 2001 were a historic aberration in US history, with significant and far-reaching impacts on national security policy, international relations, and the collective psyche of the American people. Immediately after the 9/11 attacks, public sentiment in the US was marked by a strong sense of shock, anger, and a desire for justice, along with a surge in patriotism and a willingness to support government actions to prevent future terrorist attacks. There was also a significant increase in concerns about national security and a greater willingness to sacrifice personal freedoms in the interest of greater security. The figure below conveys that, immediately after 9/11, a share of the US public’s stance on venturing outdoors and travelling overseas stagnated for the next decade. The public’s confidence seemed to restore around 2011."
  },
  {
    "objectID": "index.html#questions-to-adress",
    "href": "index.html#questions-to-adress",
    "title": "Introduction",
    "section": "Questions to Adress",
    "text": "Questions to Adress\n\nWhat’s the historical trend of the daily new confirmed COVID-19 cases and deaths in some main countires?\nIs there a seasonality in the daily new confirmed COVID-19 cases and deaths?\nWhat is the impact of temperature on the daily new confirmed COVID-19 cases and deaths?\nHow effective are vaccines in blocking the spread of the epidemic? What is the impact of vaccination rates on the daily new confirmed COVID-19 cases?\nHow effective are lockingdown in blocking the spread of the epidemic? What is the impact of different prevention policies on the daily new confirmed COVID-19 cases?\nWhether vaccines have a significant effect on reducing mortality?\nWhether medical resources (such as ventilators) have a significant effect on reducing mortality?\nHow has the public’s understanding of the COVID-19 changed?\nAre the public satisfied with the government’s measures and results in controlling the epidemic?\nBased on the above analysis, what measures can governments put in place to better prepare for the next possible pandemic?"
  },
  {
    "objectID": "Data-Sources.html#our-world-in-data-owid-covid-19-data",
    "href": "Data-Sources.html#our-world-in-data-owid-covid-19-data",
    "title": "Data Sources",
    "section": "",
    "text": "The Our World in Data Coronavirus Pandemic (COVID-19) dataset is an open and comprehensive resource for data related to the COVID-19 pandemic. The dataset provides extensive data on COVID-19 cases, deaths, testing, vaccinations, and various other metrics. It covers a global scope, with data from nearly every country in the world. The information is compiled from various reputable sources, including the World Health Organization (WHO), European Centre for Disease Prevention and Control (ECDC), and numerous other national and international health agencies. The dataset is updated regularly, often daily, to provide the most current information available on the pandemic’s status across the globe.\n\n\n\nClick on the logo to access the sample data of US!"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html",
    "href": "Exploratory-Data-Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In this section, I employ various time series analysis tools available in R to conduct a preliminary investigation into the daily counts of new COVID-19 cases and deaths in the United States. My exploration encompasses the decomposition of time series to discern underlying components, the creation of Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to examine correlations over time, and the application of the Augmented Dickey-Fuller Test to evaluate the stationarity of the data. Concurrently, I utilize moving average smoothing techniques with varying window spans to elucidate the long-term trends embedded within the data. This multi-faceted approach enables a comprehensive temporal analysis, revealing patterns and dynamics crucial for understanding the progression of the COVID-19."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#summary",
    "href": "Exploratory-Data-Analysis.html#summary",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In this section, I employ various time series analysis tools available in R to conduct a preliminary investigation into the daily counts of new COVID-19 cases and deaths in the United States. My exploration encompasses the decomposition of time series to discern underlying components, the creation of Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to examine correlations over time, and the application of the Augmented Dickey-Fuller Test to evaluate the stationarity of the data. Concurrently, I utilize moving average smoothing techniques with varying window spans to elucidate the long-term trends embedded within the data. This multi-faceted approach enables a comprehensive temporal analysis, revealing patterns and dynamics crucial for understanding the progression of the COVID-19."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#global-terrorism-database-exploratory-data-analysis",
    "href": "Exploratory-Data-Analysis.html#global-terrorism-database-exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Global Terrorism Database Exploratory Data Analysis",
    "text": "Global Terrorism Database Exploratory Data Analysis\n\nIdentifying Time Series Components of Monthly Attacks\nPlease note that as per the GTD Codebook [@GTD], incidents of terrorism from 1993 are not present because they were lost prior to START’s compilation of the database from multiple data collection efforts. Therefore, monthly attack counts for the year 1993 have been interpolated using the na.approx() function from the zoo library in R. Appendix II of the GTD Codebook provides Country-level statistics for 1993 and for the US, the attack count was 28. However, our interpolated estimates, which took into calculation 1992 and 1994 attack counts, sum up to 54 attacks, which shall be used for EDA.\nFrom the analysis in the previous section, I believe that the seven-day rolling average provides a better picture of the changes in daily new cases and deaths in the United States. Figures 1 and 2 are the results after the seven-day rolling average."
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html",
    "href": "ARIMAX-SARIMAX-VAR.html",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "",
    "text": "In this section, I will use the VAR model to examine the relationship between the daily number of newly confirmed COVID-19 deaths and the number of ICU patients and the number of hospital patients, and use the ARIMAX model to examine the relationship between the daily number of newly confirmed COVID-19 cases and the number of vaccinated people. relationships and try to create effective predictive models."
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html",
    "href": "ARMA-ARIMA-SARIMA.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this section, I start trying to fit and forecast Daily New Confirmed COVID-19 Cases and Deaths by using some basic time-series models, including the ARMA (Autoregressive Moving Average) model or its variants, ARIMA (Autoregressive Integrated Moving Average) and SARIMA (Seasonal Autoregressive Integrated Moving Average)."
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This research illuminates the inherent complexities and erratic nature of daily COVID-19 data, which, while being a rich resource for researchers, also poses numerous challenges and pitfalls in time series analysis. The study underscores the vast regional disparities in infection and mortality rates and the volatility in these figures, which highlight the limitations of current reporting and testing mechanisms, as well as the multitude of underlying trends and factors.\nThe application of ARMA/ARIMA models has proven potent, offering explanations for varying patterns in case and death numbers and aiding in short-term trend forecasting. This investigation also delved into more advanced models, like ARCH/GARCH, which, despite some fitting and predictive challenges, prompt further examination using more sophisticated techniques. Conversely, certain models exhibited promising predictive accuracy and stability, spurring continued refinement and application for forecasting COVID-19 trends and potentially other infectious diseases.\nAdditionally, the study explored the impact of vaccinations on new case numbers, indicating a mitigating effect on case growth. It also revealed a strong correlation between ICU admissions and new deaths, highlighting the critical role of medical resource allocation.\nNevertheless, constraints in technology and time precluded the exploration of other influential factors, such as climate and policy measures. The performance of some models requires further optimization. This research is a stepping stone, with the aspiration that future investigations will address these gaps and build upon the findings presented here.\nCOVID-19 has wrought considerable havoc, disrupting our daily lives and leading to the loss of many. However, I am confident that through collective human effort, we will ultimately prevail over this pandemic and emerge better prepared for any future global health crises."
  },
  {
    "objectID": "Deep-Learning.html",
    "href": "Deep-Learning.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "In this section, I try to create a model that can predict the number of daily new cases and deaths of COVID-19 by leveraging techniques such as RNN, LSTM, GRU, and regularization."
  },
  {
    "objectID": "Financial-Models.html",
    "href": "Financial-Models.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "In this section, I add an ARCH/GARCH model to the ARIMA model to further fit and predict the daily number of newly confirmed COVID-19 cases and deaths, and try to capture possible clustering of volatility in the data."
  },
  {
    "objectID": "Data-Visualization.html",
    "href": "Data-Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, the visualizations were generated utilizing Tableau along with the ggplot2 and Plotly packages within the R programming environment. I studied COVID-19 infections and deaths around the world, with a focus on the spread of COVID-19 in the United States over time."
  },
  {
    "objectID": "Data-Visualization.html#summary",
    "href": "Data-Visualization.html#summary",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, the visualizations were generated utilizing Tableau along with the ggplot2 and Plotly packages within the R programming environment. I studied COVID-19 infections and deaths around the world, with a focus on the spread of COVID-19 in the United States over time."
  },
  {
    "objectID": "Data-Visualization.html#visualizing-the-gtd",
    "href": "Data-Visualization.html#visualizing-the-gtd",
    "title": "Data Visualization",
    "section": "Visualizing the GTD™",
    "text": "Visualizing the GTD™\nFor the purpose of this project,\nTo start, the visualizations below provide a general overview of how the number of terrorist attacks and fatalities have changed over time."
  },
  {
    "objectID": "Data-Visualization.html#global-infection-rates",
    "href": "Data-Visualization.html#global-infection-rates",
    "title": "Data Visualization",
    "section": "Global Infection Rates",
    "text": "Global Infection Rates\nFor the purpose of this project,\nTo start, the visualizations below provide a general overview of how the number of terrorist attacks and fatalities have changed over time.\n\nGeo plot\n\nInfection\n\n\n    \n    \n         \n         \n        \n        \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n    \n\n\n\n\nDeath Rate\n\n\n    \n    \n         \n         \n        \n        \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n    \n\n\n\n\n\nBy country\nqc\n\nInfection RateDeath Rate\n\n\n\n\nCode\nplot_infection_rate &lt;- plot_ly(data = sorted_cases, y = ~location, x = ~total_cases_per_million, type = 'bar', orientation = 'h', name = 'Total Cases per Million', height = 2000, marker = list(color = 'red')) %&gt;%\n  layout(xaxis = list(title = 'Total Cases per Million'), title = 'COVID-19 Total Cases Per Million by Country')\n\nplot_infection_rate\n\n\n\n\n\n\n\n\n\n\nCode\nplot_death_rate &lt;- plot_ly(data = sorted_deaths, y = ~location, x = ~total_deaths_per_million, type = 'bar', orientation = 'h', name = 'Total Deaths per Million', height = 2000, marker = list(color = 'green')) %&gt;%\n  layout(xaxis = list(title = 'Total Deaths per Million'), title = 'COVID-19 Total Deaths Per Million by Country')\n\nplot_death_rate"
  },
  {
    "objectID": "Data-Visualization.html#new-casesdeaths",
    "href": "Data-Visualization.html#new-casesdeaths",
    "title": "Data Visualization",
    "section": "New Cases/deaths",
    "text": "New Cases/deaths"
  },
  {
    "objectID": "Data-Visualization.html#global-infection-rate",
    "href": "Data-Visualization.html#global-infection-rate",
    "title": "Data Visualization",
    "section": "Global Infection Rate",
    "text": "Global Infection Rate\nqc\n\nInfection RateDeath Rate\n\n\n\n\nCode\nplot_infection_rate &lt;- plot_ly(data = sorted_cases, y = ~location, x = ~total_cases_per_million, type = 'bar', orientation = 'h', name = 'Total Cases per Million', height = 2000, marker = list(color = 'red')) %&gt;%\n  layout(xaxis = list(title = 'Total Cases per Million'), title = 'COVID-19 Total Cases Per Million by Country')\n\nplot_infection_rate\n\n\n\n\n\n\n\n\n\n\nCode\nplot_death_rate &lt;- plot_ly(data = sorted_deaths, y = ~location, x = ~total_deaths_per_million, type = 'bar', orientation = 'h', name = 'Total Deaths per Million', height = 2000, marker = list(color = 'green')) %&gt;%\n  layout(xaxis = list(title = 'Total Deaths per Million'), title = 'COVID-19 Total Deaths Per Million by Country')\n\nplot_death_rate"
  },
  {
    "objectID": "Data-Visualization.html#line",
    "href": "Data-Visualization.html#line",
    "title": "Data Visualization",
    "section": "Line",
    "text": "Line\n\nTotal casesTotal Deaths\n\n\n\n\nCode\nplot_total_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_cases, type = 'scatter', mode = 'lines', name = 'Total Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Total COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Cases\"))\n\nplot_total_cases\n\n\n\n\n\n\n\n\n\n\nCode\nplot_total_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_deaths, type = 'scatter', mode = 'lines', name = 'Total Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Total COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Deaths\"))\n\nplot_total_deaths"
  },
  {
    "objectID": "Data-Visualization.html#al",
    "href": "Data-Visualization.html#al",
    "title": "Data Visualization",
    "section": "Al",
    "text": "Al"
  },
  {
    "objectID": "Data-Visualization.html#al-new",
    "href": "Data-Visualization.html#al-new",
    "title": "Data Visualization",
    "section": "Al New",
    "text": "Al New\n\nTotal casesTotal Deaths\n\n\n\n\nCode\nplot_new_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_cases, type = 'scatter', mode = 'lines', name = 'New Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Cases\"))\n\nplot_new_cases\n\n\n\n\n\n\n\n\n\n\nCode\nplot_new_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_deaths, type = 'scatter', mode = 'lines', name = 'New Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Deaths\"))\n\nplot_new_deaths"
  },
  {
    "objectID": "Data-Visualization.html#al-sommth",
    "href": "Data-Visualization.html#al-sommth",
    "title": "Data Visualization",
    "section": "Al sommth",
    "text": "Al sommth"
  },
  {
    "objectID": "Data-Visualization.html#global-covid-19-infection-and-death-rates",
    "href": "Data-Visualization.html#global-covid-19-infection-and-death-rates",
    "title": "Data Visualization",
    "section": "Global COVID-19 Infection and Death Rates",
    "text": "Global COVID-19 Infection and Death Rates\nI initially aimed to assess the global impact of COVID-19. Broadly, the infection and mortality rates of a pandemic virus serve as critical metrics in gauging its overall effect. To account for demographic variations across countries, I have opted to evaluate each nation’s COVID-19 infection and mortality rates based on the number of confirmed cases and deaths per million inhabitants.\n\nGlobal Map\n\nInfection Rate\n\n\n    \n    \n         \n         \n        \n        \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n    \n\n\n\n\nDeath Rate\n\n\n    \n    \n         \n         \n        \n        \n        \n        \n         \n        \n        \n        \n        \n        \n        \n        \n    \n\n\n\n\n\nBar Plot (By Country)\n\nInfection RateDeath Rate\n\n\n\n\nCode\nplot_infection_rate &lt;- plot_ly(data = sorted_cases, y = ~location, x = ~total_cases_per_million, type = 'bar', orientation = 'h', name = 'Total Cases per Million', height = 2000, marker = list(color = 'red')) %&gt;%\n  layout(xaxis = list(title = 'Total Cases per Million'), title = 'COVID-19 Total Cases Per Million by Country')\n\nplot_infection_rate\n\n\n\n\n\n\n\n\n\n\nCode\nplot_death_rate &lt;- plot_ly(data = sorted_deaths, y = ~location, x = ~total_deaths_per_million, type = 'bar', orientation = 'h', name = 'Total Deaths per Million', height = 2000, marker = list(color = 'green')) %&gt;%\n  layout(xaxis = list(title = 'Total Deaths per Million'), title = 'COVID-19 Total Deaths Per Million by Country')\n\nplot_death_rate\n\n\n\n\n\n\n\n\n\nIt’s evident that notable disparities exist in the infection and mortality rates of COVID-19 across various countries, attributable to an array of elements. These include the efficacy of public health measures and policies, the extent and accuracy of testing and reporting, the robustness of healthcare systems, demographic nuances such as the population’s age distribution, socioeconomic variables, cultural practices, vaccination uptake, and the emergence of different virus strains. Among these nations, the United States presents a particularly intriguing case. Despite its overall resources and capabilities, it has experienced higher rates of infection and fatalities. This juxtaposition positions the United States as a valuable case for in-depth analysis. Consequently, my forthcoming research will be dedicated to examining the trajectory of COVID-19 in the United States."
  },
  {
    "objectID": "Data-Visualization.html#covid-19-infection-and-death-rates-in-the-us",
    "href": "Data-Visualization.html#covid-19-infection-and-death-rates-in-the-us",
    "title": "Data Visualization",
    "section": "COVID-19 Infection and Death Rates in the US",
    "text": "COVID-19 Infection and Death Rates in the US\n\nTotal Cases and Deaths\n\nTotal casesTotal Deaths\n\n\n\n\nCode\nplot_total_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_cases, type = 'scatter', mode = 'lines', name = 'Total Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Total COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Cases\"))\n\nplot_total_cases\n\n\n\n\n\n\n\n\n\n\nCode\nplot_total_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_deaths, type = 'scatter', mode = 'lines', name = 'Total Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Total COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Deaths\"))\n\nplot_total_deaths"
  },
  {
    "objectID": "Data-Visualization.html#trajectory-of-covid-19-in-the-us",
    "href": "Data-Visualization.html#trajectory-of-covid-19-in-the-us",
    "title": "Data Visualization",
    "section": "Trajectory of COVID-19 in the US",
    "text": "Trajectory of COVID-19 in the US\n\nCumulative confirmed COVID-19 Cases and Deaths in the US\n\nTotal CasesTotal Deaths\n\n\n\n\nCode\nplot_total_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_cases, type = 'scatter', mode = 'lines', name = 'Total Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Cumulative confirmed COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Cases\"))\n\nplot_total_cases\n\n\n\n\n\n\n\n\n\n\nCode\nplot_total_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~total_deaths, type = 'scatter', mode = 'lines', name = 'Total Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Cumulative confirmed COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"Total Deaths\"))\n\nplot_total_deaths\n\n\n\n\n\n\n\n\n\nThe aggregate figures for confirmed COVID-19 cases and deaths provide a visual overview of the virus’s spread in the United States, yet they lack the granularity required for a detailed analysis, particularly when examining the progression in the growth of cases and fatalities. A closer examination of the changes in growth rates of these cases and deaths is more insightful, as it can more accurately indicate the effectiveness of various factors, such as temperature fluctuations and vaccination efforts, in curtailing the spread of COVID-19 and diminishing its mortality rate.\n\n\nDaily New Confirmed COVID-19 Cases and Deaths in the US\n\nDaily New CasesDaily New Deaths\n\n\n\n\nCode\nplot_new_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_cases, type = 'scatter', mode = 'lines', name = 'New Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Cases\"))\n\nplot_new_cases\n\n\n\n\n\n\n\n\n\n\nCode\nplot_new_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_deaths, type = 'scatter', mode = 'lines', name = 'New Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Deaths\"))\n\nplot_new_deaths\n\n\n\n\n\n\n\n\n\nI have noticed pronounced irregularities in the daily new confirmed COVID-19 cases and deaths in the United States, attributable to several factors:\n\nReporting Delays and Backlogs: Inconsistencies in the timing of reporting and processing new cases can cause substantial daily variations. For instance, when a backlog of cases is reported simultaneously, this leads to an abrupt surge, followed by a noticeable decrease in the subsequent days.\nWeekend and Holiday Reporting Patterns: Testing centers and reporting agencies often operate with reduced staff or close entirely during weekends and holidays. This typically results in fewer cases being reported on these days, followed by a marked increase when these delayed reports are processed and released, generally at the start of the week.\nVariability in Testing Rates: The number of tests conducted can fluctuate, influencing the reported case numbers. For instance, an escalation in testing efforts might result in identifying more cases, whereas constrained testing might not capture the true extent of case numbers.\n\nThese elements contribute to the irregular patterns observed in the daily COVID-19 statistics and underscore the need for looking at longer-term trends rather than focusing solely on day-to-day changes. Therefore, I use a 7-day rolling average to smooth out these irregularities and get a clearer picture of the overall trend.\n\n\nDaily New Confirmed COVID-19 Cases and Deaths (7-day rolling average) in the US\n\nDaily New Cases (7-day rolling average)Daily New Deaths (7-day rolling average)\n\n\n\n\nCode\nplot_new_cases_smoothed &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_cases_smoothed, type = 'scatter', mode = 'lines', name = '7-Day Smoothed New Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"7-Day Smoothed Daily New COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Cases (7-day rolling average)\"))\n\nplot_new_cases_smoothed\n\n\n\n\n\n\n\n\n\n\nCode\nplot_new_deaths_smoothed &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_deaths_smoothed, type = 'scatter', mode = 'lines', name = '7-Day Smoothed New Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"7-Day Smoothed Daily New COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Deaths (7-day rolling average)\"))\n\nplot_new_deaths_smoothed\n\n\n\n\n\n\n\n\n\nThe 7-day smoothed data charts for daily new COVID-19 cases and deaths in the United States reveal:\n\nCyclical Variability: The charts exhibit distinct oscillations with successive peaks and troughs, with peaks appearing in early 2021, August 2021, and early 2022, of which the peak in August 2021 should correspond to the delta variant pandemic and the peak in early 2022 corresponds to the Omikron variant pandemic. Notably, the case count surge in January 2022 markedly outstrips other periods. While the corresponding rise in deaths is also conspicuous, it does not mirror the cases’ peak with the same magnitude, which may reflect the difference between the Omicron variant and all previous variants: it is more transmissible, but does not show a higher fatality rate.\nSeasonal Trends: A seasonal trend is discernible, with infection and death rates typically intensifying during the colder months, such as January of both 2021 and 2022, possibly due to increased indoor congregation. Conversely, the months of August and September of each year also witness upticks, albeit less pronounced than the winter surges.\nTemporal Lags Between Cases and Deaths: An observable delay exists between the spikes in new cases and subsequent increases in deaths, aligning with the disease’s trajectory, which includes a period between initial infection, the emergence of severe symptoms, and any resulting fatalities. This lag indicates the time required for a rise in infections to manifest as an uptick in mortality rates.\n\nThe data captured in these charts underscore the pandemic’s intricate dynamics, potentially shaped by factors such as public health measures, behavioral shifts among the populace, and the advent of new viral strains."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#daily-new-confirmed-cases-7-day-rolling-average-in-the-us",
    "href": "Exploratory-Data-Analysis.html#daily-new-confirmed-cases-7-day-rolling-average-in-the-us",
    "title": "Exploratory Data Analysis",
    "section": "Daily New Confirmed Cases (7-day rolling average) in the US",
    "text": "Daily New Confirmed Cases (7-day rolling average) in the US\n\nIdentifying Time Series Components\n\n\nCode\nplot_new_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_cases, type = 'scatter', mode = 'lines', name = 'New Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Cases\"))\n\nplot_new_cases\n\n\n\n\n\n\nFrom the analysis in the previous section, I believe that the seven-day rolling average provides a better picture of the changes in daily new cases and deaths in the United States. Figures 1 and 2 are the results after the seven-day rolling average.\n\n\nIdentifying Time Series Components"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#daily-new-confirmed-deaths-7-day-rolling-average-in-the-us",
    "href": "Exploratory-Data-Analysis.html#daily-new-confirmed-deaths-7-day-rolling-average-in-the-us",
    "title": "Exploratory Data Analysis",
    "section": "Daily New Confirmed Deaths (7-day rolling average) in the US",
    "text": "Daily New Confirmed Deaths (7-day rolling average) in the US"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#daily-new-confirmed-cases-in-the-us",
    "href": "Exploratory-Data-Analysis.html#daily-new-confirmed-cases-in-the-us",
    "title": "Exploratory Data Analysis",
    "section": "Daily New Confirmed Cases in the US",
    "text": "Daily New Confirmed Cases in the US\n\nIdentifying Time Series Components\n\n\nCode\nplot_new_cases &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_cases, type = 'scatter', mode = 'lines', name = 'New Cases', line = list(color = 'red')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Cases in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Cases\"))\n\nplot_new_cases\n\n\n\n\n\n\nThe diagram reveals several key components in the COVID-19 case trends:\n\nTrend: Initially, from early 2020 to early 2021, there was a general upward trend in daily new cases, peaking in January 2021. This was followed by a decline, then a resurgence in July 2021, leading to a second peak in September 2021. A notable surge occurred in November 2021, culminating in the highest peak in January 2022, significantly surpassing previous peaks. Another peak was observed in July 2022.\nSeasonality: The data might suggest seasonal patterns, with new case peaks in each winter and summer. However, the impact of various factors, including public health measures and behavioral changes, may obscure a clear seasonal trend in COVID-19 cases.\nVariability: The data shows erratic changes, with substantial fluctuations in case numbers over time. This variability could stem from different statistical methods, the dynamics of disease transmission, response strategies, and the introduction of vaccines and treatments.\nCyclical: The data presents cyclical peaks that could align with different pandemic waves. While these patterns are not strong enough to be labeled as seasonal, they do suggest a recurring trend of rising and falling case numbers.\n\nIn summary, the time series graph of daily new confirmed COVID-19 cases in the United States indicates multiple waves, with the most significant surge around early 2022. It’s evident that new case numbers are not static but fluctuate, alternating between quieter periods and abrupt increases. The graph also shows that as time progresses, the peaks and troughs become more distinct, and the variance seems to grow alongside the series levels, hinting at a multiplicative relationship between these elements.\n\n\nLag Plots\n\n\nCode\nlag_plot_new_cases &lt;- gglagplot(new_cases_ts, do.lines=FALSE) +\n  ggtitle(\"Lag Plots for Daily New COVID-19 Cases in the US\") +\n  xlab(\"Lags\") + ylab(\"New Cases\") +\n  theme(axis.text.x = element_text(angle=45, hjust=1))\n\nlag_plot_new_cases\n\n\n\n\n\nThe lag plots demonstrate a positive correlation between new COVID-19 cases on a given day and those in subsequent days, up to a certain lag period. This pattern illustrates positive autocorrelation within the dataset, with the correlation strength diminishing as the lag interval increases. For instance, the lag 1 exhibits a robust positive correlation, whereas by lag 16, the dispersion of points suggests a weaker correlation. In the early stages of the lag series, some plots reveal potential cyclical patterns, hinting at periodicity within the data. Notably, at higher values, certain points significantly deviate from the main cluster, potentially representing days with abnormally high case counts. These outliers could be due to delayed reporting or aggregated data reporting spanning multiple days. The shape of the clusters in the lag plots suggests that the relationship between consecutive days’ case counts is not purely linear. This observation implies that predicting future case numbers may require a more sophisticated approach than a basic linear model. Collectively, these lag plots signal that historical data holds predictive value for future COVID-19 case numbers. Understanding the autocorrelation and potential periodicity in the data is crucial for effectively modeling the disease’s spread. This insight can significantly enhance public health forecasting and strategic planning.\n\n\nDecomposing\n\n\nCode\ndecomp_new_cases &lt;- decompose(new_cases_ts, type = 'multiplicative')\n\nautoplot(decomp_new_cases) + ggtitle(\"Multiplicative Decomposition of Daily New COVID-19 Cases\")\n\n\n\n\n\n\n\nACF and PACF Plots\n\n\nCode\n# Plot ACF\nggAcf(new_cases_ts, main=\"ACF for Daily New COVID-19 Cases\")\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(new_cases_ts, main=\"PACF for Daily New COVID-19 Cases\")\n\n\n\n\n\nThe ACF and PACF plots for daily new COVID-19 cases can provide insight into the time series data’s autocorrelation and stationarity:\n\nThe ACF plot shows a gradual decline in the autocorrelation as the lags increase, with several lags beyond the first being significant (i.e., the bars are above the blue dashed confidence interval lines). This slow decrease in autocorrelation suggests that there is a strong persistence in the data, meaning past values have a long-term influence on future values. It also suggests some seasonality might be present. This could be related to weekly patterns, monthly reporting cycles, or other seasonal\nThe PACF plot shows a significant spike at the first lag, which then crosses the confidence interval before gradually tapering off. This indicates that an AR(1) model might capture some of the autocorrelation in the series, but there may be additional complexity given the further significant lags.\n\nIn summary, the presence of significant autocorrelation at various lags in both the ACF and PACF suggests that the series is likely non-stationary and could benefit from differencing to remove trends or seasonal effects.\n\n\nADF Test\n\n\nCode\nadf.test(new_cases_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  new_cases_ts\nDickey-Fuller = -5.9797, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe result of the Augmented Dickey-Fuller (ADF) test indicates a test statistic of -5.9797 with a p-value of 0.01. Since the p-value is less than the common significance level of 0.05, I can reject the null hypothesis that the time series has a unit root (i.e., is non-stationary). This result suggests that the time series is stationary. However, this conclusion appears to be in contrast with the earlier analysis based on the ACF and PACF plots, which suggested non-stationarity due to the slow decay in the ACF and significant lags in the PACF."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#daily-new-confirmed-deaths-in-the-us",
    "href": "Exploratory-Data-Analysis.html#daily-new-confirmed-deaths-in-the-us",
    "title": "Exploratory Data Analysis",
    "section": "Daily New Confirmed Deaths in the US",
    "text": "Daily New Confirmed Deaths in the US\n\nIdentifying Time Series Components\n\n\nCode\nplot_new_deaths &lt;- plot_ly(data = owid_covid_data_us, x = ~date, y = ~new_deaths, type = 'scatter', mode = 'lines', name = 'New Deaths', line = list(color = 'green')) %&gt;%\n                    layout(title = \"Daily new confirmed COVID-19 Deaths in the US\", xaxis = list(title = \"Date\"), yaxis = list(title = \"New Deaths\"))\n\nplot_new_deaths\n\n\n\n\n\n\nThe graph provides a detailed insight into the trends of COVID-19 related deaths, highlighting several critical aspects:\n\nTrend: The graph illustrates multiple peaks that align with different pandemic waves. During these peaks, there’s a noticeable escalation in death trends, followed by a decline as the situation comes under control. The trend is dynamic, not uniformly increasing or decreasing, but showing significant spikes during the peak periods of the pandemic.\nSeasonality: The graph hints at potential seasonality, evident through periodic peaks possibly coinciding with certain times of the year. However, attributing these spikes solely to seasonality may be oversimplified, as they could also result from factors like the emergence of new virus variants, changes in social behavior, or public health policies.\nVariability: The intensity of these peaks varies, with some waves exhibiting higher death tolls than others. This variation likely reflects the distinct impacts of different pandemic waves.\nCyclical: The data’s cyclical pattern seems to mirror the fluctuations in daily new COVID-19 cases. These cycles occur irregularly, which aligns with the unpredictable nature of disease transmission and the impact of various intervention measures.\n\nIn summary, the mortality trend due to COVID-19 in the United States has seen significant fluctuations over time. Periods of stability are punctuated by sudden surges in deaths, with notable increases observed in late 2020, early 2021, and again around early 2022. The data suggests that the relationship between trend and seasonality might be multiplicative. This implies that as the death toll rises, the variability in data also increases, aligning more closely with the characteristics of a multiplicative model than a simple linear one.\n\n\nLag Plots\n\n\nCode\nlag_plot_new_deaths &lt;- gglagplot(new_deaths_ts, do.lines=FALSE) +\n  ggtitle(\"Lag Plots for Daily New COVID-19 Deaths in the US\") +\n  xlab(\"Lags\") + ylab(\"New Deaths\") +\n  theme(axis.text.x = element_text(angle=45, hjust=1))\n\nlag_plot_new_deaths\n\n\n\n\n\nThe plots for lower lags (e.g., lag 1 to lag 4) show a strong positive correlation, as indicated by the clustering of points along a line rising from the bottom left to the top right. This suggests that the number of deaths on one day is a good predictor of the number of deaths on the following days up to a certain point. As the lag increases, the points spread out more, indicating a decrease in the strength of the correlation. By lag 16, the points are much more dispersed, showing a much weaker predictive relationship. Across various lags, there are some points that stand out from the main cluster, especially at higher death counts. These may represent anomalous days with unusually high death numbers, which could be due to inconsistent reporting or batch reporting of data. The plots, particularly for the initial lags, suggest the possibility of periodicity or cycles in the data, which could correspond to the waves of the pandemic. The clusters’ shapes indicate that the relationship is not strictly linear, especially for higher lags. This non-linearity suggests that simple linear prediction models might not be adequate for forecasting future deaths based on past data. In conclusion, these lag plots demonstrate that past values can be indicative of future trends in COVID-19 death numbers, with a decreasing strength of prediction as time lag increases. The presence of outliers and potential non-linear patterns suggests that complex models may be required for accurate forecasting.\n\n\nDecomposing\n\n\nCode\ndecomp_new_deaths &lt;- decompose(new_deaths_ts, type = 'multiplicative')\n\nautoplot(decomp_new_deaths) + ggtitle(\"Multiplicative Decomposition of Daily New COVID-19 Deaths\")\n\n\n\n\n\n\n\nACF and PACF Plots\n\n\nCode\n# Plot ACF\nggAcf(new_deaths_ts, main=\"ACF for Daily New COVID-19 Deaths\")\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(new_deaths_ts, main=\"PACF for Daily New COVID-19 Deaths\")\n\n\n\n\n\nThe ACF and PACF plots for daily new COVID-19 deaths can provide insight into the time series data’s autocorrelation and stationarity:\n\nThe ACF plot shows a gradual decline in autocorrelation as the lags increase, but remains positive and above the significance bounds for many lags. The slow decay of the autocorrelation suggests that there is a long memory in the series, with past values influencing future values for an extended period. The autocorrelation at lag 1 is quite high, indicating a strong relationship between consecutive observations.\nThe PACF plot exhibits a sharp cut-off after the first lag, where the autocorrelation is significant, followed by a few more significant spikes at higher lags but with much smaller magnitudes. The significant spike at lag 1 in the PACF plot indicates that the series can be well explained by its own past values up to one lag prior.\n\nOverall, the long memory indicated by the ACF plot and the significant autocorrelation at lag 1 in the PACF plot suggest that the series has persistent patterns over time. The fact that the ACF plot shows a slow decay and does not cut off quickly suggests that the series is not stationary. A stationary time series would typically exhibit a quick drop-off in the ACF plot. The PACF plot would typically show a geometric or exponential decay for a stationary series, but here we see a significant spike at lag 1 and then a few other significant lags, which again suggests non-stationarity.\n\n\nADF Test\n\n\nCode\nadf.test(new_deaths_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  new_deaths_ts\nDickey-Fuller = -2.5452, Lag order = 10, p-value = 0.3475\nalternative hypothesis: stationary\n\n\nGiven that the p-value is much higher than 0.05, I fail to reject the null hypothesis. This implies that there is not enough statistical evidence to conclude that the time series is stationary. Therefore, the ADF test result suggests that the time series of new COVID-19 deaths is non-stationary. This result is consistent with the observations from the ACF and PACF plots."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#moving-average",
    "href": "Exploratory-Data-Analysis.html#moving-average",
    "title": "Exploratory Data Analysis",
    "section": "Moving average",
    "text": "Moving average"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#moving-average-smoothing",
    "href": "Exploratory-Data-Analysis.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\nMoving Average Smoothing of Daily New Confirmed COVID-19 Cases in the US\n\n\nCode\np1 &lt;- plot_ly() %&gt;%\n  add_lines(x = date_index, y = new_cases_ts, name = 'Original', line = list(color = 'blue')) %&gt;%\n  add_lines(x = date_index, y = ma_small, name = 'MA 7 days', line = list(color = 'red')) %&gt;%\n  add_lines(x = date_index, y = ma_medium, name = 'MA 30 days', line = list(color = 'green')) %&gt;%\n  add_lines(x = date_index, y = ma_large, name = 'MA 90 days', line = list(color = 'purple')) %&gt;%\n  layout(\n    title = \"Moving Average Smoothing of Daily New Confirmed COVID-19 Cases in the US\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"New Cases\")\n  )\n\np1\n\n\n\n\n\n\n\n\nMoving Average Smoothing of Daily New Confirmed COVID-19 Deaths in the US\n\n\nCode\np2 &lt;- plot_ly() %&gt;%\n  add_lines(x = date_index, y = new_deaths_ts, name = 'Original', line = list(color = 'blue')) %&gt;%\n  add_lines(x = date_index, y = ma_small, name = 'MA 7 days', line = list(color = 'red')) %&gt;%\n  add_lines(x = date_index, y = ma_medium, name = 'MA 30 days', line = list(color = 'green')) %&gt;%\n  add_lines(x = date_index, y = ma_large, name = 'MA 90 days', line = list(color = 'purple')) %&gt;%\n  layout(\n    title = \"Moving Average Smoothing of Daily New Confirmed COVID-19 Deaths in the US\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"New Deaths\")\n  )\n\np2\n\n\n\n\n\n\nThe plots display the original daily new confirmed COVID-19 cases/deaths in the US along with their moving averages using three different window sizes: 7 days, 30 days, and 90 days.\n\nMA 7 days (Red line): This moving average closely follows the original data (Blue bars), smoothing out some of the day-to-day variability but still reflecting weekly fluctuations. It allows us to observe short-term trends while reducing ‘noise’.\nMA 30 days (Green line): The 30-day moving average smooths out the data more than the 7-day MA, dampening the impact of short-term fluctuations and highlighting broader trends. It provides a clearer view of the overall direction of the trends over a monthly period. This moving average is less reactive to short-term spikes and drops, which can be particularly useful for understanding the general behavior of the data over a longer time frame.\nMA 90 days (Purple line): The 90-day moving average provides the smoothest curve, significantly reducing the impact of short-term spikes and showing long-term trends. It is the least reactive to daily changes and most effectively displays the overall trend direction over several months."
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#summary",
    "href": "ARMA-ARIMA-SARIMA.html#summary",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this section, I start trying to fit and forecast Daily New Confirmed COVID-19 Cases and Deaths by using some basic time-series models, including the ARMA (Autoregressive Moving Average) model or its variants, ARIMA (Autoregressive Integrated Moving Average) and SARIMA (Seasonal Autoregressive Integrated Moving Average)."
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#global-terrorism-database-arima-modeling",
    "href": "ARMA-ARIMA-SARIMA.html#global-terrorism-database-arima-modeling",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Global Terrorism Database ARIMA Modeling",
    "text": "Global Terrorism Database ARIMA Modeling"
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#arima-modeling-of-daily-new-confirmed-cases-in-the-us",
    "href": "ARMA-ARIMA-SARIMA.html#arima-modeling-of-daily-new-confirmed-cases-in-the-us",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ARIMA Modeling of Daily New Confirmed Cases in the US",
    "text": "ARIMA Modeling of Daily New Confirmed Cases in the US\n\nCheck Stationarity of first-differenced New Cases\n\nACF PlotPACF PlotADF test\n\n\n\n\nCode\n# Plot ACF\nggAcf(new_cases_ts_diff, 48, main=\"ACF for 1st-Differenced Daily New COVID-19 Cases\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(new_cases_ts_diff, 48, main=\"PACF for 1st-Differenced Daily New COVID-19 Cases\")\n\n\n\n\n\n\n\n\n\nCode\nadf.test(new_cases_ts_diff)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  new_cases_ts_diff\nDickey-Fuller = -2.3693, Lag order = 9, p-value = 0.422\nalternative hypothesis: stationary\n\n\n\n\n\nIt seems that the time series is not stationary, and it needs further differencing.\n\n\nCheck Stationarity of second-differenced New Cases\n\nACF PlotPACF PlotADF test\n\n\n\n\nCode\n# Plot ACF\nggAcf(new_cases_ts_diff2, 48, main=\"ACF for 2nd-Differenced Daily New COVID-19 Cases\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(new_cases_ts_diff2, 48, main=\"PACF for 2nd-Differenced Daily New COVID-19 Cases\")\n\n\n\n\n\n\n\n\n\nCode\nadf.test(new_cases_ts_diff2)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  new_cases_ts_diff2\nDickey-Fuller = -15.829, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nACF and PACF Plot prove that the time series has basically achieved stationarity, and ADF Test further proves this conclusion.\n\n\nFitting ARIMA(p,d,q)\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*24),nrow=24) # roughly nrow = 3x4x2\n\n\nfor (p in 1:4) # p=0,1,2,3\n{\n  for(q in 1:3) # q=0,1,2\n  {\n    for(d in 1:2) # d=1,2\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(new_cases_ts,order=c(p-1,d,q-1)) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\ntemp &lt;- temp[order(temp$BIC, decreasing = FALSE),] \nknitr::kable(temp)\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n18\n2\n2\n2\n17421.60\n17444.62\n17421.68\n\n\n24\n3\n2\n2\n17440.96\n17468.58\n17441.07\n\n\n16\n2\n2\n1\n17453.17\n17471.58\n17453.22\n\n\n22\n3\n2\n1\n17451.17\n17474.19\n17451.25\n\n\n13\n2\n1\n0\n17467.52\n17481.33\n17467.55\n\n\n21\n3\n1\n1\n17459.14\n17482.16\n17459.22\n\n\n15\n2\n1\n1\n17464.67\n17483.09\n17464.72\n\n\n19\n3\n1\n0\n17466.10\n17484.52\n17466.15\n\n\n17\n2\n1\n2\n17461.68\n17484.71\n17461.77\n\n\n23\n3\n1\n2\n17460.71\n17488.34\n17460.82\n\n\n5\n0\n1\n2\n17474.86\n17488.68\n17474.90\n\n\n11\n1\n1\n2\n17476.04\n17494.46\n17476.10\n\n\n12\n1\n2\n2\n17477.70\n17496.11\n17477.75\n\n\n9\n1\n1\n1\n17514.13\n17527.95\n17514.17\n\n\n6\n0\n2\n2\n17543.30\n17557.12\n17543.34\n\n\n3\n0\n1\n1\n17557.33\n17566.54\n17557.34\n\n\n10\n1\n2\n1\n17575.35\n17589.16\n17575.38\n\n\n4\n0\n2\n1\n17588.18\n17597.39\n17588.20\n\n\n7\n1\n1\n0\n17589.60\n17598.81\n17589.62\n\n\n1\n0\n1\n0\n17602.64\n17607.24\n17602.64\n\n\n20\n3\n2\n0\n17643.83\n17662.25\n17643.89\n\n\n14\n2\n2\n0\n17795.43\n17809.24\n17795.46\n\n\n8\n1\n2\n0\n18041.31\n18050.52\n18041.33\n\n\n2\n0\n2\n0\n18188.81\n18193.42\n18188.82\n\n\n\n\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q     AIC      BIC     AICc\n18 2 2 2 17421.6 17444.62 17421.68\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n   p d q     AIC      BIC     AICc\n18 2 2 2 17421.6 17444.62 17421.68\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q     AIC      BIC     AICc\n18 2 2 2 17421.6 17444.62 17421.68\n\n\nBased on the results, the ARIMA(2,2,2) is the best model.\n\n\nAuto.Arima\n\n\nCode\nauto.arima(new_cases_ts)\n\n\nSeries: new_cases_ts \nARIMA(3,1,4) \n\nCoefficients:\n         ar1      ar2     ar3      ma1     ma2      ma3     ma4\n      0.5066  -0.4138  0.6162  -0.8199  0.0390  -0.6119  0.6642\ns.e.  0.0474   0.0725  0.0731   0.0529  0.0966   0.0742  0.0341\n\nsigma^2 = 8.47e+08:  log likelihood = -8642.52\nAIC=17301.04   AICc=17301.24   BIC=17337.88\n\n\nThe auto.arima function in R suggests an ARIMA(3,1,4) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data.\n\n\nModel Diagnostic\n\nARIMA(2,2,2) PlotARIMA(2,2,2)ARIMA(3,1,4) PlotARIMA(3,1,4)\n\n\n\n\nCode\nmodel_output1 &lt;- capture.output(sarima(new_cases_ts,2,2,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output1[45:76], model_output1[length(model_output1)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ma1     ma2\n      0.4045  -0.3582  -1.7228  0.7543\ns.e.  0.0475   0.0433   0.0377  0.0385\n\nsigma^2 estimated as 1.025e+09:  log likelihood = -8705.8,  aic = 17421.6\n\n$degrees_of_freedom\n[1] 734\n\n$ttable\n    Estimate     SE  t.value p.value\nar1   0.4045 0.0475   8.5196       0\nar2  -0.3582 0.0433  -8.2627       0\nma1  -1.7228 0.0377 -45.7187       0\nma2   0.7543 0.0385  19.5834       0\n\n$AIC\n[1] 23.6065\n\n$AICc\n[1] 23.60657\n\n$BIC\n[1] 23.63769\n\n\n\n\n\n\nCode\nmodel_output2 &lt;- capture.output(sarima(new_cases_ts,3,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output2[47:83], model_output2[length(model_output2)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2     ar3      ma1     ma2      ma3     ma4   constant\n      0.5033  -0.4148  0.6132  -0.8180  0.0383  -0.6115  0.6630  1005.1702\ns.e.  0.0479   0.0726  0.0738   0.0532  0.0970   0.0746  0.0343   963.9153\n\nsigma^2 estimated as 837775001:  log likelihood = -8641.99,  aic = 17301.97\n\n$degrees_of_freedom\n[1] 731\n\n$ttable\n          Estimate       SE  t.value p.value\nar1         0.5033   0.0479  10.5180  0.0000\nar2        -0.4148   0.0726  -5.7122  0.0000\nar3         0.6132   0.0738   8.3139  0.0000\nma1        -0.8180   0.0532 -15.3705  0.0000\nma2         0.0383   0.0970   0.3952  0.6928\nma3        -0.6115   0.0746  -8.1988  0.0000\nma4         0.6630   0.0343  19.3297  0.0000\nconstant 1005.1702 963.9153   1.0428  0.2974\n\n$AIC\n[1] 23.41268\n\n$AICc\n[1] 23.41295\n\n$BIC\n[1] 23.46877\n\n\n\n\n\nBoth models show residuals that seem to have no trend, which is good. Both models’ ACF plots with most spikes within the confidence bounds, suggesting the residuals do not exhibit autocorrelation. The plots indicate that the residuals from both models do not perfectly follow a normal distribution, especially in the tails. However, this deviation is quite common in practice. The p-values for both models seem to be above the 0.05 threshold, suggesting that the residuals are white noise.\nConsidering all the diagnostics, the ARIMA(3,1,4) model has slightly better AIC values, which might make it preferable.\nThe equation for the model:\n\\[\\begin{equation}\n\\Delta X_t = c + 0.5033 \\Delta X_{t-1} - 0.4148 \\Delta X_{t-2} + 0.6132 \\Delta X_{t-3} - 0.8180 \\varepsilon_{t-1} + 0.0383 \\varepsilon_{t-2} - 0.6115 \\varepsilon_{t-3} + 0.6630 \\varepsilon_{t-4} + \\varepsilon_t\n\\end{equation}\\]\n\n\nForecasting\n\n\nCode\nsarima.for(new_cases_ts, 60, 3,1,4, main='Daily New Confirmed Cases in the US Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 11) \nEnd = c(2022, 70) \nFrequency = 365 \n [1] 861596.4 927773.1 697486.0 721165.9 869485.4 793404.1 708409.3 788436.5\n [9] 817618.4 747292.8 749164.3 797470.9 778184.9 749888.0 773566.6 785695.6\n[17] 764927.0 764262.1 780279.5 776181.7 767367.5 774752.4 779912.6 774341.6\n[25] 774225.4 779941.8 779750.8 777512.2 780269.7 782769.0 781810.2 782281.6\n[33] 784749.0 785507.2 785454.3 786926.0 788453.4 788879.1 789662.0 791116.0\n[41] 792083.8 792747.9 793871.9 795055.6 795892.1 796811.3 797952.6 798958.5\n[49] 799854.8 800888.4 801953.4 802910.3 803883.6 804929.6 805938.8 806909.6\n[57] 807920.8 808945.8 809937.3 810931.2\n\n$se\nTime Series:\nStart = c(2022, 11) \nEnd = c(2022, 70) \nFrequency = 365 \n [1]  28944.34  35088.17  35357.12  35359.25  39868.32  44445.05  45634.42\n [8]  48431.48  53955.29  57355.29  60035.77  64401.16  68786.37  71990.65\n[15]  75521.64  79666.40  83257.17  86513.91  90114.71  93690.41  96898.34\n[22] 100106.68 103423.68 106563.36 109555.03 112585.36 115575.96 118429.93\n[29] 121238.94 124042.42 126768.39 129419.09 132047.45 134637.99 137163.50\n[36] 139647.95 142105.55 144518.33 146886.01 149224.33 151531.07 153798.93\n[43] 156034.71 158243.70 160421.80 162569.05 164690.50 166786.46 168855.26\n[50] 170899.15 172920.30 174918.06 176892.75 178846.24 180779.10 182691.21\n[57] 184583.53 186457.05 188311.96 190148.62\n\n\nThe ARIMA(3,1,4) model seems to predict a trend of stable or slightly increasing cases in the immediate future. The actual data shows considerable variability, with what appears to be periodic spikes. It doesn’t appear that the model has captured these periodic spikes in the forecast, possibly because these fluctuations may not be predictable using past data alone or may require a more complex seasonal model.\n\n\nComparing Benchmark Methods\n\n\nCode\nfit1 &lt;- Arima(new_cases_ts, order=c(3,1,4))\nautoplot(new_cases_ts) +\n  autolayer(meanf(new_cases_ts, h=60), series=\"Mean\", PI=FALSE) +\n  autolayer(naive(new_cases_ts, h=60), series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(new_cases_ts, h=60), series=\"SNaïve\", PI=FALSE) +\n  autolayer(rwf(new_cases_ts, h=60, drift=TRUE), series=\"Drift\", PI=FALSE) +\n  autolayer(forecast(fit1,60), series=\"ARIMA(3,1,4)\", PI=FALSE) +\n  ggtitle(\"Forecasting ARIMA(3,1,4) and Benchmark Methods\")\n\n\n\n\n\nFrom the graph, only the predictions of the SNaïve baseline method appear to be more reasonable compared to the predictions of the ARIMA(3,1,4) model. Forecasts generated from the SNaïve benchmark have the greatest fluctuations or seasonality in the larger range of new cases. However, these indicators paint a volatile downward trend in the number of new cases. The ARIMA(3,1,4) model predicts that the number of new cases will first decrease and then stabilize. For shorter forecast horizons, SNaïve models may perform better than ARIMA models, while for longer forecast horizons, ARIMA models may perform better. This is because the SNaïve model assumes that the future values of the time series will be the same as the past values at the same time of year, which may be a reasonable assumption for shorter forecast horizons, but not for longer forecast horizons."
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#arima-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "href": "ARMA-ARIMA-SARIMA.html#arima-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ARIMA Modeling of Daily New Confirmed Deaths in the US",
    "text": "ARIMA Modeling of Daily New Confirmed Deaths in the US\n\nCheck Stationarity of first-differenced New Deaths\n\nACF PlotPACF PlotADF test\n\n\n\n\nCode\n# Plot ACF\nggAcf(new_deaths_ts_diff, lag = 48, main=\"ACF for 1st-Differenced Daily New COVID-19 Deaths\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(new_deaths_ts_diff, lag = 48, main=\"PACF for 1st-Differenced Daily New COVID-19 Deaths\")\n\n\n\n\n\n\n\n\n\nCode\nadf.test(new_deaths_ts_diff)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  new_deaths_ts_diff\nDickey-Fuller = -9.1862, Lag order = 9, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nACF and PACF Plot prove that the time series is almost stationary, and ADF Test further proves this conclusion.\n\n\nFitting ARIMA(p,d,q)\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*32),nrow=32) # roughly nrow = 5x6x1\n\n\nfor (p in 1:7) # p=0,1,2,3,4,5,6\n{\n  for(q in 1:6) # q=0,1,2,3,4,5\n  {\n    for(d in 1) # d=1\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(new_deaths_ts,order=c(p-1,d,q-1)) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\ntemp &lt;- temp[order(temp$BIC, decreasing = FALSE),] \nknitr::kable(temp)\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n30\n5\n1\n2\n10582.26\n10619.11\n10582.46\n\n\n27\n4\n1\n3\n10593.72\n10630.56\n10593.92\n\n\n32\n6\n1\n1\n10605.04\n10641.88\n10605.24\n\n\n31\n6\n1\n0\n10610.07\n10642.31\n10610.23\n\n\n26\n4\n1\n2\n10618.72\n10650.95\n10618.87\n\n\n23\n3\n1\n4\n10627.88\n10664.72\n10628.07\n\n\n17\n2\n1\n4\n10634.99\n10667.23\n10635.14\n\n\n22\n3\n1\n3\n10637.71\n10669.95\n10637.87\n\n\n29\n5\n1\n1\n10642.17\n10674.41\n10642.33\n\n\n28\n5\n1\n0\n10679.09\n10706.72\n10679.20\n\n\n21\n3\n1\n2\n10733.05\n10760.68\n10733.17\n\n\n15\n2\n1\n2\n10759.49\n10782.52\n10759.57\n\n\n25\n4\n1\n1\n10778.09\n10805.72\n10778.20\n\n\n18\n2\n1\n5\n10786.03\n10822.87\n10786.22\n\n\n11\n1\n1\n4\n10816.27\n10843.90\n10816.38\n\n\n12\n1\n1\n5\n10817.06\n10849.30\n10817.21\n\n\n6\n0\n1\n5\n10827.60\n10855.24\n10827.72\n\n\n20\n3\n1\n1\n10842.16\n10865.19\n10842.24\n\n\n5\n0\n1\n4\n10860.54\n10883.57\n10860.62\n\n\n14\n2\n1\n1\n10875.01\n10893.43\n10875.06\n\n\n10\n1\n1\n3\n10900.19\n10923.22\n10900.28\n\n\n4\n0\n1\n3\n10905.84\n10924.26\n10905.90\n\n\n16\n2\n1\n3\n10899.01\n10926.64\n10899.13\n\n\n9\n1\n1\n2\n10913.98\n10932.40\n10914.03\n\n\n3\n0\n1\n2\n10921.08\n10934.89\n10921.11\n\n\n24\n4\n1\n0\n10932.28\n10955.30\n10932.36\n\n\n8\n1\n1\n1\n10959.13\n10972.95\n10959.17\n\n\n19\n3\n1\n0\n11002.82\n11021.24\n11002.87\n\n\n13\n2\n1\n0\n11042.55\n11056.37\n11042.59\n\n\n1\n0\n1\n0\n11085.49\n11090.10\n11085.50\n\n\n2\n0\n1\n1\n11086.60\n11095.81\n11086.61\n\n\n7\n1\n1\n0\n11087.05\n11096.27\n11087.07\n\n\n\n\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n30 5 1 2 10582.26 10619.11 10582.46\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n30 5 1 2 10582.26 10619.11 10582.46\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n30 5 1 2 10582.26 10619.11 10582.46\n\n\nBased on the results, the ARIMA(5,1,2) is the best model.\n\n\nAuto.Arima\n\n\nCode\nauto.arima(new_deaths_ts)\n\n\nSeries: new_deaths_ts \nARIMA(3,1,2) \n\nCoefficients:\n         ar1      ar2      ar3      ma1     ma2\n      0.9251  -0.5823  -0.3006  -1.3064  0.8341\ns.e.  0.0393   0.0471   0.0377   0.0194  0.0435\n\nsigma^2 = 117294:  log likelihood = -5360.53\nAIC=10733.05   AICc=10733.17   BIC=10760.68\n\n\nThe auto.arima function in R suggests an ARIMA(3,1,2) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data.\n\n\nModel Diagnostic\n\nARIMA(5,1,2) PlotARIMA(5,1,2)ARIMA(3,1,2) PlotARIMA(3,1,2)\n\n\n\n\nCode\nmodel_output3 &lt;- capture.output(sarima(new_deaths_ts,5,1,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output3[51:86], model_output3[length(model_output3)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3      ar4      ar5      ma1     ma2  constant\n      0.5145  -0.7522  -0.1719  -0.3084  -0.2779  -1.0594  0.7524    2.4559\ns.e.  0.0598   0.0425   0.0506   0.0396   0.0454   0.0501  0.0510    3.9293\n\nsigma^2 estimated as 94281:  log likelihood = -5282.94,  aic = 10583.87\n\n$degrees_of_freedom\n[1] 731\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.5145 0.0598   8.6098  0.0000\nar2       -0.7522 0.0425 -17.6905  0.0000\nar3       -0.1719 0.0506  -3.3979  0.0007\nar4       -0.3084 0.0396  -7.7939  0.0000\nar5       -0.2779 0.0454  -6.1255  0.0000\nma1       -1.0594 0.0501 -21.1268  0.0000\nma2        0.7524 0.0510  14.7517  0.0000\nconstant   2.4559 3.9293   0.6250  0.5321\n\n$AIC\n[1] 14.32188\n\n$AICc\n[1] 14.32215\n\n$BIC\n[1] 14.37797\n\n\n\n\n\n\nCode\nmodel_output4 &lt;- capture.output(sarima(new_deaths_ts,3,1,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output4[62:95], model_output4[length(model_output4)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3      ma1     ma2  constant\n      0.9250  -0.5821  -0.3006  -1.3064  0.8338    2.1160\ns.e.  0.0393   0.0471   0.0377   0.0194  0.0440    6.9182\n\nsigma^2 estimated as 116486:  log likelihood = -5360.48,  aic = 10734.96\n\n$degrees_of_freedom\n[1] 733\n\n$ttable\n         Estimate     SE  t.value p.value\nar1        0.9250 0.0393  23.5524  0.0000\nar2       -0.5821 0.0471 -12.3571  0.0000\nar3       -0.3006 0.0377  -7.9825  0.0000\nma1       -1.3064 0.0194 -67.4476  0.0000\nma2        0.8338 0.0440  18.9587  0.0000\nconstant   2.1160 6.9182   0.3059  0.7598\n\n$AIC\n[1] 14.52633\n\n$AICc\n[1] 14.52649\n\n$BIC\n[1] 14.56995\n\n\n\n\n\nBoth models seem to produce residuals that do not show any obvious patterns or trends. The ACF plots for both models show that most of the autocorrelations for the residuals are within the confidence bounds, indicating that there is no significant autocorrelation left in the residuals. The Q-Q plots for both models indicate that the residuals are not perfectly normally distributed, as the points in the tails do not lie on the line. The p-value plots for both models show that the p-values are generally above the significance level (e.g., 0.05), suggesting that there is no significant autocorrelation in the residuals at various lags. The ARIMA(5,1,2) model has slightly lower values of AICc and BIC compared to the ARIMA(3,1,2), suggesting it may be a better model in terms of information criteria.\nBased on this comparison, the ARIMA(5,1,2) model has a slight edge due to its lower information criteria values, indicating a better fit.\nThe equation for the model:\n\\[\\begin{equation}\n\\Delta X_t = 2.4559 + 0.5145 \\Delta X_{t-1} - 0.7522 \\Delta X_{t-2} - 0.1719 \\Delta X_{t-3} - 0.3084 \\Delta X_{t-4} - 0.2779 \\Delta X_{t-5} - 1.0594 \\varepsilon_{t-1} + 0.7524 \\varepsilon_{t-2} + \\varepsilon_t\n\\end{equation}\\]\n\n\nForecasting\n\n\nCode\nsarima.for(new_deaths_ts, 60, 5,1,2, main='Daily New Confirmed Deaths in the US Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2022, 11) \nEnd = c(2022, 70) \nFrequency = 365 \n [1] 1817.378 2533.310 2482.004 2225.955 1717.000 1129.880 1076.396 1676.108\n [9] 2358.844 2595.624 2285.371 1665.098 1166.319 1171.729 1691.089 2322.385\n[17] 2586.706 2300.419 1689.015 1210.230 1220.939 1711.417 2311.035 2571.239\n[25] 2304.436 1719.035 1257.475 1264.212 1730.356 2304.046 2557.359 2307.133\n[33] 1748.510 1304.202 1306.153 1749.059 2298.561 2544.969 2310.062 1777.167\n[41] 1349.679 1347.150 1767.871 2294.246 2533.875 2313.396 1805.086 1393.857\n[49] 1387.239 1786.834 2291.029 2523.994 2317.143 1832.323 1436.784 1426.448\n[57] 1805.938 2288.859 2515.272 2321.295\n\n$se\nTime Series:\nStart = c(2022, 11) \nEnd = c(2022, 70) \nFrequency = 365 \n [1] 307.0520 337.3582 341.6075 351.4300 364.3356 369.6587 383.7552 416.5040\n [9] 444.4938 457.2325 463.2090 467.7570 474.4112 488.2983 510.4862 531.5371\n[17] 543.7563 549.1946 552.7645 558.5675 570.7212 589.1581 606.8158 617.6566\n[25] 622.7088 626.0103 631.2296 641.9146 657.9523 673.4421 683.2216 687.9733\n[33] 691.1358 695.9627 705.5463 719.8128 733.7062 742.6824 747.2089 750.2808\n[41] 754.8203 763.5571 776.4456 789.0872 797.4299 801.7837 804.7930 809.1103\n[49] 817.1723 828.9508 840.5750 848.4005 852.6175 855.5824 859.7220 867.2304\n[57] 878.0912 888.8675 896.2598 900.3652\n\n\nThe forecast shows a volatile trend with a lot of ups and downs, which suggests that the model expects the number of deaths to continue fluctuating rather than settling into a steady trend. The forecast seems to suggest a repeating pattern or cyclical behavior in the near future. This could imply that the model has detected some seasonal patterns in the historical data, or it may be a sign that the model is overfitting to It’s crucial to consider external factors that could affect the actual outcomes, such as new variants of the virus, changes in public health policy, vaccine distribution, and public behavior. These factors are not accounted for in the model but can significantly influence the actual number of deaths.\n\n\nComparing Benchmark Methods\n\n\nCode\nfit2 &lt;- Arima(new_deaths_ts, order=c(5,1,2))\nautoplot(new_deaths_ts) +\n  autolayer(meanf(new_deaths_ts, h=60), series=\"Mean\", PI=FALSE) +\n  autolayer(naive(new_deaths_ts, h=60), series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(new_deaths_ts, h=60), series=\"SNaïve\", PI=FALSE) +\n  autolayer(rwf(new_deaths_ts, h=60, drift=TRUE), series=\"Drift\", PI=FALSE) +\n  autolayer(forecast(fit2,60), series=\"ARIMA(5,1,2)\", PI=FALSE) +\n  ggtitle(\"Forecasting ARIMA(5,1,2) and Benchmark Methods\")\n\n\n\n\n\nSimilar to the previous analysis, only the predictions of the SNaïve benchmark method appear to be more reasonable compared to the predictions of the ARIMA(5,1,2) model. The ARIMA(5,1,2) model predicts that the number of new deaths will fluctuate within a certain range. And forecasts generated from the SNaïve benchmark show even stronger volatility or seasonality. Overall, these forecasts reveal a strong cyclicality in new deaths."
  },
  {
    "objectID": "index.html#flowchart",
    "href": "index.html#flowchart",
    "title": "Introduction",
    "section": "Flowchart",
    "text": "Flowchart\n\n\n\n\nflowchart TB\n    COVID19[COVID-19] --&gt; Infection\n    COVID19 --&gt; Virulence\n    \n    Infection --&gt; TotalCases[Total cases]\n    Infection --&gt; CasesPerMillion[Total cases per million]\n    Infection --&gt; NewCases[New cases]\n    \n    Virulence --&gt; TotalDeaths[Total deaths]\n    Virulence --&gt; DeathsPerMillion[Total deaths per million]\n    Virulence --&gt; NewDeaths[New deaths]\n    \n    NewCases --&gt; Vaccination\n    NewCases --&gt; Temperature\n    NewCases --&gt; Stocks\n    NewCases --&gt; Policy\n    \n    NewDeaths --&gt; Vaccination\n    NewDeaths --&gt; Temperature\n    NewDeaths --&gt; Stocks\n    NewDeaths --&gt; Policy\n    NewDeaths --&gt; HospitalBed[Hospital bed]\n    NewDeaths --&gt; ICU\n    \n    Vaccination --&gt; Suggestion\n    Temperature --&gt; Suggestion\n    Stocks --&gt; Suggestion\n    Policy --&gt; Suggestion\n    HospitalBed --&gt; Suggestion\n    ICU --&gt; Suggestion"
  },
  {
    "objectID": "index.html#data-science-question",
    "href": "index.html#data-science-question",
    "title": "Introduction",
    "section": "Data Science Question",
    "text": "Data Science Question\nThe impact of COVID-19 has been profound and multi-faceted, affecting nearly every aspect of life globally. Two of the most significant impacts have been on human life and the global economy.\nCOVID-19 has resulted in a tragic loss of life worldwide. Globally, as of 3:52pm CET, 30 November 2023, there have been 772,052,752 confirmed cases of COVID-19, including 6,985,278 deaths, reported to WHO. This loss has not only been a human tragedy but has also had psychological and social repercussions, affecting the mental health of communities and changing the way people mourn and grieve.\n\nThe pandemic has had a devastating impact on the global economy. To control the spread of the virus, many countries implemented lockdowns and social distancing measures, which led to a dramatic slowdown in economic activities. Key sectors such as travel, hospitality, and retail were particularly hard hit. The economic disruption caused widespread job losses, business closures, and financial strain for individuals and families. Governments around the world have had to inject substantial fiscal stimulus to support their economies, leading to increased national debts.\nOverall, the COVID-19 pandemic has been a defining global crisis of the early 21st century, with its impacts likely to be felt for many years to come. Consequently, I believe that a thorough analysis of the factors affecting COVID-19’s transmission and mortality is imperative. Such an investigation will enhance our comprehension of, and preparedness for, any future global pandemics. By understanding the variables that influence the spread and lethality of such diseases, we can develop more effective strategies to curb their proliferation and minimize fatalities. This proactive approach is vital to safeguard public health and preserve lives."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Introduction",
    "section": "Literature Review",
    "text": "Literature Review\n\nPopulation Density and the Transmission of COVID-19\nThe COVID-19 pandemic has prompted extensive research into factors influencing its spread. A significant area of focus has been the role of population density.\nSy et al. (2021) conducted a comprehensive study across U.S. counties, revealing a strong correlation between population density and the basic reproductive number (R0) of COVID-19. They found that areas with higher population densities experienced greater transmission rates, likely due to increased interpersonal contact. The study identified a critical population density threshold necessary to sustain an outbreak, emphasizing the significance of population density in virus transmission dynamics (Sy et al., 2021).\nIn a study focusing on Turkey, Coşkun et al. (2021) explored the combined effect of population density and wind on COVID-19 spread. They concluded that these factors accounted for a significant portion of the variance in virus transmission. The study highlighted that wind, by increasing air circulation, could exacerbate the spread in denser areas, thus underlining the complex interplay between environmental factors and population density in the pandemic’s trajectory (Coşkun et al., 2021).\nYin et al. (2021) investigated the association between population density and COVID-19 infection rates in both China and the USA. Their findings underscored a positive correlation, particularly in regions with severe outbreaks. The study supported the efficacy of social distancing and travel restrictions, pointing out the critical role of population density in managing the spread of the virus (Yin et al., 2021).\nLastly, Wong and Li (2020) presented a study demonstrating that population density was an effective predictor of cumulative infection cases in U.S. counties. Their research incorporated additional demographic variables, such as the percentages of African Americans and Hispanic-Latinas, finding that while these factors influenced infection rates, the impact of population density remained consistently significant. This study highlighted the necessity of including population density in predictive models for COVID-19 spread (Wong & Li, 2020).\nIn conclusion, these studies collectively underscore the crucial role of population density in the transmission dynamics of COVID-19.\n\n\nVaccination and the Infection/Death Rates of COVID-19\nThe onset of the COVID-19 pandemic has prompted unprecedented global efforts in vaccine development and distribution at the same time. As vaccination campaigns roll out worldwide, it becomes crucial to evaluate their impact on reducing infection and mortality rates.\nA study by the BMJ (2023) conducted a comprehensive observational study to assess the public health impact of COVID-19 vaccines across counties in the United States. Using data from December 2020 to December 2021, the study utilized generalized linear mixed models to explore the association between vaccination coverage and the rates of COVID-19 cases and deaths. The study’s findings indicate a significant reduction in COVID-19 cases and deaths correlating with increased vaccination coverage, even when accounting for social vulnerability and community mobility (The BMJ, 2023).\nSimilarly, another study published in Scientific Reports (2023) analyzed the impact of COVID-19 vaccination on the pandemic’s trajectory in various U.S. states. This study focused on the average treatment effect of vaccination on the growth rates of total cases and hospitalizations. It found that COVID-19 vaccines have significantly slowed the pandemic, with a notable reduction in the number of cases and hospitalizations. This study also explored potential biases and the heterogeneous effects of vaccination across different states, finding no significant differences based on demographic or political factors (Scientific Reports, 2023).\nExpanding the scope to a global perspective, a study indexed in PubMed (2023) analyzed the effect of COVID-19 vaccination on daily cases and deaths worldwide. The study demonstrated that increased vaccination rates are associated with a decrease in new COVID-19 cases and deaths globally. However, it also highlighted the challenges of unequal vaccine distribution across countries, emphasizing the need for fair and accelerated distribution to combat the pandemic effectively (PubMed, 2023).\nCollectively, these studies offer robust evidence of the positive impact of COVID-19 vaccination campaigns on reducing infection and mortality rates."
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-cases-in-the-us",
    "href": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-cases-in-the-us",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA Modeling of Daily New Confirmed Cases in the US",
    "text": "SARIMA Modeling of Daily New Confirmed Cases in the US"
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "href": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA Modeling of Daily New Confirmed Deaths in the US",
    "text": "SARIMA Modeling of Daily New Confirmed Deaths in the US\nIn the preceding analysis, the ACF plot for daily new COVID-19 cases did not exhibit clear cyclical patterns. However, the ACF plot for daily new COVID-19 deaths demonstrated pronounced cyclical behavior, with significant peaks at lags that are multiples of 7. This suggests the presence of weekly seasonality in the data concerning COVID-19 related deaths.\n\nACF and PACF Plots of Seasonality and first-differenced New Deaths\n\nACF PlotPACF Plot\n\n\n\n\nCode\n# Plot ACF\nggAcf(weekly_new_deaths_ts_diff, 48, main=\"ACF for seasonally&1st-Differenced Daily New COVID-19 Deaths\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(weekly_new_deaths_ts_diff, 48, main=\"PACF for seasonally&1st-Differenced Daily New COVID-19 Deaths\")\n\n\n\n\n\n\n\n\nFrom the seasonal differencing and ordinary differencing (together) ACF and PACF plots, the following combinations for p,d,q,P,D,Q are:\n\nq values obtained from ACF = 0,1;\nQ values obtained from ACF = 1;\np values obtained from PACF = 0,1,2,3,4,5;\nP values obtained from PACF = 1,2;\nd (Difference) = 1;\nD (Seasonal Difference) = 1\n\n\n\nFitting SARIMA(p,d,q)(P,D,Q)\n\n\nCode\n# Fit a simple model (like ARIMA) and then perform Ljung-Box test on residuals\nfit &lt;- auto.arima(new_deaths_ts)\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,1,2)\nQ* = 1808.3, df = 143, p-value &lt; 2.2e-16\n\nModel df: 5.   Total lags used: 148"
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-casesdeaths-in-the-us",
    "href": "ARMA-ARIMA-SARIMA.html#sarima-modeling-of-daily-new-confirmed-casesdeaths-in-the-us",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA Modeling of Daily New Confirmed Cases/Deaths in the US",
    "text": "SARIMA Modeling of Daily New Confirmed Cases/Deaths in the US\nIn the previous analysis, the ACF plots of daily new COVID-19 cases and deaths did not exhibit clear cyclical patterns. This suggests that using the SARIMA model may not be a suitable choice."
  },
  {
    "objectID": "Financial-Models.html#liter-review",
    "href": "Financial-Models.html#liter-review",
    "title": "Financial Time Series Models",
    "section": "Liter Review",
    "text": "Liter Review\nARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are commonly used to model financial time series data where there is volatility clustering—periods of swings followed by periods of relative calm. These models are well-suited to series with non-constant variance (heteroskedasticity), which is often observed in financial markets. For epidemiological data like daily new COVID-19 cases or deaths, it’s possible that similar patterns of volatility could exist. For example, there might be periods of rapid increases or decreases in case numbers due to external factors like new variants, policy changes, or behavioral shifts. If such patterns are present and result in volatility clustering, then using ARCH or GARCH models could be appropriate."
  },
  {
    "objectID": "Financial-Models.html#arimagarch-modeling-of-daily-new-confirmed-cases-in-the-us",
    "href": "Financial-Models.html#arimagarch-modeling-of-daily-new-confirmed-cases-in-the-us",
    "title": "Financial Time Series Models",
    "section": "ARIMA+GARCH Modeling of Daily New Confirmed Cases in the US",
    "text": "ARIMA+GARCH Modeling of Daily New Confirmed Cases in the US\n\nFitting ARIMA\nAccording to previous analysis, ARIMA(3,1,4) is the best fit model for Daily New Confirmed Cases in the US.\n\n\nCode\n# Fit chosen ARIMA model\nfit1 &lt;- Arima(new_cases_ts, order=c(3,1,4))\n\n# Extract resids and its square\nresids1 &lt;- fit1$residuals\nsquare_resids1 &lt;- resids1^2\n\n\n\n\nDetermining Need for Additional Model\n\nACF PlotPACF PlotArch test\n\n\n\n\nCode\n# Plot ACF\nggAcf(square_resids1, 48, main=\"ACF for Squared Residuals\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(square_resids1, 48, main=\"PACF for Squared Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nArchTest(resids1)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  resids1\nChi-squared = 528.8, df = 12, p-value &lt; 2.2e-16\n\n\n\n\n\nI observe significant spikes in the ACF of squared residuals that are outside the confidence bands, and these spikes taper off as the lags increase, this would suggest volatility clustering and potential ARCH effects. The PACF plot also shows significant spikes at the first few lags, it suggests that a GARCH model is more appropriate than an ARCH model.\nThe result of Arch Test indicates that there is very strong evidence of ARCH effects in the time series residuals. This means that the volatility of the time series data is clustering, and the variance of the residuals is not constant over time but rather depends on the past values of the squared residuals. This further proves my conclusion before.\nGiven this result, an ARCH or GARCH model may be appropriate to model the time-varying volatility in the data. These models are capable of capturing the observed volatility clustering in time series data, which standard ARIMA models cannot do.\n\n\nFitting Garch(p,q) to residuals\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:3) {\n  for (q in 1:7) {\n  \nmodel[[cc]] &lt;- garch(resids1,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) \n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]] ## model with lowest AIC is the best and output model summary\n\n\n\nCall:\ngarch(x = resids1, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         a3         b1         b2         b3  \n4.189e+08  8.722e-02  1.187e-01  1.039e-01  8.217e-03  1.970e-02  1.747e-02  \n       b4         b5         b6         b7  \n1.644e-02  2.845e-03  3.612e-03  1.489e-06  \n\n\nAlthough GARCH(3,7) seems to have a smaller AIC, the model is obviously overfitted and leads to numerical instability, which is not the result I want. Therefore, consider a more parsimonious model (fewer parameters) that may perform better.\n\n\nCode\nsummary(garchFit(~garch(2,1), resids1, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = resids1, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n&lt;environment: 0x7f9a66488e48&gt;\n [data = resids1]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2      beta1  \n  1.08129  837.76265    0.83000    1.00000    0.34548  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.081e+00   5.257e+00    0.206  0.83705    \nomega  8.378e+02   8.986e+04    0.009  0.99256    \nalpha1 8.300e-01   3.069e-01    2.705  0.00684 ** \nalpha2 1.000e+00   2.532e-01    3.949 7.86e-05 ***\nbeta1  3.455e-01   2.719e-02   12.704  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -7865.927    normalized:  -10.62963 \n\nDescription:\n Fri Dec  8 21:42:09 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                   Statistic p-Value\n Jarque-Bera Test   R    Chi^2  3.584037e+05       0\n Shapiro-Wilk Test  R    W      7.070114e-01       0\n Ljung-Box Test     R    Q(10)  1.361311e+02       0\n Ljung-Box Test     R    Q(15)  2.372577e+02       0\n Ljung-Box Test     R    Q(20)  2.815998e+02       0\n Ljung-Box Test     R^2  Q(10)  1.044666e-01       1\n Ljung-Box Test     R^2  Q(15)  2.768365e-01       1\n Ljung-Box Test     R^2  Q(20)  3.529359e-01       1\n LM Arch Test       R    TR^2   1.311712e-01       1\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n21.27278 21.30390 21.27269 21.28478 \n\n\nThe GARCH(2,1) model results show a simpler model compared to the previously attempted GARCH(3,7) model.\nThe result indicates that: The mu coefficient is again not significant, suggesting that the mean of the residuals is not significantly different from zero. The omega coefficient has a very large standard error, indicating instability or issues with the estimation of this parameter. The alpha1 and alpha2 coefficients are significant, suggesting that past squared residuals are important in predicting current volatility. The beta1 coefficient is also significant, implying that past conditional variances are predictive of current conditional variance.\nThe Ljung-Box test for the residuals (R) shows significant autocorrelation, suggesting that the model might not be fully capturing the temporal structure in the data. The Ljung-Box test for the squared residuals (R^2) and the LM Arch Test show no significant autocorrelation, indicating that the GARCH model is successfully modeling volatility clustering.\nOverall, while the GARCH(2,1) model has some issues with the estimation of the omega coefficient, it seems to be a better fit than the more complex models. However, the significant autocorrelation in the residuals suggests that further refinement of the model may be necessary, for example, consider using TGARCH or EGARCH models, but I won’t go into further analysis here.\nThe equation for the model:\n\\[\\begin{equation}\n    \\Delta y_t = 1005.1702 + 0.5033 \\Delta y_{t-1} - 0.4148 \\Delta y_{t-2} + 0.6132 \\Delta y_{t-3} - 0.8180 \\epsilon_{t-1} + 0.0383 \\epsilon_{t-2} - 0.6115 \\epsilon_{t-3} + 0.6630 \\epsilon_{t-4} + \\epsilon_t\n\\end{equation}\\]\n\\[\\begin{equation}\n    \\epsilon_t = \\sigma_t z_t\n\\end{equation}\\]\n\\[\\begin{equation}\n    \\sigma_t^2 = 837.76265 + 0.83000 \\epsilon_{t-1}^2 + 1.00000 \\epsilon_{t-2}^2 + 0.34548 \\sigma_{t-1}^2\n\\end{equation}\\]"
  },
  {
    "objectID": "Financial-Models.html#arimagarch-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "href": "Financial-Models.html#arimagarch-modeling-of-daily-new-confirmed-deaths-in-the-us",
    "title": "Financial Time Series Models",
    "section": "ARIMA+GARCH Modeling of Daily New Confirmed Deaths in the US",
    "text": "ARIMA+GARCH Modeling of Daily New Confirmed Deaths in the US\n\nFitting ARIMA\nAccording to previous analysis, ARIMA(5,1,2) is the best fit model for Daily New Confirmed Deaths in the US.\n\n\nCode\n# Fit chosen ARIMA model\nfit2 &lt;- Arima(new_deaths_ts, order=c(5,1,2))\n\n# Extract resids and its square\nresids2 &lt;- fit2$residuals\nsquare_resids2 &lt;- resids2^2\n\n\n\n\nDetermining Need for Additional Model\n\nACF PlotPACF PlotArch test\n\n\n\n\nCode\n# Plot ACF\nggAcf(square_resids2, 48, main=\"ACF for Squared Residuals\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot PACF\nggPacf(square_resids2, 48, main=\"PACF for Squared Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nArchTest(resids2)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  resids2\nChi-squared = 68.057, df = 12, p-value = 7.385e-10\n\n\n\n\n\nCombining the visual evidence from the ACF and PACF plots with the statistical evidence from the ARCH LM test, it is clear that there are ARCH effects present in the residuals.\n\n\nFitting Garch(p,q) to residuals\n\n\nCode\nmodel &lt;- list() ## set counter\ncc &lt;- 1\nfor (p in 1:1) {\n  for (q in 1:1) {\n  \nmodel[[cc]] &lt;- garch(resids2,order=c(q,p),trace=F)\ncc &lt;- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC &lt;- sapply(model, AIC) \n\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]] ## model with lowest AIC is the best and output model summary\n\n\n\nCall:\ngarch(x = resids2, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n8.485e+04  5.294e-01  4.663e-10  \n\n\n\n\nCode\nsummary(garchFit(~garch(1,1), resids2, trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = resids2, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n&lt;environment: 0x7f9a61c8c2b0&gt;\n [data = resids2]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n      mu     omega    alpha1     beta1  \n0.028830  0.094282  1.000000  0.587135  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      0.028830    0.080063     0.36    0.719    \nomega   0.094282         NaN      NaN      NaN    \nalpha1  1.000000    0.017914    55.82   &lt;2e-16 ***\nbeta1   0.587135    0.009125    64.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -4855.116    normalized:  -6.560968 \n\nDescription:\n Fri Dec  8 21:42:10 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                   Statistic      p-Value\n Jarque-Bera Test   R    Chi^2  5.895898e+04 0.000000e+00\n Shapiro-Wilk Test  R    W      7.684892e-01 0.000000e+00\n Ljung-Box Test     R    Q(10)  4.009290e+01 1.631816e-05\n Ljung-Box Test     R    Q(15)  6.727345e+01 1.360363e-08\n Ljung-Box Test     R    Q(20)  8.478714e+01 5.956960e-10\n Ljung-Box Test     R^2  Q(10)  5.722002e-01 9.999874e-01\n Ljung-Box Test     R^2  Q(15)  7.869662e-01 1.000000e+00\n Ljung-Box Test     R^2  Q(20)  9.989590e-01 1.000000e+00\n LM Arch Test       R    TR^2   7.388805e-01 9.999974e-01\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n13.13275 13.15765 13.13269 13.14235 \n\n\nThe output provided from the GARCH(1,1) modeling procedure includes several important components that give us insights into the fitted model’s performance:\nThe mu coefficient is not significant with a p-value of 0.719, implying that the conditional mean is not different from zero. The omega coefficient has a problem as its standard error is not available (NaN). This could be a result of numerical issues during optimization or model misspecification. The alpha1 coefficient is at the boundary of the parameter space, being exactly 1. This could indicate a model at the edge of stationarity or nonstationarity in variance, and this could be a sign of the Integrated GARCH (IGARCH) effect, where shocks have a persistent effect on volatility. The beta1 coefficient is significant and less than one, suggesting that past conditional variances do have an impact on current variance, but the effect is not indefinitely persistent.\nThe Ljung-Box tests on the residuals R show significant autocorrelations at various lags, suggesting that the GARCH(1,1) model may not be adequately capturing all the dependencies in the data. The Ljung-Box tests on the squared residuals R^2 and the LM Arch Test do not show significant results (p-value ~ 1), indicating no autocorrelation in the squared residuals. This suggests that the GARCH(1,1) model is capturing the volatility clustering in the data.\nGiven the output, while the GARCH(1,1) model captures the volatility clustering (as indicated by the non-significant Ljung-Box tests on squared residuals), the issue with omega needs to be addressed. The significance of alpha1 and beta1 suggests that the model is capturing the dynamics of volatility, but the perfect boundary value of 1 for alpha1 could be problematic, indicating that the model might better be specified as an IGARCH model. Additionally, the significant autocorrelations in the residuals suggest that the mean model might need to be expanded or that there are other forms of temporal dependencies not captured by the current model.\nThe equation for the model:\n\\[\\begin{equation}\n    \\Delta y_t = 2.4559 + 0.5145 \\Delta y_{t-1} - 0.7522 \\Delta y_{t-2} - 0.1719 \\Delta y_{t-3} - 0.3084 \\Delta y_{t-4} - 0.2779 \\Delta y_{t-5} - 1.0594 \\epsilon_{t-1} + 0.7524 \\epsilon_{t-2} + \\epsilon_t\n\\end{equation}\\]\n\\[\\begin{equation}\n    \\epsilon_t = \\sigma_t z_t\n\\end{equation}\\]\n\\[\\begin{equation}\n    \\sigma_t^2 = 0.094282 + 1.000000 \\epsilon_{t-1}^2 + 0.587135 \\sigma_{t-1}^2\n\\end{equation}\\]"
  },
  {
    "objectID": "Financial-Models.html#summary",
    "href": "Financial-Models.html#summary",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "In this section, I add an ARCH/GARCH model to the ARIMA model to further fit and predict the daily number of newly confirmed COVID-19 cases and deaths, and try to capture possible clustering of volatility in the data."
  },
  {
    "objectID": "Financial-Models.html#literature-review",
    "href": "Financial-Models.html#literature-review",
    "title": "Financial Time Series Models",
    "section": "Literature Review",
    "text": "Literature Review\nARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are commonly used to model financial time series data where there is volatility clustering—periods of swings followed by periods of relative calm. These models are well-suited to series with non-constant variance (heteroskedasticity), which is often observed in financial markets.\nFor epidemiological data like daily new COVID-19 cases or deaths, it’s possible that similar patterns of volatility could exist. For example, there might be periods of rapid increases or decreases in case numbers due to external factors like new variants, policy changes, or behavioral shifts. If such patterns are present and result in volatility clustering, then using ARCH or GARCH models could be appropriate. In fact, there are many studies using ARCH/GARCH models to predict COVID-19:\nEkinci (2021) utilized ARMA-GARCH models to forecast the daily growth rate of new COVID-19 cases in nine heavily affected countries, highlighting the models’ superior performance over traditional ARMA models due to their ability to capture conditional heteroskedasticity and heavy-tailed distributions (Ekinci, 2021).\nIn the context of the United Arab Emirates, a study deployed seasonal autoregressive moving average and ARCH models to forecast COVID-19 spread, underscoring the significance of advanced modeling techniques in pandemic management (Kamalov F, Thabtah F., 2021).\nLastly, a South African study by Mthethwa N et al. (2022) employed Markov-switching GARCH-type models with heavy-tailed distributions, focusing on estimating the minimum daily death toll from COVID-19. This research emphasized the similarity between the volatility clustering in COVID-19 death data and financial returns (Mthethwa N, Chifurira R, Chinhamu K., 2022)."
  },
  {
    "objectID": "Deep-Learning.html#data-prepare",
    "href": "Deep-Learning.html#data-prepare",
    "title": "Deep Learning for Time Series",
    "section": "Data Prepare",
    "text": "Data Prepare"
  },
  {
    "objectID": "Deep-Learning.html#rnn",
    "href": "Deep-Learning.html#rnn",
    "title": "Deep Learning for Time Series",
    "section": "RNN",
    "text": "RNN\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\n\n\n\nTrain RMSE: 0.717 RMSE\nTest RMSE: 0.265 RMSE"
  },
  {
    "objectID": "Deep-Learning.html#deep-learning-for-daily-new-confirmed-cases-in-the-us",
    "href": "Deep-Learning.html#deep-learning-for-daily-new-confirmed-cases-in-the-us",
    "title": "Deep Learning for Time Series",
    "section": "Deep Learning for Daily New Confirmed Cases in the US",
    "text": "Deep Learning for Daily New Confirmed Cases in the US\n\nRecurrent Neural Network\nThe RMSE values for training are similar for both models, which suggests that regularization hasn’t impacted the ability of the model to fit the training data much. However, the test RMSE is slightly better in the regularized model (0.301) compared to the non-regularized one (0.312). This improvement in test RMSE indicates that the regularized model generalizes slightly better to unseen data.\nThe lower validation loss as compared to the training loss in both models is not typical and could suggest that the validation set is easier for the model to predict than the training set.\nOverall, the regularization seems to be serving its purpose by slightly improving the model’s generalization, as indicated by the lower test RMSE. However, the difference is minimal, suggesting that the model was not significantly overfitting the training data even without regularization.\n\nRNNRNN including Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\n\n\n\nTrain RMSE: 0.715 RMSE\nTest RMSE: 0.283 RMSE\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.721 RMSE\nTest RMSE: 0.311 RMSE\n\n\n\n\n\n\n\nGated Recurrent unit\nThe GRU model without regularization achieves a lower training RMSE (0.572) compared to the regularized model (1.004), which is consistent with expectations as regularization typically increases training error due to the added constraints. However, the test RMSE is nearly identical for both models, with the regularized model showing a marginally better test RMSE (0.403) compared to the non-regularized model (0.408).\nThis marginal improvement in test RMSE for the regularized model suggests that regularization has helped to improve the generalization of the model to new data, although the effect is slight. This effect is not as clearly reflected in the validation loss, which is consistently low for both models. Again, the lower validation loss compared to the training loss is unusual and suggests potential issues with the validation process or data.\nIn conclusion, while the regularized GRU model exhibits higher training loss due to the effects of L1L2 regularization, it achieves a slightly better generalization on the test data, as evidenced by the lower test RMSE.\n\nGRUGRU including Regularization\n\n\n\n\nCode\n# Create a GRU\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.575 RMSE\nTest RMSE: 0.404 RMSE\n\n\n\n\n\n\nCode\n# Create a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.555 RMSE\nTest RMSE: 0.413 RMSE\n\n\n\n\n\n\n\nLong Short Term Memory\nWith the LSTM models, we see that regularization has led to a slight increase in both the training and test RMSE. The training RMSE has increased from 0.481 to 0.513, which could be due to the regularization term adding a penalty to the cost function and therefore making it harder for the model to fit the training data perfectly. This is a typical effect of regularization, as it trades off variance for bias, leading to a less complex model that might not capture all the nuances in the training data.\nHowever, contrary to the usual benefits of regularization, the test RMSE has also increased, albeit slightly, from 0.302 to 0.309. This suggests that for this particular dataset and model architecture, the regularization has not provided a benefit in terms of generalization to the test data. It’s possible that the non-regularized model was not overfitting to begin with, so the regularization did not lead to a performance improvement on the test set. Alternatively, the type and amount of regularization might not be optimal for this problem, and tweaking the regularization parameters could potentially yield different results.\nIt’s important to note that while the training and validation losses provide a good indication of the model’s learning process, the ultimate measure of performance is the test RMSE, which tells us how well the model is expected to perform on unseen data. The slight increase in test RMSE for the regularized model indicates that in this case, regularization may not be beneficial.\n\nLSTMLSTM including Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.489 RMSE\nTest RMSE: 0.303 RMSE\n\n\n\n\n\n\nCode\n# Create a LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.490 RMSE\nTest RMSE: 0.315 RMSE\n\n\n\n\n\n\n\nComparing\n\n\n\n\n\n\n\n\n\nmodel\ntraining_rmse\ntesting_rmse\n\n\n\n\n0\nRecurrent Neural Network\n0.714508\n0.282691\n\n\n1\nRecurrent Neural Network (with L1L2 Regulariza...\n0.721246\n0.311048\n\n\n2\nGRU Neural Network\n0.575008\n0.404393\n\n\n3\nGRU Neural Network (with L1L2 Regularization)\n0.555211\n0.412992\n\n\n4\nLSTM Neural Network\n0.488574\n0.302897\n\n\n5\nLSTM Neural Network (with L1L2 Regularization)\n0.490416\n0.314806\n\n\n\n\n\n\n\nThe LSTM without regularization has shown the best performance on the training data, indicating good fitting capabilities. The RNN with regularization appears to have the best performance on the test data, indicating better generalization and predictive power.\nRegularization improved the predictive power of the RNN model, as indicated by the decrease in test RMSE. For the GRU and LSTM models, the impact of regularization on the test RMSE is minimal, with a slight decrease for the GRU and a slight increase for the LSTM. This suggests that the benefit of regularization in these cases is less clear, and it may not be necessary or the parameters might need adjustment."
  },
  {
    "objectID": "Deep-Learning.html#as",
    "href": "Deep-Learning.html#as",
    "title": "Deep Learning for Time Series",
    "section": "as",
    "text": "as"
  },
  {
    "objectID": "Deep-Learning.html#deep-learning-for-daily-new-confirmed-deaths-in-the-us",
    "href": "Deep-Learning.html#deep-learning-for-daily-new-confirmed-deaths-in-the-us",
    "title": "Deep Learning for Time Series",
    "section": "Deep Learning for Daily New Confirmed Deaths in the US",
    "text": "Deep Learning for Daily New Confirmed Deaths in the US\n\nRecurrent Neural Network\nBased on the RMSE values, the RNN with regularization performs better on both the training and test datasets, with the test RMSE decreasing from 0.533 to 0.495, which is a significant improvement in predictive accuracy.\nIn summary, regularization has improved the RNN model’s performance, evidenced by both the loss curves and the RMSE values. The smoother loss curves and reduced RMSE values suggest that the regularized model is more reliable when making predictions on new data.\n\nRNNRNN including Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2 = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\n\n\n\nTrain RMSE: 0.586 RMSE\nTest RMSE: 0.504 RMSE\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table_2['training_rmse'].append(train_rmse)\nrmse_table_2['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.573 RMSE\nTest RMSE: 0.488 RMSE\n\n\n\n\n\n\n\nGated Recurrent unit\nThe GRU model with L1L2 regularization shows a better learning trend and reduced RMSE values, suggesting an improvement in model performance. However, the high variance in validation loss and the relatively small improvement in test RMSE indicate that there may still be issues to address, such as further hyperparameter tuning, gathering more training data, or experimenting with different model architectures.\n\nGRUGRU including Regularization\n\n\n\n\nCode\n# Create a GRU\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2['model'].append('GRU Neural Network')\nrmse_table_2['training_rmse'].append(train_rmse)\nrmse_table_2['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.995 RMSE\nTest RMSE: 0.935 RMSE\n\n\n\n\n\n\nCode\n# Create a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table_2['training_rmse'].append(train_rmse)\nrmse_table_2['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.767 RMSE\nTest RMSE: 0.885 RMSE\n\n\n\n\n\n\n\nLong Short Term Memory\nThe LSTM network benefits from the use of L1L2 regularization, albeit modestly. The regularization helps to slightly improve the model’s prediction accuracy and stability, as evidenced by the reduced RMSE values on both the training and test datasets. The behavior of the loss curves suggests that the model with regularization might be more reliable when applied to new, unseen data, although the effect of regularization is not dramatically large in this case.\n\nLSTMLSTM including Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2['model'].append('LSTM Neural Network')\nrmse_table_2['training_rmse'].append(train_rmse)\nrmse_table_2['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.524 RMSE\nTest RMSE: 0.480 RMSE\n\n\n\n\n\n\nCode\n# Create a LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table_2['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table_2['training_rmse'].append(train_rmse)\nrmse_table_2['testing_rmse'].append(test_rmse)\n\n\n\n\n\nTrain RMSE: 0.492 RMSE\nTest RMSE: 0.485 RMSE\n\n\n\n\n\n\n\nComparing\n\n\n\n\n\n\n\n\n\nmodel\ntraining_rmse\ntesting_rmse\n\n\n\n\n0\nRecurrent Neural Network\n0.585985\n0.504004\n\n\n1\nRecurrent Neural Network (with L1L2 Regulariza...\n0.573473\n0.488104\n\n\n2\nGRU Neural Network\n0.995457\n0.935351\n\n\n3\nGRU Neural Network (with L1L2 Regularization)\n0.766639\n0.884788\n\n\n4\nLSTM Neural Network\n0.523506\n0.480279\n\n\n5\nLSTM Neural Network (with L1L2 Regularization)\n0.492418\n0.485291\n\n\n\n\n\n\n\nLSTM shows the best performance both with and without regularization, having the lowest RMSE values across all models. This suggests that the LSTM model has the highest accuracy and predictive power among the three.\nRNN benefits significantly from regularization, showing a good reduction in RMSE values when regularization is applied. This indicates an improvement in the model’s ability to generalize, although not to the extent of the LSTM model.\nGRU shows the most significant decrease in training RMSE after applying regularization, but the improvement in the test RMSE is not as pronounced as with the RNN. Despite this improvement, the GRU’s performance is still behind both the LSTM and the regularized RNN in terms of test RMSE.\nIn conclusion, LSTM appears to be the most accurate and powerful model for the data and tasks presented, with the lowest test RMSE reflecting its strong predictive power. RNN and GRU benefit from regularization but do not reach the performance level of the LSTM."
  },
  {
    "objectID": "Deep-Learning.html#summary",
    "href": "Deep-Learning.html#summary",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "In this section, I try to create a model that can predict the number of daily new cases and deaths of COVID-19 by leveraging techniques such as RNN, LSTM, GRU, and regularization."
  },
  {
    "objectID": "Deep-Learning.html#deep-learning-vs-traditional-ts-model",
    "href": "Deep-Learning.html#deep-learning-vs-traditional-ts-model",
    "title": "Deep Learning for Time Series",
    "section": "Deep Learning vs Traditional TS Model",
    "text": "Deep Learning vs Traditional TS Model\nThe ability of a deep learning model to accurately predict the future depends on a variety of factors, including the complexity of the problem, the quality and quantity of the data, the architecture of the model, and more. Models such as LSTM and GRU can remember long sequences of information, which theoretically allows them to further predict the future. However, the further out the forecast, the more likely it is to be affected by overfitting to past data or the compounding of small forecast errors, especially when we are predicting the number of daily new cases and deaths from COVID-19. When affecting complex data, it is possible that short-term forecasts (e.g., 30 days or less) will be relatively more reliable.\nDeep learning models and traditional time-series models like ARMA (Autoregressive Moving Average) and ARIMA (Autoregressive Integrated Moving Average) are fundamentally different in their approaches and capabilities, and each has its advantages and disadvantages depending on the context of the problem.\n\nDeep learning models are well-suited for handling large datasets with complex patterns and multiple variables (multivariate time series). They can automatically detect and model nonlinear relationships in the data without the need for explicit feature engineering.\nTraditional time-series models like ARMA or ARIMA are more data-efficient and can often produce good results with smaller datasets. They are typically used for univariate time series, although extensions like Vector ARIMA (VARIMA) exist for multivariate cases.\n\nOverall, Deep learning models might outperform ARMA/ARIMA when dealing with complex patterns and large multivariate time series. However, for simpler, smaller, or more stable univariate time series, ARMA/ARIMA models can be competitive or even superior."
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#summary",
    "href": "ARIMAX-SARIMAX-VAR.html#summary",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "",
    "text": "In this section, I will use the VAR model to examine the relationship between the daily number of newly confirmed COVID-19 deaths and the number of ICU patients and the number of hospital patients, and use the ARIMAX model to examine the relationship between the daily number of newly confirmed COVID-19 cases and the number of vaccinated people. relationships and try to create effective predictive models."
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#literature-revie",
    "href": "ARIMAX-SARIMAX-VAR.html#literature-revie",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "Literature Revie",
    "text": "Literature Revie"
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#qwx",
    "href": "ARIMAX-SARIMAX-VAR.html#qwx",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "QWX",
    "text": "QWX"
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#literature-review",
    "href": "ARIMAX-SARIMAX-VAR.html#literature-review",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "Literature review",
    "text": "Literature review\nI synthesizes findings from three critical studies that investigate the rate of Intensive Care Unit (ICU) admissions and outcomes among patients with coronavirus, as well as factors associated with ICU mortality in COVID-19 patients:\nAbate, et al (2020) did a systematic review and meta-analysis aimed to understand the rate of Intensive Care Unit (ICU) admissions and outcomes among coronavirus patients. Their study synthesized data from various observational studies to provide a global perspective on the critical care aspects of COVID-19. It emphasized the variability in ICU admission rates and highlighted the significant factors influencing patient outcomes in ICUs, such as healthcare system capabilities, patient demographics, and infection severity (Abate, S. M., Ahmed Ali, S., Mantfardo, B., & Basu, B., 2020).\nLavrentieva, A., et al (2023) conducted an observational study, with a critical review of the literature to identify factors associated with ICU mortality in COVID-19 patients. Through analyzing patient data, the research identified key predictors of mortality within ICU environments, such as comorbidities, age, and treatment modalities. The study’s findings provide valuable insights for healthcare practitioners in understanding and predicting the risk factors for mortality among critically ill COVID-19 patients (Lavrentieva, A., Kaimakamis, E., Voutsas, V., & Bitzani, M., 2023).\nKowsar, R., et al (2023), their article presents a meta- and network analysis to assess the risk of mortality in COVID-19 patients. Combining data from various sources, the study offers a comprehensive view of the factors contributing to patient outcomes during the pandemic. It highlights the complexity of managing COVID-19, particularly in critical care settings, and underscores the importance of understanding various factors, including patient demographics and comorbidities, in determining patient outcomes (Kowsar, R., Rahimi, A. M., Sroka, M., Mansouri, A., Sadeghi, K., Bonakdar, E., Kateb, S. F., & Mahdavi, A. H., 2023).\nThe ongoing COVID-19 pandemic has prompted extensive research into the impact of vaccination on the spread and severity of the virus. This literature review synthesizes findings from recent studies to understand better the correlation between daily new COVID-19 cases and vaccination:\nA global study emphasized the effectiveness of COVID-19 vaccination in reducing new cases and deaths. This research, analyzing global vaccine data, concluded that while vaccination significantly curtails the spread and fatality rates of COVID-19, the inequitable distribution of vaccines across countries poses a significant challenge (Global Vaccine Data Analysis, PubMed). This study’s findings are crucial in understanding the global dynamics of vaccine impact on COVID-19 cases (Li Z, Liu X, Liu M, Wu Z, Liu Y, Li W, Liu M, Wang X, Gao B, Luo Y, Li X, Tao L, Wang W, Guo X., 2021).\nFocusing on the United States, a study in Nature explored the impact of vaccination across various states. It reported no significant correlation between pre-vaccination growth rates and vaccination rates, suggesting a consistent effectiveness of vaccines across different demographics and geographical areas. This research implies that the effectiveness of COVID-19 vaccines is relatively uniform, regardless of variations in states’ demographic and political characteristics (Chen, X., Huang, H., Ju, J. et al., 2022).\nAnother research investigated the correlation between COVID-19 vaccine coverage rates and hospitalization on a global scale. This study found a direct relationship between vaccine coverage and the number of hospital and ICU patients, indicating that higher vaccination rates correlate with lower hospitalization rates (Huang C, Yang L, Pan J, Xu X, Peng R., 2022).\nMathematical modeling studies have also contributed significantly to this topic. Various studies assessed the effectiveness of vaccines, the impact on COVID-19 case numbers, and the role of vaccination in conjunction with non-pharmaceutical interventions. These models provide a predictive insight into the impact of vaccination strategies and their potential to control the pandemic (Moore, S., Hill, E.M., Dyson, L. et al., 2022).\nLastly, an analysis by Johns Hopkins University revealed a significant negative correlation between vaccination coverage and the incidence and mortality rates of COVID-19 in U.S. counties. This study highlighted the disparities in vaccination uptake and the impact of COVID-19 across different county demographics, such as political affiliation, land use, and socioeconomic status (Ensheng Dong and Lauren Gardner, 2021).\nIn conclusion, the reviewed literature consistently indicates that COVID-19 vaccination is effective in reducing new cases and deaths. While the impact varies globally due to differences in vaccine distribution and uptake, the overall trend underscores the critical role of vaccination in controlling the pandemic. The variations observed in different regions and among various demographics also point to the need for tailored vaccination strategies and equitable vaccine distribution to maximize public health benefits."
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#var---daily-new-confirmed-cases-in-the-us-hospital-patients-and-icu-patients",
    "href": "ARIMAX-SARIMAX-VAR.html#var---daily-new-confirmed-cases-in-the-us-hospital-patients-and-icu-patients",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "VAR - Daily New Confirmed Cases in the US, Hospital Patients, and ICU Patients",
    "text": "VAR - Daily New Confirmed Cases in the US, Hospital Patients, and ICU Patients\n\nExploratory Analysis\n\n\nCode\n# Plot Timeseries of all 3 variables\nautoplot(var_data, facets = T) + labs(title = \"Correlation Time Series Plots\")+theme_bw()\n\n\n\n\n\n\n\nCode\n# Plot Pairs\npairs(cbind(New_Deaths=new_deaths_ts, ICU_Patients=icu_ts, Hospital_Patients=hosp_ts))\n\n\n\n\n\nThe “icu_ts” plot shows fluctuations similar to “new_deaths_ts” but with less pronounced peaks, suggesting that ICU admissions follow a similar trend as deaths but might not correlate perfectly. The “hosp_ts” plot likely shows the number of hospital patients over time, which, like the ICU data, exhibits waves of increases and decreases. The pattern might follow a similar trend to the ICU and death data, potentially with a lead time as hospital admissions typically precede ICU admissions and deaths.\nThe scatterplot between “New_Deaths” and “ICU_Patients” might show a positive correlation, indicated by the clustering of points along an upward trajectory. This suggests that as ICU admissions increase, the number of deaths also increases, which is expected as more severe cases lead to higher mortality. The scatterplot between “ICU_Patients” and “Hospital_Patients” probably shows a strong positive correlation, as a high number of hospital admissions could lead to more patients requiring ICU care.\n\n\nFitting VAR(p)\n\n\nCode\nVARselect(var_data, lag.max = 10, type = \"const\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      8      8     10 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 3.721412e+01 3.659801e+01 3.636002e+01 3.623836e+01 3.606997e+01\nHQ(n)  3.724355e+01 3.664952e+01 3.643359e+01 3.633401e+01 3.618769e+01\nSC(n)  3.729036e+01 3.673143e+01 3.655061e+01 3.648614e+01 3.637492e+01\nFPE(n) 1.451738e+16 7.840012e+15 6.179557e+15 5.471747e+15 4.623790e+15\n                  6            7            8            9           10\nAIC(n) 3.560885e+01 3.530470e+01 3.510303e+01 3.508653e+01 3.507037e+01\nHQ(n)  3.574864e+01 3.546656e+01 3.528697e+01 3.529254e+01 3.529846e+01\nSC(n)  3.597098e+01 3.572401e+01 3.557952e+01 3.562020e+01 3.566122e+01\nFPE(n) 2.915694e+15 2.151099e+15 1.758284e+15 1.729570e+15 1.701919e+15\n\n\n\nSummary VAR(10)Summary VAR(8)\n\n\n\n\nCode\nsummary(vars::VAR(var_data, p=10, type=c('const')))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: new_deaths_ts, icu_ts, hosp_ts \nDeterministic variables: const \nSample size: 721 \nLog Likelihood: -15619.034 \nRoots of the characteristic polynomial:\n0.9888 0.9888 0.9855 0.9855 0.9771 0.9771 0.9593 0.9593 0.9329 0.8817 0.8817 0.8768 0.8768 0.8729 0.8729 0.8317 0.8317 0.817 0.817 0.8146 0.8146 0.7998 0.7998 0.7626 0.7128 0.4857 0.4782 0.4782 0.4084 0.2348\nCall:\nvars::VAR(y = var_data, p = 10, type = c(\"const\"))\n\n\nEstimation results for equation new_deaths_ts: \n============================================== \nnew_deaths_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1    0.327715   0.037994   8.625  &lt; 2e-16 ***\nicu_ts.l1          -0.020742   0.081921  -0.253 0.800188    \nhosp_ts.l1         -0.057217   0.014777  -3.872 0.000118 ***\nnew_deaths_ts.l2   -0.087889   0.039796  -2.209 0.027537 *  \nicu_ts.l2           0.019591   0.107480   0.182 0.855419    \nhosp_ts.l2          0.083567   0.021184   3.945  8.8e-05 ***\nnew_deaths_ts.l3    0.002839   0.040004   0.071 0.943444    \nicu_ts.l3           0.173186   0.104094   1.664 0.096618 .  \nhosp_ts.l3         -0.020234   0.021187  -0.955 0.339903    \nnew_deaths_ts.l4    0.007016   0.035516   0.198 0.843449    \nicu_ts.l4          -0.178972   0.104009  -1.721 0.085746 .  \nhosp_ts.l4          0.028808   0.021156   1.362 0.173737    \nnew_deaths_ts.l5   -0.109857   0.034449  -3.189 0.001492 ** \nicu_ts.l5           0.048250   0.104011   0.464 0.642869    \nhosp_ts.l5         -0.045305   0.021013  -2.156 0.031429 *  \nnew_deaths_ts.l6    0.200848   0.034387   5.841  8.0e-09 ***\nicu_ts.l6           0.018764   0.102552   0.183 0.854872    \nhosp_ts.l6          0.009816   0.020949   0.469 0.639526    \nnew_deaths_ts.l7    0.457076   0.035449  12.894  &lt; 2e-16 ***\nicu_ts.l7          -0.034151   0.102600  -0.333 0.739345    \nhosp_ts.l7         -0.004347   0.020889  -0.208 0.835198    \nnew_deaths_ts.l8   -0.022132   0.039590  -0.559 0.576321    \nicu_ts.l8          -0.014865   0.103011  -0.144 0.885299    \nhosp_ts.l8         -0.006256   0.020752  -0.301 0.763159    \nnew_deaths_ts.l9   -0.087018   0.038837  -2.241 0.025370 *  \nicu_ts.l9          -0.081341   0.103077  -0.789 0.430307    \nhosp_ts.l9          0.031191   0.020559   1.517 0.129696    \nnew_deaths_ts.l10   0.082077   0.036355   2.258 0.024278 *  \nicu_ts.l10          0.077634   0.075250   1.032 0.302585    \nhosp_ts.l10        -0.015651   0.014698  -1.065 0.287322    \nconst             -47.786431  22.706221  -2.105 0.035691 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 302.1 on 690 degrees of freedom\nMultiple R-Squared: 0.9025, Adjusted R-squared: 0.8982 \nF-statistic: 212.8 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation icu_ts: \n======================================= \nicu_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1   0.0029745  0.0209510   0.142 0.887141    \nicu_ts.l1          0.9272595  0.0451731  20.527  &lt; 2e-16 ***\nhosp_ts.l1         0.0176839  0.0081483   2.170 0.030329 *  \nnew_deaths_ts.l2   0.0473695  0.0219443   2.159 0.031224 *  \nicu_ts.l2          0.1736561  0.0592671   2.930 0.003501 ** \nhosp_ts.l2        -0.0002385  0.0116813  -0.020 0.983720    \nnew_deaths_ts.l3   0.0181799  0.0220592   0.824 0.410144    \nicu_ts.l3         -0.0780128  0.0574004  -1.359 0.174559    \nhosp_ts.l3         0.0023755  0.0116833   0.203 0.838942    \nnew_deaths_ts.l4   0.0419309  0.0195843   2.141 0.032620 *  \nicu_ts.l4          0.0022318  0.0573532   0.039 0.968971    \nhosp_ts.l4        -0.0244430  0.0116658  -2.095 0.036511 *  \nnew_deaths_ts.l5  -0.0105252  0.0189961  -0.554 0.579709    \nicu_ts.l5          0.0604113  0.0573541   1.053 0.292570    \nhosp_ts.l5         0.0173592  0.0115873   1.498 0.134560    \nnew_deaths_ts.l6  -0.0379871  0.0189619  -2.003 0.045531 *  \nicu_ts.l6         -0.0420519  0.0565499  -0.744 0.457357    \nhosp_ts.l6         0.0020607  0.0115516   0.178 0.858469    \nnew_deaths_ts.l7  -0.0019781  0.0195474  -0.101 0.919423    \nicu_ts.l7          0.2727134  0.0565763   4.820 1.76e-06 ***\nhosp_ts.l7         0.0043068  0.0115189   0.374 0.708598    \nnew_deaths_ts.l8   0.0208004  0.0218311   0.953 0.341030    \nicu_ts.l8         -0.2315191  0.0568031  -4.076 5.12e-05 ***\nhosp_ts.l8        -0.0231636  0.0114430  -2.024 0.043327 *  \nnew_deaths_ts.l9  -0.0396008  0.0214157  -1.849 0.064864 .  \nicu_ts.l9         -0.1490258  0.0568393  -2.622 0.008937 ** \nhosp_ts.l9        -0.0126608  0.0113369  -1.117 0.264477    \nnew_deaths_ts.l10  0.0091967  0.0200469   0.459 0.646550    \nicu_ts.l10         0.0653495  0.0414950   1.575 0.115743    \nhosp_ts.l10        0.0143484  0.0081049   1.770 0.077110 .  \nconst             44.3890802 12.5208093   3.545 0.000419 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 166.6 on 690 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 5.445e+04 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation hosp_ts: \n======================================== \nhosp_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1   -0.114957   0.115580  -0.995  0.32027    \nicu_ts.l1           0.089584   0.249205   0.359  0.71935    \nhosp_ts.l1          1.104448   0.044952  24.570  &lt; 2e-16 ***\nnew_deaths_ts.l2    0.158645   0.121060   1.310  0.19047    \nicu_ts.l2           0.908556   0.326957   2.779  0.00560 ** \nhosp_ts.l2         -0.056499   0.064442  -0.877  0.38093    \nnew_deaths_ts.l3    0.262472   0.121694   2.157  0.03136 *  \nicu_ts.l3          -1.264251   0.316659  -3.992 7.24e-05 ***\nhosp_ts.l3          0.113914   0.064453   1.767  0.07761 .  \nnew_deaths_ts.l4    0.193323   0.108040   1.789  0.07400 .  \nicu_ts.l4          -0.118625   0.316399  -0.375  0.70783    \nhosp_ts.l4         -0.089850   0.064357  -1.396  0.16313    \nnew_deaths_ts.l5    0.053538   0.104795   0.511  0.60960    \nicu_ts.l5           0.494735   0.316404   1.564  0.11837    \nhosp_ts.l5          0.087882   0.063923   1.375  0.16964    \nnew_deaths_ts.l6   -0.398812   0.104607  -3.812  0.00015 ***\nicu_ts.l6          -0.857383   0.311967  -2.748  0.00615 ** \nhosp_ts.l6         -0.007494   0.063726  -0.118  0.90642    \nnew_deaths_ts.l7    0.281324   0.107836   2.609  0.00928 ** \nicu_ts.l7           0.929703   0.312113   2.979  0.00300 ** \nhosp_ts.l7          0.121091   0.063546   1.906  0.05712 .  \nnew_deaths_ts.l8   -0.027177   0.120435  -0.226  0.82154    \nicu_ts.l8           0.021918   0.313364   0.070  0.94426    \nhosp_ts.l8         -0.158762   0.063127  -2.515  0.01213 *  \nnew_deaths_ts.l9   -0.052870   0.118144  -0.448  0.65465    \nicu_ts.l9          -0.386696   0.313564  -1.233  0.21791    \nhosp_ts.l9         -0.280582   0.062542  -4.486 8.49e-06 ***\nnew_deaths_ts.l10   0.003540   0.110592   0.032  0.97447    \nicu_ts.l10          0.214646   0.228914   0.938  0.34874    \nhosp_ts.l10         0.145883   0.044712   3.263  0.00116 ** \nconst             210.933125  69.073198   3.054  0.00235 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 918.9 on 690 degrees of freedom\nMultiple R-Squared: 0.9994, Adjusted R-squared: 0.9993 \nF-statistic: 3.539e+04 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n              new_deaths_ts icu_ts hosp_ts\nnew_deaths_ts         91242  -3743   -9817\nicu_ts                -3743  27744   83031\nhosp_ts               -9817  83031  844356\n\nCorrelation matrix of residuals:\n              new_deaths_ts  icu_ts  hosp_ts\nnew_deaths_ts       1.00000 -0.0744 -0.03537\nicu_ts             -0.07440  1.0000  0.54249\nhosp_ts            -0.03537  0.5425  1.00000\n\n\n\n\n\n\nCode\nsummary(vars::VAR(var_data, p=8, type=c('const')))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: new_deaths_ts, icu_ts, hosp_ts \nDeterministic variables: const \nSample size: 723 \nLog Likelihood: -15719.638 \nRoots of the characteristic polynomial:\n0.9874 0.9874 0.9839 0.9839 0.9781 0.9781 0.945 0.945 0.9431 0.8818 0.8818 0.8519 0.8519 0.8372 0.8372 0.8332 0.8332 0.8208 0.8208 0.7678 0.7678 0.7261 0.7261 0.09812\nCall:\nvars::VAR(y = var_data, p = 8, type = c(\"const\"))\n\n\nEstimation results for equation new_deaths_ts: \n============================================== \nnew_deaths_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  3.254e-01  3.784e-02   8.598  &lt; 2e-16 ***\nicu_ts.l1        -5.010e-02  7.365e-02  -0.680 0.496566    \nhosp_ts.l1       -5.794e-02  1.407e-02  -4.119 4.27e-05 ***\nnew_deaths_ts.l2 -1.164e-01  3.508e-02  -3.318 0.000955 ***\nicu_ts.l2         2.950e-02  1.026e-01   0.287 0.773897    \nhosp_ts.l2        8.572e-02  2.083e-02   4.115 4.33e-05 ***\nnew_deaths_ts.l3  3.116e-02  3.411e-02   0.914 0.361233    \nicu_ts.l3         1.901e-01  1.025e-01   1.854 0.064118 .  \nhosp_ts.l3       -1.770e-02  2.083e-02  -0.850 0.395642    \nnew_deaths_ts.l4  2.707e-02  3.386e-02   0.799 0.424374    \nicu_ts.l4        -1.882e-01  1.020e-01  -1.846 0.065324 .  \nhosp_ts.l4        2.321e-02  2.069e-02   1.122 0.262432    \nnew_deaths_ts.l5 -1.176e-01  3.404e-02  -3.455 0.000584 ***\nicu_ts.l5         3.541e-02  1.015e-01   0.349 0.727287    \nhosp_ts.l5       -4.386e-02  2.076e-02  -2.113 0.034984 *  \nnew_deaths_ts.l6  2.026e-01  3.413e-02   5.937 4.57e-09 ***\nicu_ts.l6         4.782e-02  1.010e-01   0.473 0.636167    \nhosp_ts.l6        8.415e-03  2.070e-02   0.407 0.684465    \nnew_deaths_ts.l7  4.797e-01  3.422e-02  14.017  &lt; 2e-16 ***\nicu_ts.l7        -3.686e-02  1.009e-01  -0.365 0.714865    \nhosp_ts.l7        2.031e-04  2.039e-02   0.010 0.992054    \nnew_deaths_ts.l8 -4.895e-02  3.621e-02  -1.352 0.176888    \nicu_ts.l8        -2.054e-02  7.163e-02  -0.287 0.774370    \nhosp_ts.l8        5.997e-03  1.409e-02   0.425 0.670641    \nconst            -4.196e+01  2.206e+01  -1.902 0.057543 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 302.8 on 698 degrees of freedom\nMultiple R-Squared: 0.9008, Adjusted R-squared: 0.8974 \nF-statistic: 264.2 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation icu_ts: \n======================================= \nicu_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  0.006811   0.021555   0.316 0.752103    \nicu_ts.l1         0.943548   0.041951  22.492  &lt; 2e-16 ***\nhosp_ts.l1        0.016133   0.008012   2.014 0.044439 *  \nnew_deaths_ts.l2  0.031038   0.019981   1.553 0.120788    \nicu_ts.l2         0.118300   0.058455   2.024 0.043375 *  \nhosp_ts.l2       -0.003767   0.011865  -0.317 0.750967    \nnew_deaths_ts.l3  0.005092   0.019427   0.262 0.793293    \nicu_ts.l3        -0.019454   0.058385  -0.333 0.739089    \nhosp_ts.l3        0.011128   0.011863   0.938 0.348534    \nnew_deaths_ts.l4  0.056087   0.019286   2.908 0.003751 ** \nicu_ts.l4        -0.061904   0.058076  -1.066 0.286831    \nhosp_ts.l4       -0.037058   0.011786  -3.144 0.001736 ** \nnew_deaths_ts.l5 -0.014330   0.019388  -0.739 0.460080    \nicu_ts.l5         0.133387   0.057808   2.307 0.021323 *  \nhosp_ts.l5        0.027497   0.011824   2.325 0.020333 *  \nnew_deaths_ts.l6 -0.037338   0.019439  -1.921 0.055166 .  \nicu_ts.l6        -0.034694   0.057546  -0.603 0.546778    \nhosp_ts.l6       -0.001643   0.011789  -0.139 0.889171    \nnew_deaths_ts.l7 -0.004833   0.019492  -0.248 0.804242    \nicu_ts.l7         0.204202   0.057448   3.555 0.000404 ***\nhosp_ts.l7        0.014777   0.011612   1.273 0.203617    \nnew_deaths_ts.l8  0.019637   0.020625   0.952 0.341385    \nicu_ts.l8        -0.281813   0.040799  -6.907 1.11e-11 ***\nhosp_ts.l8       -0.029890   0.008028  -3.723 0.000213 ***\nconst            46.701628  12.563784   3.717 0.000218 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 172.5 on 698 degrees of freedom\nMultiple R-Squared: 0.9995, Adjusted R-squared: 0.9995 \nF-statistic: 6.349e+04 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation hosp_ts: \n======================================== \nhosp_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  -0.10799    0.11767  -0.918 0.359091    \nicu_ts.l1          0.17936    0.22901   0.783 0.433765    \nhosp_ts.l1         1.11381    0.04374  25.466  &lt; 2e-16 ***\nnew_deaths_ts.l2   0.11474    0.10907   1.052 0.293167    \nicu_ts.l2          0.65129    0.31910   2.041 0.041628 *  \nhosp_ts.l2        -0.09295    0.06477  -1.435 0.151727    \nnew_deaths_ts.l3   0.23116    0.10605   2.180 0.029606 *  \nicu_ts.l3         -1.16170    0.31872  -3.645 0.000287 ***\nhosp_ts.l3         0.17041    0.06476   2.631 0.008689 ** \nnew_deaths_ts.l4   0.29553    0.10528   2.807 0.005140 ** \nicu_ts.l4         -0.18435    0.31703  -0.581 0.561105    \nhosp_ts.l4        -0.14710    0.06434  -2.286 0.022540 *  \nnew_deaths_ts.l5   0.02368    0.10584   0.224 0.823010    \nicu_ts.l5          0.76787    0.31557   2.433 0.015213 *  \nhosp_ts.l5         0.14955    0.06455   2.317 0.020797 *  \nnew_deaths_ts.l6  -0.41670    0.10612  -3.927 9.46e-05 ***\nicu_ts.l6         -0.96670    0.31414  -3.077 0.002171 ** \nhosp_ts.l6        -0.04013    0.06435  -0.624 0.533087    \nnew_deaths_ts.l7   0.16365    0.10640   1.538 0.124495    \nicu_ts.l7          0.70499    0.31360   2.248 0.024887 *  \nhosp_ts.l7         0.13112    0.06339   2.068 0.038967 *  \nnew_deaths_ts.l8   0.04158    0.11259   0.369 0.712004    \nicu_ts.l8          0.04308    0.22272   0.193 0.846670    \nhosp_ts.l8        -0.30499    0.04382  -6.959 7.90e-12 ***\nconst            214.50350   68.58498   3.128 0.001836 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 941.5 on 698 degrees of freedom\nMultiple R-Squared: 0.9993, Adjusted R-squared: 0.9993 \nF-statistic: 4.214e+04 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n              new_deaths_ts icu_ts hosp_ts\nnew_deaths_ts         91687  -3044  -12186\nicu_ts                -3044  29744   87327\nhosp_ts              -12186  87327  886383\n\nCorrelation matrix of residuals:\n              new_deaths_ts   icu_ts  hosp_ts\nnew_deaths_ts       1.00000 -0.05829 -0.04275\nicu_ts             -0.05829  1.00000  0.53782\nhosp_ts            -0.04275  0.53782  1.00000\n\n\n\n\n\nThe log likelihood is higher (closer to zero) for VAR(10) (-15619.034) compared to VAR(8) (-15719.638), suggesting that the VAR(10) model may fit the data better. In both models, many of the lagged values of new_deaths_ts, icu_ts, and hosp_ts are significant predictors of themselves and each other, which suggests that past values have a substantial influence on current values.\nThe Multiple R-Squared values are high for both models, indicating a good fit. The Adjusted R-squared is slightly higher for VAR(10) than for VAR(8), which suggests that the additional lags in VAR(10) are providing useful information. The standard errors of the residuals are slightly lower for VAR(10), indicating a better fit to the data.\n\n\nCross Validation\n\n\nCode\n# Selecting relevant columns and handling NAs with interpolation\ntime_series_data &lt;- na.approx(filtered_data[, c(\"new_deaths\", \"icu_patients\", \"hosp_patients\")])\n\n# Split data into training and testing sets (80-20 split)\nsplit_ratio &lt;- 0.8\nsplit_index &lt;- floor(nrow(time_series_data) * split_ratio)\ntrain_data &lt;- head(time_series_data, split_index)\ntest_data &lt;- tail(time_series_data, nrow(time_series_data) - split_index)\n\n# Initialize vector to store RMSEs\nrmse_values &lt;- numeric(10)\n\n# Loop through each lag order (1 to 10)\nfor (i in 1:10) {\n  model &lt;- VAR(train_data, p = i, type = \"const\")\n  forecasted &lt;- predict(model, n.ahead = nrow(test_data))\n  forecasted_values &lt;- sapply(forecasted$fcst, function(x) x[,1])  # Extracts forecasts\n  \n  # Calculate RMSE and store it\n  rmse_values[i] &lt;- sqrt(mean((test_data - forecasted_values)^2))\n}\n\n# Plotting RMSEs\nplot(1:10, rmse_values, type = \"b\", xlab = \"Lag Order\", ylab = \"RMSE\", main = \"RMSE vs Lag Order\")\n\n\n\n\n\nBecause VAR(4) has the lowest RMSE, it indicates that it might generalize better when making predictions, despite not fitting the training data as closely as VAR(10). My primary goal is forecasting, the model with the lower RMSE might be preferable, as it indicates better predictive performance. Besides, a model with too many lags might overfit the historical data, leading to poorer performance on new data. The lower RMSE of VAR(4) could suggest it is less prone to overfitting compared to VAR(10). Therefore, I would pick VAR(4) to forecast.\n\n\nForecasting\n\n\nCode\nfit1 &lt;- VAR(var_data, p =4, type = \"const\")\nautoplot(forecast(fit1, h = 60))+theme_bw()"
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#var---daily-new-confirmed-deaths-in-the-us-hospital-patients-and-icu-patients",
    "href": "ARIMAX-SARIMAX-VAR.html#var---daily-new-confirmed-deaths-in-the-us-hospital-patients-and-icu-patients",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "VAR - Daily New Confirmed Deaths in the US, Hospital Patients, and ICU Patients",
    "text": "VAR - Daily New Confirmed Deaths in the US, Hospital Patients, and ICU Patients\n\nExploratory Analysis\n\n\nCode\n# Plot Timeseries of all 3 variables\nautoplot(var_data, facets = T) + labs(title = \"Correlation Time Series Plots\")+theme_bw()\n\n\n\n\n\n\n\nCode\n# Plot Pairs\npairs(cbind(New_Deaths=new_deaths_ts, ICU_Patients=icu_ts, Hospital_Patients=hosp_ts))\n\n\n\n\n\nThe “icu_ts” plot shows fluctuations similar to “new_deaths_ts” but with less pronounced peaks, suggesting that ICU admissions follow a similar trend as deaths but might not correlate perfectly. The “hosp_ts” plot likely shows the number of hospital patients over time, which, like the ICU data, exhibits waves of increases and decreases. The pattern might follow a similar trend to the ICU and death data, potentially with a lead time as hospital admissions typically precede ICU admissions and deaths.\nThe scatterplot between “New_Deaths” and “ICU_Patients” might show a positive correlation, indicated by the clustering of points along an upward trajectory. This suggests that as ICU admissions increase, the number of deaths also increases, which is expected as more severe cases lead to higher mortality. The scatterplot between “ICU_Patients” and “Hospital_Patients” probably shows a strong positive correlation, as a high number of hospital admissions could lead to more patients requiring ICU care.\n\n\nFitting VAR(p)\n\n\nCode\nVARselect(var_data, lag.max = 10, type = \"const\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      8      8     10 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 3.721412e+01 3.659801e+01 3.636002e+01 3.623836e+01 3.606997e+01\nHQ(n)  3.724355e+01 3.664952e+01 3.643359e+01 3.633401e+01 3.618769e+01\nSC(n)  3.729036e+01 3.673143e+01 3.655061e+01 3.648614e+01 3.637492e+01\nFPE(n) 1.451738e+16 7.840012e+15 6.179557e+15 5.471747e+15 4.623790e+15\n                  6            7            8            9           10\nAIC(n) 3.560885e+01 3.530470e+01 3.510303e+01 3.508653e+01 3.507037e+01\nHQ(n)  3.574864e+01 3.546656e+01 3.528697e+01 3.529254e+01 3.529846e+01\nSC(n)  3.597098e+01 3.572401e+01 3.557952e+01 3.562020e+01 3.566122e+01\nFPE(n) 2.915694e+15 2.151099e+15 1.758284e+15 1.729570e+15 1.701919e+15\n\n\n\nSummary VAR(10)Summary VAR(8)\n\n\n\n\nCode\nsummary(vars::VAR(var_data, p=10, type=c('const')))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: new_deaths_ts, icu_ts, hosp_ts \nDeterministic variables: const \nSample size: 721 \nLog Likelihood: -15619.034 \nRoots of the characteristic polynomial:\n0.9888 0.9888 0.9855 0.9855 0.9771 0.9771 0.9593 0.9593 0.9329 0.8817 0.8817 0.8768 0.8768 0.8729 0.8729 0.8317 0.8317 0.817 0.817 0.8146 0.8146 0.7998 0.7998 0.7626 0.7128 0.4857 0.4782 0.4782 0.4084 0.2348\nCall:\nvars::VAR(y = var_data, p = 10, type = c(\"const\"))\n\n\nEstimation results for equation new_deaths_ts: \n============================================== \nnew_deaths_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1    0.327715   0.037994   8.625  &lt; 2e-16 ***\nicu_ts.l1          -0.020742   0.081921  -0.253 0.800188    \nhosp_ts.l1         -0.057217   0.014777  -3.872 0.000118 ***\nnew_deaths_ts.l2   -0.087889   0.039796  -2.209 0.027537 *  \nicu_ts.l2           0.019591   0.107480   0.182 0.855419    \nhosp_ts.l2          0.083567   0.021184   3.945  8.8e-05 ***\nnew_deaths_ts.l3    0.002839   0.040004   0.071 0.943444    \nicu_ts.l3           0.173186   0.104094   1.664 0.096618 .  \nhosp_ts.l3         -0.020234   0.021187  -0.955 0.339903    \nnew_deaths_ts.l4    0.007016   0.035516   0.198 0.843449    \nicu_ts.l4          -0.178972   0.104009  -1.721 0.085746 .  \nhosp_ts.l4          0.028808   0.021156   1.362 0.173737    \nnew_deaths_ts.l5   -0.109857   0.034449  -3.189 0.001492 ** \nicu_ts.l5           0.048250   0.104011   0.464 0.642869    \nhosp_ts.l5         -0.045305   0.021013  -2.156 0.031429 *  \nnew_deaths_ts.l6    0.200848   0.034387   5.841  8.0e-09 ***\nicu_ts.l6           0.018764   0.102552   0.183 0.854872    \nhosp_ts.l6          0.009816   0.020949   0.469 0.639526    \nnew_deaths_ts.l7    0.457076   0.035449  12.894  &lt; 2e-16 ***\nicu_ts.l7          -0.034151   0.102600  -0.333 0.739345    \nhosp_ts.l7         -0.004347   0.020889  -0.208 0.835198    \nnew_deaths_ts.l8   -0.022132   0.039590  -0.559 0.576321    \nicu_ts.l8          -0.014865   0.103011  -0.144 0.885299    \nhosp_ts.l8         -0.006256   0.020752  -0.301 0.763159    \nnew_deaths_ts.l9   -0.087018   0.038837  -2.241 0.025370 *  \nicu_ts.l9          -0.081341   0.103077  -0.789 0.430307    \nhosp_ts.l9          0.031191   0.020559   1.517 0.129696    \nnew_deaths_ts.l10   0.082077   0.036355   2.258 0.024278 *  \nicu_ts.l10          0.077634   0.075250   1.032 0.302585    \nhosp_ts.l10        -0.015651   0.014698  -1.065 0.287322    \nconst             -47.786431  22.706221  -2.105 0.035691 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 302.1 on 690 degrees of freedom\nMultiple R-Squared: 0.9025, Adjusted R-squared: 0.8982 \nF-statistic: 212.8 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation icu_ts: \n======================================= \nicu_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1   0.0029745  0.0209510   0.142 0.887141    \nicu_ts.l1          0.9272595  0.0451731  20.527  &lt; 2e-16 ***\nhosp_ts.l1         0.0176839  0.0081483   2.170 0.030329 *  \nnew_deaths_ts.l2   0.0473695  0.0219443   2.159 0.031224 *  \nicu_ts.l2          0.1736561  0.0592671   2.930 0.003501 ** \nhosp_ts.l2        -0.0002385  0.0116813  -0.020 0.983720    \nnew_deaths_ts.l3   0.0181799  0.0220592   0.824 0.410144    \nicu_ts.l3         -0.0780128  0.0574004  -1.359 0.174559    \nhosp_ts.l3         0.0023755  0.0116833   0.203 0.838942    \nnew_deaths_ts.l4   0.0419309  0.0195843   2.141 0.032620 *  \nicu_ts.l4          0.0022318  0.0573532   0.039 0.968971    \nhosp_ts.l4        -0.0244430  0.0116658  -2.095 0.036511 *  \nnew_deaths_ts.l5  -0.0105252  0.0189961  -0.554 0.579709    \nicu_ts.l5          0.0604113  0.0573541   1.053 0.292570    \nhosp_ts.l5         0.0173592  0.0115873   1.498 0.134560    \nnew_deaths_ts.l6  -0.0379871  0.0189619  -2.003 0.045531 *  \nicu_ts.l6         -0.0420519  0.0565499  -0.744 0.457357    \nhosp_ts.l6         0.0020607  0.0115516   0.178 0.858469    \nnew_deaths_ts.l7  -0.0019781  0.0195474  -0.101 0.919423    \nicu_ts.l7          0.2727134  0.0565763   4.820 1.76e-06 ***\nhosp_ts.l7         0.0043068  0.0115189   0.374 0.708598    \nnew_deaths_ts.l8   0.0208004  0.0218311   0.953 0.341030    \nicu_ts.l8         -0.2315191  0.0568031  -4.076 5.12e-05 ***\nhosp_ts.l8        -0.0231636  0.0114430  -2.024 0.043327 *  \nnew_deaths_ts.l9  -0.0396008  0.0214157  -1.849 0.064864 .  \nicu_ts.l9         -0.1490258  0.0568393  -2.622 0.008937 ** \nhosp_ts.l9        -0.0126608  0.0113369  -1.117 0.264477    \nnew_deaths_ts.l10  0.0091967  0.0200469   0.459 0.646550    \nicu_ts.l10         0.0653495  0.0414950   1.575 0.115743    \nhosp_ts.l10        0.0143484  0.0081049   1.770 0.077110 .  \nconst             44.3890802 12.5208093   3.545 0.000419 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 166.6 on 690 degrees of freedom\nMultiple R-Squared: 0.9996, Adjusted R-squared: 0.9996 \nF-statistic: 5.445e+04 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation hosp_ts: \n======================================== \nhosp_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + new_deaths_ts.l9 + icu_ts.l9 + hosp_ts.l9 + new_deaths_ts.l10 + icu_ts.l10 + hosp_ts.l10 + const \n\n                    Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1   -0.114957   0.115580  -0.995  0.32027    \nicu_ts.l1           0.089584   0.249205   0.359  0.71935    \nhosp_ts.l1          1.104448   0.044952  24.570  &lt; 2e-16 ***\nnew_deaths_ts.l2    0.158645   0.121060   1.310  0.19047    \nicu_ts.l2           0.908556   0.326957   2.779  0.00560 ** \nhosp_ts.l2         -0.056499   0.064442  -0.877  0.38093    \nnew_deaths_ts.l3    0.262472   0.121694   2.157  0.03136 *  \nicu_ts.l3          -1.264251   0.316659  -3.992 7.24e-05 ***\nhosp_ts.l3          0.113914   0.064453   1.767  0.07761 .  \nnew_deaths_ts.l4    0.193323   0.108040   1.789  0.07400 .  \nicu_ts.l4          -0.118625   0.316399  -0.375  0.70783    \nhosp_ts.l4         -0.089850   0.064357  -1.396  0.16313    \nnew_deaths_ts.l5    0.053538   0.104795   0.511  0.60960    \nicu_ts.l5           0.494735   0.316404   1.564  0.11837    \nhosp_ts.l5          0.087882   0.063923   1.375  0.16964    \nnew_deaths_ts.l6   -0.398812   0.104607  -3.812  0.00015 ***\nicu_ts.l6          -0.857383   0.311967  -2.748  0.00615 ** \nhosp_ts.l6         -0.007494   0.063726  -0.118  0.90642    \nnew_deaths_ts.l7    0.281324   0.107836   2.609  0.00928 ** \nicu_ts.l7           0.929703   0.312113   2.979  0.00300 ** \nhosp_ts.l7          0.121091   0.063546   1.906  0.05712 .  \nnew_deaths_ts.l8   -0.027177   0.120435  -0.226  0.82154    \nicu_ts.l8           0.021918   0.313364   0.070  0.94426    \nhosp_ts.l8         -0.158762   0.063127  -2.515  0.01213 *  \nnew_deaths_ts.l9   -0.052870   0.118144  -0.448  0.65465    \nicu_ts.l9          -0.386696   0.313564  -1.233  0.21791    \nhosp_ts.l9         -0.280582   0.062542  -4.486 8.49e-06 ***\nnew_deaths_ts.l10   0.003540   0.110592   0.032  0.97447    \nicu_ts.l10          0.214646   0.228914   0.938  0.34874    \nhosp_ts.l10         0.145883   0.044712   3.263  0.00116 ** \nconst             210.933125  69.073198   3.054  0.00235 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 918.9 on 690 degrees of freedom\nMultiple R-Squared: 0.9994, Adjusted R-squared: 0.9993 \nF-statistic: 3.539e+04 on 30 and 690 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n              new_deaths_ts icu_ts hosp_ts\nnew_deaths_ts         91242  -3743   -9817\nicu_ts                -3743  27744   83031\nhosp_ts               -9817  83031  844356\n\nCorrelation matrix of residuals:\n              new_deaths_ts  icu_ts  hosp_ts\nnew_deaths_ts       1.00000 -0.0744 -0.03537\nicu_ts             -0.07440  1.0000  0.54249\nhosp_ts            -0.03537  0.5425  1.00000\n\n\n\n\n\n\nCode\nsummary(vars::VAR(var_data, p=8, type=c('const')))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: new_deaths_ts, icu_ts, hosp_ts \nDeterministic variables: const \nSample size: 723 \nLog Likelihood: -15719.638 \nRoots of the characteristic polynomial:\n0.9874 0.9874 0.9839 0.9839 0.9781 0.9781 0.945 0.945 0.9431 0.8818 0.8818 0.8519 0.8519 0.8372 0.8372 0.8332 0.8332 0.8208 0.8208 0.7678 0.7678 0.7261 0.7261 0.09812\nCall:\nvars::VAR(y = var_data, p = 8, type = c(\"const\"))\n\n\nEstimation results for equation new_deaths_ts: \n============================================== \nnew_deaths_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                   Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  3.254e-01  3.784e-02   8.598  &lt; 2e-16 ***\nicu_ts.l1        -5.010e-02  7.365e-02  -0.680 0.496566    \nhosp_ts.l1       -5.794e-02  1.407e-02  -4.119 4.27e-05 ***\nnew_deaths_ts.l2 -1.164e-01  3.508e-02  -3.318 0.000955 ***\nicu_ts.l2         2.950e-02  1.026e-01   0.287 0.773897    \nhosp_ts.l2        8.572e-02  2.083e-02   4.115 4.33e-05 ***\nnew_deaths_ts.l3  3.116e-02  3.411e-02   0.914 0.361233    \nicu_ts.l3         1.901e-01  1.025e-01   1.854 0.064118 .  \nhosp_ts.l3       -1.770e-02  2.083e-02  -0.850 0.395642    \nnew_deaths_ts.l4  2.707e-02  3.386e-02   0.799 0.424374    \nicu_ts.l4        -1.882e-01  1.020e-01  -1.846 0.065324 .  \nhosp_ts.l4        2.321e-02  2.069e-02   1.122 0.262432    \nnew_deaths_ts.l5 -1.176e-01  3.404e-02  -3.455 0.000584 ***\nicu_ts.l5         3.541e-02  1.015e-01   0.349 0.727287    \nhosp_ts.l5       -4.386e-02  2.076e-02  -2.113 0.034984 *  \nnew_deaths_ts.l6  2.026e-01  3.413e-02   5.937 4.57e-09 ***\nicu_ts.l6         4.782e-02  1.010e-01   0.473 0.636167    \nhosp_ts.l6        8.415e-03  2.070e-02   0.407 0.684465    \nnew_deaths_ts.l7  4.797e-01  3.422e-02  14.017  &lt; 2e-16 ***\nicu_ts.l7        -3.686e-02  1.009e-01  -0.365 0.714865    \nhosp_ts.l7        2.031e-04  2.039e-02   0.010 0.992054    \nnew_deaths_ts.l8 -4.895e-02  3.621e-02  -1.352 0.176888    \nicu_ts.l8        -2.054e-02  7.163e-02  -0.287 0.774370    \nhosp_ts.l8        5.997e-03  1.409e-02   0.425 0.670641    \nconst            -4.196e+01  2.206e+01  -1.902 0.057543 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 302.8 on 698 degrees of freedom\nMultiple R-Squared: 0.9008, Adjusted R-squared: 0.8974 \nF-statistic: 264.2 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation icu_ts: \n======================================= \nicu_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  0.006811   0.021555   0.316 0.752103    \nicu_ts.l1         0.943548   0.041951  22.492  &lt; 2e-16 ***\nhosp_ts.l1        0.016133   0.008012   2.014 0.044439 *  \nnew_deaths_ts.l2  0.031038   0.019981   1.553 0.120788    \nicu_ts.l2         0.118300   0.058455   2.024 0.043375 *  \nhosp_ts.l2       -0.003767   0.011865  -0.317 0.750967    \nnew_deaths_ts.l3  0.005092   0.019427   0.262 0.793293    \nicu_ts.l3        -0.019454   0.058385  -0.333 0.739089    \nhosp_ts.l3        0.011128   0.011863   0.938 0.348534    \nnew_deaths_ts.l4  0.056087   0.019286   2.908 0.003751 ** \nicu_ts.l4        -0.061904   0.058076  -1.066 0.286831    \nhosp_ts.l4       -0.037058   0.011786  -3.144 0.001736 ** \nnew_deaths_ts.l5 -0.014330   0.019388  -0.739 0.460080    \nicu_ts.l5         0.133387   0.057808   2.307 0.021323 *  \nhosp_ts.l5        0.027497   0.011824   2.325 0.020333 *  \nnew_deaths_ts.l6 -0.037338   0.019439  -1.921 0.055166 .  \nicu_ts.l6        -0.034694   0.057546  -0.603 0.546778    \nhosp_ts.l6       -0.001643   0.011789  -0.139 0.889171    \nnew_deaths_ts.l7 -0.004833   0.019492  -0.248 0.804242    \nicu_ts.l7         0.204202   0.057448   3.555 0.000404 ***\nhosp_ts.l7        0.014777   0.011612   1.273 0.203617    \nnew_deaths_ts.l8  0.019637   0.020625   0.952 0.341385    \nicu_ts.l8        -0.281813   0.040799  -6.907 1.11e-11 ***\nhosp_ts.l8       -0.029890   0.008028  -3.723 0.000213 ***\nconst            46.701628  12.563784   3.717 0.000218 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 172.5 on 698 degrees of freedom\nMultiple R-Squared: 0.9995, Adjusted R-squared: 0.9995 \nF-statistic: 6.349e+04 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation hosp_ts: \n======================================== \nhosp_ts = new_deaths_ts.l1 + icu_ts.l1 + hosp_ts.l1 + new_deaths_ts.l2 + icu_ts.l2 + hosp_ts.l2 + new_deaths_ts.l3 + icu_ts.l3 + hosp_ts.l3 + new_deaths_ts.l4 + icu_ts.l4 + hosp_ts.l4 + new_deaths_ts.l5 + icu_ts.l5 + hosp_ts.l5 + new_deaths_ts.l6 + icu_ts.l6 + hosp_ts.l6 + new_deaths_ts.l7 + icu_ts.l7 + hosp_ts.l7 + new_deaths_ts.l8 + icu_ts.l8 + hosp_ts.l8 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nnew_deaths_ts.l1  -0.10799    0.11767  -0.918 0.359091    \nicu_ts.l1          0.17936    0.22901   0.783 0.433765    \nhosp_ts.l1         1.11381    0.04374  25.466  &lt; 2e-16 ***\nnew_deaths_ts.l2   0.11474    0.10907   1.052 0.293167    \nicu_ts.l2          0.65129    0.31910   2.041 0.041628 *  \nhosp_ts.l2        -0.09295    0.06477  -1.435 0.151727    \nnew_deaths_ts.l3   0.23116    0.10605   2.180 0.029606 *  \nicu_ts.l3         -1.16170    0.31872  -3.645 0.000287 ***\nhosp_ts.l3         0.17041    0.06476   2.631 0.008689 ** \nnew_deaths_ts.l4   0.29553    0.10528   2.807 0.005140 ** \nicu_ts.l4         -0.18435    0.31703  -0.581 0.561105    \nhosp_ts.l4        -0.14710    0.06434  -2.286 0.022540 *  \nnew_deaths_ts.l5   0.02368    0.10584   0.224 0.823010    \nicu_ts.l5          0.76787    0.31557   2.433 0.015213 *  \nhosp_ts.l5         0.14955    0.06455   2.317 0.020797 *  \nnew_deaths_ts.l6  -0.41670    0.10612  -3.927 9.46e-05 ***\nicu_ts.l6         -0.96670    0.31414  -3.077 0.002171 ** \nhosp_ts.l6        -0.04013    0.06435  -0.624 0.533087    \nnew_deaths_ts.l7   0.16365    0.10640   1.538 0.124495    \nicu_ts.l7          0.70499    0.31360   2.248 0.024887 *  \nhosp_ts.l7         0.13112    0.06339   2.068 0.038967 *  \nnew_deaths_ts.l8   0.04158    0.11259   0.369 0.712004    \nicu_ts.l8          0.04308    0.22272   0.193 0.846670    \nhosp_ts.l8        -0.30499    0.04382  -6.959 7.90e-12 ***\nconst            214.50350   68.58498   3.128 0.001836 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 941.5 on 698 degrees of freedom\nMultiple R-Squared: 0.9993, Adjusted R-squared: 0.9993 \nF-statistic: 4.214e+04 on 24 and 698 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n              new_deaths_ts icu_ts hosp_ts\nnew_deaths_ts         91687  -3044  -12186\nicu_ts                -3044  29744   87327\nhosp_ts              -12186  87327  886383\n\nCorrelation matrix of residuals:\n              new_deaths_ts   icu_ts  hosp_ts\nnew_deaths_ts       1.00000 -0.05829 -0.04275\nicu_ts             -0.05829  1.00000  0.53782\nhosp_ts            -0.04275  0.53782  1.00000\n\n\n\n\n\nThe log likelihood is higher (closer to zero) for VAR(10) (-15619.034) compared to VAR(8) (-15719.638), suggesting that the VAR(10) model may fit the data better. In both models, many of the lagged values of new_deaths_ts, icu_ts, and hosp_ts are significant predictors of themselves and each other, which suggests that past values have a substantial influence on current values.\nThe Multiple R-Squared values are high for both models, indicating a good fit. The Adjusted R-squared is slightly higher for VAR(10) than for VAR(8), which suggests that the additional lags in VAR(10) are providing useful information. The standard errors of the residuals are slightly lower for VAR(10), indicating a better fit to the data.\n\n\nCross Validation\n\n\nCode\n# Selecting relevant columns and handling NAs with interpolation\ntime_series_data &lt;- na.approx(filtered_data[, c(\"new_deaths\", \"icu_patients\", \"hosp_patients\")])\n\n# Split data into training and testing sets (80-20 split)\nsplit_ratio &lt;- 0.8\nsplit_index &lt;- floor(nrow(time_series_data) * split_ratio)\ntrain_data &lt;- head(time_series_data, split_index)\ntest_data &lt;- tail(time_series_data, nrow(time_series_data) - split_index)\n\n# Initialize vector to store RMSEs\nrmse_values &lt;- numeric(10)\n\n# Loop through each lag order (1 to 10)\nfor (i in 1:10) {\n  model &lt;- VAR(train_data, p = i, type = \"const\")\n  forecasted &lt;- predict(model, n.ahead = nrow(test_data))\n  forecasted_values &lt;- sapply(forecasted$fcst, function(x) x[,1])  # Extracts forecasts\n  \n  # Calculate RMSE and store it\n  rmse_values[i] &lt;- sqrt(mean((test_data - forecasted_values)^2))\n}\n\n# Plotting RMSEs\nplot(1:10, rmse_values, type = \"b\", xlab = \"Lag Order\", ylab = \"RMSE\", main = \"RMSE vs Lag Order\")\n\n\n\n\n\nBecause VAR(4) has the lowest RMSE, it indicates that it might generalize better when making predictions, despite not fitting the training data as closely as VAR(10). My primary goal is forecasting, the model with the lower RMSE might be preferable, as it indicates better predictive performance. Besides, a model with too many lags might overfit the historical data, leading to poorer performance on new data. The lower RMSE of VAR(4) could suggest it is less prone to overfitting compared to VAR(10). Therefore, I would pick VAR(4) to forecast.\n\n\nForecasting\n\n\nCode\nfit1 &lt;- VAR(var_data, p =4, type = \"const\")\nautoplot(forecast(fit1, h = 60))+theme_bw()"
  },
  {
    "objectID": "ARIMAX-SARIMAX-VAR.html#arimax---daily-new-confirmed-cases-in-the-us-and-people-vaccinated",
    "href": "ARIMAX-SARIMAX-VAR.html#arimax---daily-new-confirmed-cases-in-the-us-and-people-vaccinated",
    "title": "ARIMAX/SARIMAX/VAR Models",
    "section": "ARIMAX - Daily New Confirmed Cases in the US, and People Vaccinated",
    "text": "ARIMAX - Daily New Confirmed Cases in the US, and People Vaccinated\n\nExploratory Analysis\n\n\nCode\n# Plot Timeseries of all 3 variables\nautoplot(var_data2, facets = T) + labs(title = \"Correlation Time Series Plots\")+theme_bw()\n\n\n\n\n\n\n\nAuto.Arima\n\n\nCode\n# Fit ARIMAX model\narimax_model &lt;- auto.arima(new_cases_ts, xreg = vacc_ts)\n\nsummary(arimax_model)\n\n\nSeries: new_cases_ts \nRegression with ARIMA(3,1,2) errors \n\nCoefficients:\n         ar1      ar2      ar3      ma1     ma2    xreg\n      0.8896  -0.5252  -0.2790  -1.3627  0.7798  -5e-04\ns.e.  0.0641   0.0672   0.0713   0.0552  0.0415   6e-04\n\nsigma^2 = 429712594:  log likelihood = -4143.79\nAIC=8301.57   AICc=8301.89   BIC=8328.87\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE       MASE\nTraining set -105.7928 20530.32 12979.24 -4.348986 17.59038 0.07136468\n                    ACF1\nTraining set -0.06542263\n\n\nThe auto.arima function in R suggests an ARIMAX(3,1,2) model as the best fit for the data.\n\n\nFitting ARIMAX Model Manually\n\n\nCode\nfit.reg &lt;- lm(new_cases ~ people_vaccinated, data=filtered_data)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = new_cases ~ people_vaccinated, data = filtered_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-114533  -77412  -39840   21840 1145560 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.114e+05  1.529e+04   7.284 9.07e-13 ***\npeople_vaccinated 3.478e-05  7.187e-05   0.484    0.629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 146800 on 674 degrees of freedom\nMultiple R-squared:  0.0003474, Adjusted R-squared:  -0.001136 \nF-statistic: 0.2342 on 1 and 674 DF,  p-value: 0.6286\n\n\n\nACF Plot of ResidualsPACF Plot of Residuals\n\n\n\n\nCode\nggAcf(residuals(fit.reg))\n\n\n\n\n\n\n\n\n\nCode\nggPacf(residuals(fit.reg))\n\n\n\n\n\n\n\n\n\nACF Plot of first-differenced ResidualsPACF Plot of first-differenced Residuals\n\n\n\n\nCode\nggAcf(residuals(fit.reg) %&gt;% diff())\n\n\n\n\n\n\n\n\n\nCode\nggPacf(residuals(fit.reg) %&gt;% diff())\n\n\n\n\n\n\n\n\nFinding the model parameters.\n\n\nCode\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*12),nrow=12) # roughly nrow = 3x4x1\n\n\nfor (p in 1:4) # p=0,1,2,3\n{\n  for(q in 1:3) # q=0,1,2\n  {\n    for(d in 1) # d=1\n    {\n      \n      if(p-1+d+q-1&lt;=8)\n      {\n        \n        model&lt;- Arima(new_deaths_ts,order=c(p-1,d,q-1)) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\ntemp &lt;- temp[order(temp$BIC, decreasing = FALSE),] \nknitr::kable(temp)\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n12\n3\n1\n2\n10848.65\n10876.21\n10848.77\n\n\n9\n2\n1\n2\n10853.64\n10876.60\n10853.72\n\n\n11\n3\n1\n1\n10961.86\n10984.83\n10961.95\n\n\n8\n2\n1\n1\n10978.49\n10996.87\n10978.55\n\n\n3\n0\n1\n2\n11027.00\n11040.78\n11027.04\n\n\n6\n1\n1\n2\n11026.77\n11045.14\n11026.83\n\n\n5\n1\n1\n1\n11097.38\n11111.16\n11097.41\n\n\n10\n3\n1\n0\n11152.25\n11170.63\n11152.31\n\n\n7\n2\n1\n0\n11173.88\n11187.65\n11173.91\n\n\n1\n0\n1\n0\n11254.47\n11259.07\n11254.48\n\n\n2\n0\n1\n1\n11256.19\n11265.37\n11256.20\n\n\n4\n1\n1\n0\n11256.38\n11265.57\n11256.40\n\n\n\n\n\n\n\nCode\n# Extract lowest AIC\ntemp[which.min(temp$AIC),] \n\n\n   p d q      AIC      BIC     AICc\n12 3 1 2 10848.65 10876.21 10848.77\n\n\nCode\n# Extract lowest BIC\ntemp[which.min(temp$BIC),]\n\n\n   p d q      AIC      BIC     AICc\n12 3 1 2 10848.65 10876.21 10848.77\n\n\nCode\n# Extract lowest AICc\ntemp[which.min(temp$AICc),]\n\n\n   p d q      AIC      BIC     AICc\n12 3 1 2 10848.65 10876.21 10848.77\n\n\nBoth give us the same results: ARIMA(3,1,2) is the best model.\n\n\nCode\nfit &lt;- Arima(new_cases_ts,order=c(3,1,2),xreg=vacc_ts)\nsummary(fit)\n\n\nSeries: new_cases_ts \nRegression with ARIMA(3,1,2) errors \n\nCoefficients:\n         ar1      ar2      ar3      ma1     ma2    xreg\n      0.8896  -0.5252  -0.2790  -1.3627  0.7798  -5e-04\ns.e.  0.0641   0.0672   0.0713   0.0552  0.0415   6e-04\n\nsigma^2 = 429712594:  log likelihood = -4143.79\nAIC=8301.57   AICc=8301.89   BIC=8328.87\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE       MASE\nTraining set -105.7928 20530.32 12979.24 -4.348986 17.59038 0.07136468\n                    ACF1\nTraining set -0.06542263\n\n\n\n\nModel Diagnostics\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(3,1,2) errors\nQ* = 375.72, df = 68, p-value &lt; 2.2e-16\n\nModel df: 5.   Total lags used: 73\n\n\n\n\nForecasting\n\n\nCode\nfcast_cases = forecast(fit, xreg = vacc_ts, h = 30)\nautoplot(fcast_cases) + xlab(\"Date\")+ylab(\"New Cases\")+ggtitle(\"Predictions\")"
  },
  {
    "objectID": "Conclusion.html#reference",
    "href": "Conclusion.html#reference",
    "title": "Conclusion",
    "section": "Reference",
    "text": "Reference\nEkinci A. Modelling and forecasting of growth rate of new COVID-19 cases in top nine affected countries: Considering conditional variance and asymmetric effect. Chaos Solitons Fractals. 2021 Oct;151:111227. doi: 10.1016/j.chaos.2021.111227. Epub 2021 Jul 8. PMID: 34253942; PMCID: PMC8264537. https://pubmed.ncbi.nlm.nih.gov/34253942/\nKamalov F, Thabtah F. Forecasting Covid-19: SARMA-ARCH approach. Health Technol (Berl). 2021;11(5):1139-1148. doi: 10.1007/s12553-021-00587-x. Epub 2021 Aug 18. PMID: 34422542; PMCID: PMC8370786. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8370786/\nMthethwa N, Chifurira R, Chinhamu K. Estimating the risk of SARS-CoV-2 deaths using a Markov switching-volatility model combined with heavy-tailed distributions for South Africa. BMC Public Health. 2022 Oct 7;22(1):1873. doi: 10.1186/s12889-022-14249-8. PMID: 36207700; PMCID: PMC9540091. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9540091/\nAbate SM, Ahmed Ali S, Mantfardo B, Basu B. Rate of Intensive Care Unit admission and outcomes among patients with coronavirus: A systematic review and Meta-analysis. PLoS One. 2020 Jul 10;15(7):e0235653. doi: 10.1371/journal.pone.0235653. PMID: 32649661; PMCID: PMC7351172. https://pubmed.ncbi.nlm.nih.gov/32649661/\nLavrentieva A, Kaimakamis E, Voutsas V, Bitzani M. An observational study on factors associated with ICU mortality in Covid-19 patients and critical review of the literature. Sci Rep. 2023 May 13;13(1):7804. doi: 10.1038/s41598-023-34613-x. PMID: 37179397; PMCID: PMC10182846. https://pubmed.ncbi.nlm.nih.gov/37179397/\nKowsar R, Rahimi AM, Sroka M, Mansouri A, Sadeghi K, Bonakdar E, Kateb SF, Mahdavi AH. Risk of mortality in COVID-19 patients: a meta- and network analysis. Sci Rep. 2023 Feb 6;13(1):2138. doi: 10.1038/s41598-023-29364-8. PMID: 36747045; PMCID: PMC9901837. https://pubmed.ncbi.nlm.nih.gov/36747045/\nLi Z, Liu X, Liu M, Wu Z, Liu Y, Li W, Liu M, Wang X, Gao B, Luo Y, Li X, Tao L, Wang W, Guo X. The Effect of the COVID-19 Vaccine on Daily Cases and Deaths Based on Global Vaccine Data. Vaccines (Basel). 2021 Nov 15;9(11):1328. doi: 10.3390/vaccines9111328. PMID: 34835259; PMCID: PMC8622191. https://pubmed.ncbi.nlm.nih.gov/34835259/\nChen, X., Huang, H., Ju, J. et al. Impact of vaccination on the COVID-19 pandemic in U.S. states. Sci Rep 12, 1554 (2022). https://doi.org/10.1038/s41598-022-05498-z Huang C, Yang L, Pan J, Xu X, Peng R. Correlation between vaccine coverage and the COVID-19 pandemic throughout the world: Based on real-world data. J Med Virol. 2022 May;94(5):2181-2187. doi: 10.1002/jmv.27609. Epub 2022 Feb 4. PMID: 35075651; PMCID: PMC9015592. https://pubmed.ncbi.nlm.nih.gov/35075651/\nMoore, S., Hill, E.M., Dyson, L. et al. Retrospectively modeling the effects of increased global vaccine sharing on the COVID-19 pandemic. Nat Med 28, 2416–2423 (2022). https://doi.org/10.1038/s41591-022-02064-y"
  }
]