<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DSAN-5600 Project - Yifan Hu - Deep Learning for Time Series</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Deep-Learning.html">Deep Learning for TS</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">DSAN-5600 Project - Yifan Hu</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://github.com/hyf112/DSAN-5600-Project.git">
            Source Code
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Data-Sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Sources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Data-Visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Exploratory-Data-Analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exploratory Data Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ARMA-ARIMA-SARIMA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ARMA/ARIMA/SARIMA Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ARIMAX-SARIMAX-VAR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ARIMAX/SARIMAX/VAR Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Financial-Models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Financial Time Series Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Deep-Learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Deep Learning for TS</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#deep-learning-for-daily-new-confirmed-cases-in-the-us" id="toc-deep-learning-for-daily-new-confirmed-cases-in-the-us" class="nav-link" data-scroll-target="#deep-learning-for-daily-new-confirmed-cases-in-the-us">Deep Learning for Daily New Confirmed Cases in the US</a>
  <ul class="collapse">
  <li><a href="#recurrent-neural-network" id="toc-recurrent-neural-network" class="nav-link" data-scroll-target="#recurrent-neural-network">Recurrent Neural Network</a></li>
  <li><a href="#gated-recurrent-unit" id="toc-gated-recurrent-unit" class="nav-link" data-scroll-target="#gated-recurrent-unit">Gated Recurrent unit</a></li>
  <li><a href="#long-short-term-memory" id="toc-long-short-term-memory" class="nav-link" data-scroll-target="#long-short-term-memory">Long Short Term Memory</a></li>
  <li><a href="#comparing" id="toc-comparing" class="nav-link" data-scroll-target="#comparing">Comparing</a></li>
  </ul></li>
  <li><a href="#deep-learning-for-daily-new-confirmed-deaths-in-the-us" id="toc-deep-learning-for-daily-new-confirmed-deaths-in-the-us" class="nav-link" data-scroll-target="#deep-learning-for-daily-new-confirmed-deaths-in-the-us">Deep Learning for Daily New Confirmed Deaths in the US</a>
  <ul class="collapse">
  <li><a href="#recurrent-neural-network-1" id="toc-recurrent-neural-network-1" class="nav-link" data-scroll-target="#recurrent-neural-network-1">Recurrent Neural Network</a></li>
  <li><a href="#gated-recurrent-unit-1" id="toc-gated-recurrent-unit-1" class="nav-link" data-scroll-target="#gated-recurrent-unit-1">Gated Recurrent unit</a></li>
  <li><a href="#long-short-term-memory-1" id="toc-long-short-term-memory-1" class="nav-link" data-scroll-target="#long-short-term-memory-1">Long Short Term Memory</a></li>
  <li><a href="#comparing-1" id="toc-comparing-1" class="nav-link" data-scroll-target="#comparing-1">Comparing</a></li>
  </ul></li>
  <li><a href="#deep-learning-vs-traditional-ts-model" id="toc-deep-learning-vs-traditional-ts-model" class="nav-link" data-scroll-target="#deep-learning-vs-traditional-ts-model">Deep Learning vs Traditional TS Model</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Deep Learning for Time Series</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In this section, I try to create a model that can predict the number of daily new cases and deaths of COVID-19 by leveraging techniques such as RNN, LSTM, GRU, and regularization.</p>
</section>
<section id="deep-learning-for-daily-new-confirmed-cases-in-the-us" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-for-daily-new-confirmed-cases-in-the-us">Deep Learning for Daily New Confirmed Cases in the US</h2>
<section id="recurrent-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network">Recurrent Neural Network</h3>
<p>The RMSE values for training are similar for both models, which suggests that regularization hasn’t impacted the ability of the model to fit the training data much. However, the test RMSE is slightly better in the regularized model (0.301) compared to the non-regularized one (0.312). This improvement in test RMSE indicates that the regularized model generalizes slightly better to unseen data.</p>
<p>The lower validation loss as compared to the training loss in both models is not typical and could suggest that the validation set is easier for the model to predict than the training set.</p>
<p>Overall, the regularization seems to be serving its purpose by slightly improving the model’s generalization, as indicated by the lower test RMSE. However, the difference is minimal, suggesting that the model was not significantly overfitting the training data even without regularization.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">RNN</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">RNN including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RNN</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>,kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple neural network layer</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    model.add(SimpleRNN(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>              kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>,validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>yhat_d <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [y[<span class="dv">0</span>] <span class="cf">for</span> y <span class="kw">in</span> Yval]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>rmse_table <span class="op">=</span> {</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model'</span>: [<span class="st">'Recurrent Neural Network'</span>],</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'training_rmse'</span>: [train_rmse],</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'testing_rmse'</span>: [test_rmse]</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-8-output-1.png" width="571" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.715 RMSE
Test RMSE: 0.283 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network with regularization</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>), dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model (with L1L2 Regularization)'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>yhat_d_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Recurrent Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-9-output-1.png" width="717" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.721 RMSE
Test RMSE: 0.311 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-unit" class="level3">
<h3 class="anchored" data-anchor-id="gated-recurrent-unit">Gated Recurrent unit</h3>
<p>The GRU model without regularization achieves a lower training RMSE (0.572) compared to the regularized model (1.004), which is consistent with expectations as regularization typically increases training error due to the added constraints. However, the test RMSE is nearly identical for both models, with the regularized model showing a marginally better test RMSE (0.403) compared to the non-regularized model (0.408).</p>
<p>This marginal improvement in test RMSE for the regularized model suggests that regularization has helped to improve the generalization of the model to new data, although the effect is slight. This effect is not as clearly reflected in the validation loss, which is consistently low for both models. Again, the lower validation loss compared to the training loss is unusual and suggests potential issues with the validation process or data.</p>
<p>In conclusion, while the regularized GRU model exhibits higher training loss due to the effects of L1L2 regularization, it achieves a slightly better generalization on the test data, as evidenced by the lower test RMSE.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">GRU</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">GRU including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple GRU layer</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'sgd'</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Training and evaluating a GRU-based model</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>yhat_gru <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network'</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-10-output-1.png" width="571" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.575 RMSE
Test RMSE: 0.404 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU-based model with regularization</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>],dropout_rate <span class="op">=</span> <span class="fl">0.2</span>,  kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model (with L1L2 Regularization)'</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>yhat_gru_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-11-output-1.png" width="573" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.555 RMSE
Test RMSE: 0.413 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="long-short-term-memory" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory">Long Short Term Memory</h3>
<p>With the LSTM models, we see that regularization has led to a slight increase in both the training and test RMSE. The training RMSE has increased from 0.481 to 0.513, which could be due to the regularization term adding a penalty to the cost function and therefore making it harder for the model to fit the training data perfectly. This is a typical effect of regularization, as it trades off variance for bias, leading to a less complex model that might not capture all the nuances in the training data.</p>
<p>However, contrary to the usual benefits of regularization, the test RMSE has also increased, albeit slightly, from 0.302 to 0.309. This suggests that for this particular dataset and model architecture, the regularization has not provided a benefit in terms of generalization to the test data. It’s possible that the non-regularized model was not overfitting to begin with, so the regularization did not lead to a performance improvement on the test set. Alternatively, the type and amount of regularization might not be optimal for this problem, and tweaking the regularization parameters could potentially yield different results.</p>
<p>It’s important to note that while the training and validation losses provide a good indication of the model’s learning process, the ultimate measure of performance is the test RMSE, which tells us how well the model is expected to perform on unseen data. The slight increase in test RMSE for the regularized model indicates that in this case, regularization may not be beneficial.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true">LSTM</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false">LSTM including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM Neural Network</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple long short term memory neural network</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(hidden_units,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>], input_shape<span class="op">=</span>input_shape))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"RMSprop"</span>, loss<span class="op">=</span><span class="st">'mae'</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an LSTM neural network</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>yhat_lstm <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network'</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-12-output-1.png" width="579" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.489 RMSE
Test RMSE: 0.303 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM neural network with regularization</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model (with L1L2 Regularization)'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>yhat_lstm_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-13-output-1.png" width="585" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.490 RMSE
Test RMSE: 0.315 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="comparing" class="level3">
<h3 class="anchored" data-anchor-id="comparing">Comparing</h3>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">training_rmse</th>
<th data-quarto-table-cell-role="th">testing_rmse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Recurrent Neural Network</td>
<td>0.714508</td>
<td>0.282691</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Recurrent Neural Network (with L1L2 Regulariza...</td>
<td>0.721246</td>
<td>0.311048</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>GRU Neural Network</td>
<td>0.575008</td>
<td>0.404393</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>GRU Neural Network (with L1L2 Regularization)</td>
<td>0.555211</td>
<td>0.412992</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>LSTM Neural Network</td>
<td>0.488574</td>
<td>0.302897</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>LSTM Neural Network (with L1L2 Regularization)</td>
<td>0.490416</td>
<td>0.314806</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The LSTM without regularization has shown the best performance on the training data, indicating good fitting capabilities. The RNN with regularization appears to have the best performance on the test data, indicating better generalization and predictive power.</p>
<p>Regularization improved the predictive power of the RNN model, as indicated by the decrease in test RMSE. For the GRU and LSTM models, the impact of regularization on the test RMSE is minimal, with a slight decrease for the GRU and a slight increase for the LSTM. This suggests that the benefit of regularization in these cases is less clear, and it may not be necessary or the parameters might need adjustment.</p>
</section>
</section>
<section id="deep-learning-for-daily-new-confirmed-deaths-in-the-us" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-for-daily-new-confirmed-deaths-in-the-us">Deep Learning for Daily New Confirmed Deaths in the US</h2>
<section id="recurrent-neural-network-1" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-network-1">Recurrent Neural Network</h3>
<p>Based on the RMSE values, the RNN with regularization performs better on both the training and test datasets, with the test RMSE decreasing from 0.533 to 0.495, which is a significant improvement in predictive accuracy.</p>
<p>In summary, regularization has improved the RNN model’s performance, evidenced by both the loss curves and the RMSE values. The smoother loss curves and reduced RMSE values suggest that the regularized model is more reliable when making predictions on new data.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">RNN</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">RNN including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RNN</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>,kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple neural network layer</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    model.add(SimpleRNN(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>],</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>              kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>,validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model'</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>yhat_d <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [y[<span class="dv">0</span>] <span class="cf">for</span> y <span class="kw">in</span> Yval]</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>rmse_table_2 <span class="op">=</span> {</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model'</span>: [<span class="st">'Recurrent Neural Network'</span>],</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'training_rmse'</span>: [train_rmse],</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'testing_rmse'</span>: [test_rmse]</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-16-output-1.png" width="571" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.586 RMSE
Test RMSE: 0.504 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network with regularization</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>), dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model (with L1L2 Regularization)'</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>yhat_d_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Recurrent Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-17-output-1.png" width="717" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.573 RMSE
Test RMSE: 0.488 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gated-recurrent-unit-1" class="level3">
<h3 class="anchored" data-anchor-id="gated-recurrent-unit-1">Gated Recurrent unit</h3>
<p>The GRU model with L1L2 regularization shows a better learning trend and reduced RMSE values, suggesting an improvement in model performance. However, the high variance in validation loss and the relatively small improvement in test RMSE indicate that there may still be issues to address, such as further hyperparameter tuning, gathering more training data, or experimenting with different model architectures.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">GRU</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">GRU including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple GRU layer</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'sgd'</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Training and evaluating a GRU-based model</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model'</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>yhat_gru <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network'</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-18-output-1.png" width="579" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.995 RMSE
Test RMSE: 0.935 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU-based model with regularization</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>],dropout_rate <span class="op">=</span> <span class="fl">0.2</span>,  kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model (with L1L2 Regularization)'</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>yhat_gru_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-19-output-1.png" width="590" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.767 RMSE
Test RMSE: 0.885 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="long-short-term-memory-1" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-1">Long Short Term Memory</h3>
<p>The LSTM network benefits from the use of L1L2 regularization, albeit modestly. The regularization helps to slightly improve the model’s prediction accuracy and stability, as evidenced by the reduced RMSE values on both the training and test datasets. The behavior of the loss curves suggests that the model with regularization might be more reliable when applied to new, unseen data, although the effect of regularization is not dramatically large in this case.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true">LSTM</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false">LSTM including Regularization</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM Neural Network</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple long short term memory neural network</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(hidden_units,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>], input_shape<span class="op">=</span>input_shape))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"RMSprop"</span>, loss<span class="op">=</span><span class="st">'mae'</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an LSTM neural network</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model'</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>yhat_lstm <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network'</span>)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-20-output-1.png" width="579" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.524 RMSE
Test RMSE: 0.480 RMSE</code></pre>
</div>
</div>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM neural network with regularization</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model (with L1L2 Regularization)'</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>yhat_lstm_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="Deep-Learning_files/figure-html/cell-21-output-1.png" width="593" height="431"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Train RMSE: 0.492 RMSE
Test RMSE: 0.485 RMSE</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="comparing-1" class="level3">
<h3 class="anchored" data-anchor-id="comparing-1">Comparing</h3>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">training_rmse</th>
<th data-quarto-table-cell-role="th">testing_rmse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Recurrent Neural Network</td>
<td>0.585985</td>
<td>0.504004</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Recurrent Neural Network (with L1L2 Regulariza...</td>
<td>0.573473</td>
<td>0.488104</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>GRU Neural Network</td>
<td>0.995457</td>
<td>0.935351</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>GRU Neural Network (with L1L2 Regularization)</td>
<td>0.766639</td>
<td>0.884788</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>LSTM Neural Network</td>
<td>0.523506</td>
<td>0.480279</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>LSTM Neural Network (with L1L2 Regularization)</td>
<td>0.492418</td>
<td>0.485291</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>LSTM shows the best performance both with and without regularization, having the lowest RMSE values across all models. This suggests that the LSTM model has the highest accuracy and predictive power among the three.</p>
<p>RNN benefits significantly from regularization, showing a good reduction in RMSE values when regularization is applied. This indicates an improvement in the model’s ability to generalize, although not to the extent of the LSTM model.</p>
<p>GRU shows the most significant decrease in training RMSE after applying regularization, but the improvement in the test RMSE is not as pronounced as with the RNN. Despite this improvement, the GRU’s performance is still behind both the LSTM and the regularized RNN in terms of test RMSE.</p>
<p>In conclusion, LSTM appears to be the most accurate and powerful model for the data and tasks presented, with the lowest test RMSE reflecting its strong predictive power. RNN and GRU benefit from regularization but do not reach the performance level of the LSTM.</p>
</section>
</section>
<section id="deep-learning-vs-traditional-ts-model" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-vs-traditional-ts-model">Deep Learning vs Traditional TS Model</h2>
<p>The ability of a deep learning model to accurately predict the future depends on a variety of factors, including the complexity of the problem, the quality and quantity of the data, the architecture of the model, and more. Models such as LSTM and GRU can remember long sequences of information, which theoretically allows them to further predict the future. However, the further out the forecast, the more likely it is to be affected by overfitting to past data or the compounding of small forecast errors, especially when we are predicting the number of daily new cases and deaths from COVID-19. When affecting complex data, it is possible that short-term forecasts (e.g., 30 days or less) will be relatively more reliable.</p>
<p>Deep learning models and traditional time-series models like ARMA (Autoregressive Moving Average) and ARIMA (Autoregressive Integrated Moving Average) are fundamentally different in their approaches and capabilities, and each has its advantages and disadvantages depending on the context of the problem.</p>
<ul>
<li><p>Deep learning models are well-suited for handling large datasets with complex patterns and multiple variables (multivariate time series). They can automatically detect and model nonlinear relationships in the data without the need for explicit feature engineering.</p></li>
<li><p>Traditional time-series models like ARMA or ARIMA are more data-efficient and can often produce good results with smaller datasets. They are typically used for univariate time series, although extensions like Vector ARIMA (VARIMA) exist for multivariate cases.</p></li>
</ul>
<p>Overall, Deep learning models might outperform ARMA/ARIMA when dealing with complex patterns and large multivariate time series. However, for simpler, smaller, or more stable univariate time series, ARMA/ARIMA models can be competitive or even superior.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb25" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Deep Learning for Time Series</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    smooth-scroll: true</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    embed-resources: false</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">    mermaid:</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">      theme: neutral</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">#bibliography: citation.bib</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: false</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>In this section, I try to create a model that can predict the number of daily new cases and deaths of COVID-19 by leveraging techniques such as RNN, LSTM, GRU, and regularization.</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="in">```{python setup}</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dropout</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, SimpleRNN, LSTM, GRU</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> regularizers</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="in">```{python load}</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_excel(<span class="st">'../data/owid_covid_data_us.xlsx'</span>)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'date'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'date'</span>])</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[(df[<span class="st">'date'</span>] <span class="op">&lt;</span> <span class="st">'2022-10-20'</span>)]</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'new_cases'</span>] <span class="op">=</span> df[<span class="st">'new_cases'</span>].astype(<span class="st">'float64'</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'new_deaths'</span>] <span class="op">=</span> df[<span class="st">'new_deaths'</span>].astype(<span class="st">'float64'</span>)</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'new_cases'</span>] <span class="op">=</span> pd.to_numeric(df[<span class="st">'new_cases'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'new_deaths'</span>] <span class="op">=</span> pd.to_numeric(df[<span class="st">'new_deaths'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset<span class="op">=</span>[<span class="st">'new_cases'</span>])</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset<span class="op">=</span>[<span class="st">'new_deaths'</span>])</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>df_1 <span class="op">=</span> df[[<span class="st">'date'</span>, <span class="st">'new_cases'</span>]]</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>df_2 <span class="op">=</span> df[[<span class="st">'date'</span>, <span class="st">'new_deaths'</span>]]</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>df_1.sort_values(<span class="st">'date'</span>, inplace<span class="op">=</span><span class="va">True</span>, ascending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>df_1 <span class="op">=</span> df_1.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>df_2.sort_values(<span class="st">'date'</span>, inplace<span class="op">=</span><span class="va">True</span>, ascending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>df_2 <span class="op">=</span> df_2.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a><span class="in">```{python create_X_Y}</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_X_Y(ts: np.array, lag<span class="op">=</span><span class="dv">1</span>, n_ahead<span class="op">=</span><span class="dv">1</span>, target_index<span class="op">=</span><span class="dv">0</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>:</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a><span class="co">    A method to create X and Y matrix from a time series array for the training of </span></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a><span class="co">    deep learning models </span></span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extracting the number of features that are passed from the array</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> ts.shape[<span class="dv">1</span>]</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creating placeholder lists</span></span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> [], []</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(ts) <span class="op">-</span> lag <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>        X.append(ts)</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(ts) <span class="op">-</span> lag <span class="op">-</span> n_ahead):</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>            Y.append(ts[(i <span class="op">+</span> lag):(i <span class="op">+</span> lag <span class="op">+</span> n_ahead), target_index])</span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>            X.append(ts[i:(i <span class="op">+</span> lag)])</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> np.array(X), np.array(Y)</span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshaping the X array to an RNN input shape</span></span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.reshape(X, (X.shape[<span class="dv">0</span>], lag, n_features))</span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, Y</span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{python plt_model}</span></span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_model(history, model_title):</span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> history.history[<span class="st">'loss'</span>]</span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> history.history[<span class="st">'val_loss'</span>]  <span class="co"># Get validation loss</span></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(loss) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, loss, <span class="st">'b'</span>, label<span class="op">=</span><span class="st">'Training loss'</span>, color<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, val_loss, <span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Validation loss'</span>, color<span class="op">=</span><span class="st">"orange"</span>)  <span class="co"># Plot validation loss</span></span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'</span><span class="sc">{</span>model_title<span class="sc">}</span><span class="ss"> Training and Validation loss'</span>)</span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb25-99"><a href="#cb25-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-100"><a href="#cb25-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python plt_error}</span></span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_error(trainY, testY, train_predict, test_predict):</span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Error of predictions</span></span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a>    train_rmse <span class="op">=</span> math.sqrt(mean_squared_error(</span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>        trainY[:, <span class="dv">0</span>], train_predict[:, <span class="dv">0</span>]))</span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>    test_rmse <span class="op">=</span> math.sqrt(mean_squared_error(testY[:, <span class="dv">0</span>], test_predict[:, <span class="dv">0</span>]))</span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print RMSE</span></span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Train RMSE: </span><span class="sc">%.3f</span><span class="st"> RMSE'</span> <span class="op">%</span> (train_rmse))</span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Test RMSE: </span><span class="sc">%.3f</span><span class="st"> RMSE'</span> <span class="op">%</span> (test_rmse))</span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_rmse, test_rmse</span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning for Daily New Confirmed Cases in the US</span></span>
<span id="cb25-114"><a href="#cb25-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-115"><a href="#cb25-115" aria-hidden="true" tabindex="-1"></a><span class="in">```{python prepare_1}</span></span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> df_1[[<span class="st">'new_cases'</span>]].values</span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12345</span>)</span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a>nrows <span class="op">=</span> ts.shape[<span class="dv">0</span>]</span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a>test_share <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a><span class="co"># Spliting into train and test sets</span></span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> ts[<span class="dv">0</span>:<span class="bu">int</span>(nrows <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> ts[<span class="bu">int</span>(nrows <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaling the data</span></span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a>train_mean <span class="op">=</span> train.mean()</span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>train_std <span class="op">=</span> train.std()</span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> (train <span class="op">-</span> train_mean) <span class="op">/</span> train_std</span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> (test <span class="op">-</span> train_mean) <span class="op">/</span> train_std</span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the final scaled frame</span></span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a>ts_s <span class="op">=</span> np.concatenate([train, test])</span>
<span id="cb25-132"><a href="#cb25-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-133"><a href="#cb25-133" aria-hidden="true" tabindex="-1"></a>lag <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb25-134"><a href="#cb25-134" aria-hidden="true" tabindex="-1"></a>ahead <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb25-135"><a href="#cb25-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-136"><a href="#cb25-136" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the X and Y for training</span></span>
<span id="cb25-137"><a href="#cb25-137" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> create_X_Y(ts_s, lag<span class="op">=</span>lag, n_ahead<span class="op">=</span>ahead)</span>
<span id="cb25-138"><a href="#cb25-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-139"><a href="#cb25-139" aria-hidden="true" tabindex="-1"></a>Xtrain <span class="op">=</span> X[<span class="dv">0</span>:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-140"><a href="#cb25-140" aria-hidden="true" tabindex="-1"></a>Ytrain <span class="op">=</span> Y[<span class="dv">0</span>:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-141"><a href="#cb25-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-142"><a href="#cb25-142" aria-hidden="true" tabindex="-1"></a>Xval <span class="op">=</span> X[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-143"><a href="#cb25-143" aria-hidden="true" tabindex="-1"></a>Yval <span class="op">=</span> Y[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-144"><a href="#cb25-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-145"><a href="#cb25-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-146"><a href="#cb25-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recurrent Neural Network</span></span>
<span id="cb25-147"><a href="#cb25-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-148"><a href="#cb25-148" aria-hidden="true" tabindex="-1"></a>The RMSE values for training are similar for both models, which suggests that regularization hasn't impacted the ability of the model to fit the training data much. However, the test RMSE is slightly better in the regularized model (0.301) compared to the non-regularized one (0.312). This improvement in test RMSE indicates that the regularized model generalizes slightly better to unseen data.</span>
<span id="cb25-149"><a href="#cb25-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-150"><a href="#cb25-150" aria-hidden="true" tabindex="-1"></a>The lower validation loss as compared to the training loss in both models is not typical and could suggest that the validation set is easier for the model to predict than the training set.</span>
<span id="cb25-151"><a href="#cb25-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-152"><a href="#cb25-152" aria-hidden="true" tabindex="-1"></a>Overall, the regularization seems to be serving its purpose by slightly improving the model's generalization, as indicated by the lower test RMSE. However, the difference is minimal, suggesting that the model was not significantly overfitting the training data even without regularization.</span>
<span id="cb25-153"><a href="#cb25-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-154"><a href="#cb25-154" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-155"><a href="#cb25-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-156"><a href="#cb25-156" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RNN</span></span>
<span id="cb25-157"><a href="#cb25-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-158"><a href="#cb25-158" aria-hidden="true" tabindex="-1"></a><span class="in">```{python RNN_1}</span></span>
<span id="cb25-159"><a href="#cb25-159" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-160"><a href="#cb25-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RNN</span></span>
<span id="cb25-161"><a href="#cb25-161" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>,kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-162"><a href="#cb25-162" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-163"><a href="#cb25-163" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple neural network layer</span></span>
<span id="cb25-164"><a href="#cb25-164" aria-hidden="true" tabindex="-1"></a>    model.add(SimpleRNN(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb25-165"><a href="#cb25-165" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb25-166"><a href="#cb25-166" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-167"><a href="#cb25-167" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-168"><a href="#cb25-168" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>],</span>
<span id="cb25-169"><a href="#cb25-169" aria-hidden="true" tabindex="-1"></a>              kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-170"><a href="#cb25-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-171"><a href="#cb25-171" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-172"><a href="#cb25-172" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-173"><a href="#cb25-173" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-174"><a href="#cb25-174" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>)</span>
<span id="cb25-175"><a href="#cb25-175" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-176"><a href="#cb25-176" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-177"><a href="#cb25-177" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network</span></span>
<span id="cb25-178"><a href="#cb25-178" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-179"><a href="#cb25-179" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-180"><a href="#cb25-180" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>,validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-181"><a href="#cb25-181" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model'</span>)</span>
<span id="cb25-182"><a href="#cb25-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-183"><a href="#cb25-183" aria-hidden="true" tabindex="-1"></a>yhat_d <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-184"><a href="#cb25-184" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [y[<span class="dv">0</span>] <span class="cf">for</span> y <span class="kw">in</span> Yval]</span>
<span id="cb25-185"><a href="#cb25-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-186"><a href="#cb25-186" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-187"><a href="#cb25-187" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-188"><a href="#cb25-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-189"><a href="#cb25-189" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-190"><a href="#cb25-190" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-191"><a href="#cb25-191" aria-hidden="true" tabindex="-1"></a>rmse_table <span class="op">=</span> {</span>
<span id="cb25-192"><a href="#cb25-192" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model'</span>: [<span class="st">'Recurrent Neural Network'</span>],</span>
<span id="cb25-193"><a href="#cb25-193" aria-hidden="true" tabindex="-1"></a>    <span class="st">'training_rmse'</span>: [train_rmse],</span>
<span id="cb25-194"><a href="#cb25-194" aria-hidden="true" tabindex="-1"></a>    <span class="st">'testing_rmse'</span>: [test_rmse]</span>
<span id="cb25-195"><a href="#cb25-195" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-196"><a href="#cb25-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-197"><a href="#cb25-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-198"><a href="#cb25-198" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RNN including Regularization</span></span>
<span id="cb25-199"><a href="#cb25-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-200"><a href="#cb25-200" aria-hidden="true" tabindex="-1"></a><span class="in">```{python RNN_2}</span></span>
<span id="cb25-201"><a href="#cb25-201" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-202"><a href="#cb25-202" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network with regularization</span></span>
<span id="cb25-203"><a href="#cb25-203" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-204"><a href="#cb25-204" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>), dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-205"><a href="#cb25-205" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-206"><a href="#cb25-206" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-207"><a href="#cb25-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-208"><a href="#cb25-208" aria-hidden="true" tabindex="-1"></a>yhat_d_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-209"><a href="#cb25-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-210"><a href="#cb25-210" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-211"><a href="#cb25-211" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-212"><a href="#cb25-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-213"><a href="#cb25-213" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-214"><a href="#cb25-214" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-215"><a href="#cb25-215" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(</span>
<span id="cb25-216"><a href="#cb25-216" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Recurrent Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-217"><a href="#cb25-217" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-218"><a href="#cb25-218" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-219"><a href="#cb25-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-220"><a href="#cb25-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-221"><a href="#cb25-221" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-222"><a href="#cb25-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-223"><a href="#cb25-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gated Recurrent unit</span></span>
<span id="cb25-224"><a href="#cb25-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-225"><a href="#cb25-225" aria-hidden="true" tabindex="-1"></a>The GRU model without regularization achieves a lower training RMSE (0.572) compared to the regularized model (1.004), which is consistent with expectations as regularization typically increases training error due to the added constraints. However, the test RMSE is nearly identical for both models, with the regularized model showing a marginally better test RMSE (0.403) compared to the non-regularized model (0.408).</span>
<span id="cb25-226"><a href="#cb25-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-227"><a href="#cb25-227" aria-hidden="true" tabindex="-1"></a>This marginal improvement in test RMSE for the regularized model suggests that regularization has helped to improve the generalization of the model to new data, although the effect is slight. This effect is not as clearly reflected in the validation loss, which is consistently low for both models. Again, the lower validation loss compared to the training loss is unusual and suggests potential issues with the validation process or data.</span>
<span id="cb25-228"><a href="#cb25-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-229"><a href="#cb25-229" aria-hidden="true" tabindex="-1"></a>In conclusion, while the regularized GRU model exhibits higher training loss due to the effects of L1L2 regularization, it achieves a slightly better generalization on the test data, as evidenced by the lower test RMSE.</span>
<span id="cb25-230"><a href="#cb25-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-231"><a href="#cb25-231" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-232"><a href="#cb25-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-233"><a href="#cb25-233" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GRU</span></span>
<span id="cb25-234"><a href="#cb25-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-235"><a href="#cb25-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{python GRU_1}</span></span>
<span id="cb25-236"><a href="#cb25-236" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-237"><a href="#cb25-237" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU</span></span>
<span id="cb25-238"><a href="#cb25-238" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-239"><a href="#cb25-239" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-240"><a href="#cb25-240" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple GRU layer</span></span>
<span id="cb25-241"><a href="#cb25-241" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb25-242"><a href="#cb25-242" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb25-243"><a href="#cb25-243" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-244"><a href="#cb25-244" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-245"><a href="#cb25-245" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-246"><a href="#cb25-246" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-247"><a href="#cb25-247" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-248"><a href="#cb25-248" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-249"><a href="#cb25-249" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'sgd'</span>)</span>
<span id="cb25-250"><a href="#cb25-250" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-251"><a href="#cb25-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-252"><a href="#cb25-252" aria-hidden="true" tabindex="-1"></a><span class="co"># Training and evaluating a GRU-based model</span></span>
<span id="cb25-253"><a href="#cb25-253" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-254"><a href="#cb25-254" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-255"><a href="#cb25-255" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-256"><a href="#cb25-256" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model'</span>)</span>
<span id="cb25-257"><a href="#cb25-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-258"><a href="#cb25-258" aria-hidden="true" tabindex="-1"></a>yhat_gru <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-259"><a href="#cb25-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-260"><a href="#cb25-260" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-261"><a href="#cb25-261" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-262"><a href="#cb25-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-263"><a href="#cb25-263" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-264"><a href="#cb25-264" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-265"><a href="#cb25-265" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network'</span>)</span>
<span id="cb25-266"><a href="#cb25-266" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-267"><a href="#cb25-267" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-268"><a href="#cb25-268" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-269"><a href="#cb25-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-270"><a href="#cb25-270" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GRU including Regularization</span></span>
<span id="cb25-271"><a href="#cb25-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-272"><a href="#cb25-272" aria-hidden="true" tabindex="-1"></a><span class="in">```{python GRU_2}</span></span>
<span id="cb25-273"><a href="#cb25-273" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-274"><a href="#cb25-274" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU-based model with regularization</span></span>
<span id="cb25-275"><a href="#cb25-275" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-276"><a href="#cb25-276" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>],dropout_rate <span class="op">=</span> <span class="fl">0.2</span>,  kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb25-277"><a href="#cb25-277" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-278"><a href="#cb25-278" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-279"><a href="#cb25-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-280"><a href="#cb25-280" aria-hidden="true" tabindex="-1"></a>yhat_gru_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-281"><a href="#cb25-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-282"><a href="#cb25-282" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-283"><a href="#cb25-283" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-284"><a href="#cb25-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-285"><a href="#cb25-285" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-286"><a href="#cb25-286" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-287"><a href="#cb25-287" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-288"><a href="#cb25-288" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-289"><a href="#cb25-289" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-290"><a href="#cb25-290" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-291"><a href="#cb25-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-292"><a href="#cb25-292" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-293"><a href="#cb25-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-294"><a href="#cb25-294" aria-hidden="true" tabindex="-1"></a><span class="fu">### Long Short Term Memory</span></span>
<span id="cb25-295"><a href="#cb25-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-296"><a href="#cb25-296" aria-hidden="true" tabindex="-1"></a>With the LSTM models, we see that regularization has led to a slight increase in both the training and test RMSE. The training RMSE has increased from 0.481 to 0.513, which could be due to the regularization term adding a penalty to the cost function and therefore making it harder for the model to fit the training data perfectly. This is a typical effect of regularization, as it trades off variance for bias, leading to a less complex model that might not capture all the nuances in the training data.</span>
<span id="cb25-297"><a href="#cb25-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-298"><a href="#cb25-298" aria-hidden="true" tabindex="-1"></a>However, contrary to the usual benefits of regularization, the test RMSE has also increased, albeit slightly, from 0.302 to 0.309. This suggests that for this particular dataset and model architecture, the regularization has not provided a benefit in terms of generalization to the test data. It's possible that the non-regularized model was not overfitting to begin with, so the regularization did not lead to a performance improvement on the test set. Alternatively, the type and amount of regularization might not be optimal for this problem, and tweaking the regularization parameters could potentially yield different results.</span>
<span id="cb25-299"><a href="#cb25-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-300"><a href="#cb25-300" aria-hidden="true" tabindex="-1"></a>It's important to note that while the training and validation losses provide a good indication of the model's learning process, the ultimate measure of performance is the test RMSE, which tells us how well the model is expected to perform on unseen data. The slight increase in test RMSE for the regularized model indicates that in this case, regularization may not be beneficial.</span>
<span id="cb25-301"><a href="#cb25-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-302"><a href="#cb25-302" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-303"><a href="#cb25-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-304"><a href="#cb25-304" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LSTM</span></span>
<span id="cb25-305"><a href="#cb25-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-306"><a href="#cb25-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python LSTM_1}</span></span>
<span id="cb25-307"><a href="#cb25-307" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-308"><a href="#cb25-308" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM Neural Network</span></span>
<span id="cb25-309"><a href="#cb25-309" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-310"><a href="#cb25-310" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-311"><a href="#cb25-311" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple long short term memory neural network</span></span>
<span id="cb25-312"><a href="#cb25-312" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(hidden_units,</span>
<span id="cb25-313"><a href="#cb25-313" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>], input_shape<span class="op">=</span>input_shape))</span>
<span id="cb25-314"><a href="#cb25-314" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-315"><a href="#cb25-315" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-316"><a href="#cb25-316" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-317"><a href="#cb25-317" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-318"><a href="#cb25-318" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-319"><a href="#cb25-319" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-320"><a href="#cb25-320" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"RMSprop"</span>, loss<span class="op">=</span><span class="st">'mae'</span>)</span>
<span id="cb25-321"><a href="#cb25-321" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-322"><a href="#cb25-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-323"><a href="#cb25-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an LSTM neural network</span></span>
<span id="cb25-324"><a href="#cb25-324" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-325"><a href="#cb25-325" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-326"><a href="#cb25-326" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-327"><a href="#cb25-327" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model'</span>)</span>
<span id="cb25-328"><a href="#cb25-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-329"><a href="#cb25-329" aria-hidden="true" tabindex="-1"></a>yhat_lstm <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-330"><a href="#cb25-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-331"><a href="#cb25-331" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-332"><a href="#cb25-332" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-333"><a href="#cb25-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-334"><a href="#cb25-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-335"><a href="#cb25-335" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-336"><a href="#cb25-336" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network'</span>)</span>
<span id="cb25-337"><a href="#cb25-337" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-338"><a href="#cb25-338" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-339"><a href="#cb25-339" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-340"><a href="#cb25-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-341"><a href="#cb25-341" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LSTM including Regularization</span></span>
<span id="cb25-342"><a href="#cb25-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-343"><a href="#cb25-343" aria-hidden="true" tabindex="-1"></a><span class="in">```{python LSTM_2}</span></span>
<span id="cb25-344"><a href="#cb25-344" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-345"><a href="#cb25-345" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM neural network with regularization</span></span>
<span id="cb25-346"><a href="#cb25-346" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-347"><a href="#cb25-347" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb25-348"><a href="#cb25-348" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-349"><a href="#cb25-349" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-350"><a href="#cb25-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-351"><a href="#cb25-351" aria-hidden="true" tabindex="-1"></a>yhat_lstm_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-352"><a href="#cb25-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-353"><a href="#cb25-353" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-354"><a href="#cb25-354" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-355"><a href="#cb25-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-356"><a href="#cb25-356" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-357"><a href="#cb25-357" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-358"><a href="#cb25-358" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-359"><a href="#cb25-359" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-360"><a href="#cb25-360" aria-hidden="true" tabindex="-1"></a>rmse_table[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-361"><a href="#cb25-361" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-362"><a href="#cb25-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-363"><a href="#cb25-363" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-364"><a href="#cb25-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-365"><a href="#cb25-365" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing</span></span>
<span id="cb25-366"><a href="#cb25-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-367"><a href="#cb25-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{python compare_1}</span></span>
<span id="cb25-368"><a href="#cb25-368" aria-hidden="true" tabindex="-1"></a>rmse_df <span class="op">=</span> pd.DataFrame(rmse_table)</span>
<span id="cb25-369"><a href="#cb25-369" aria-hidden="true" tabindex="-1"></a>rmse_df</span>
<span id="cb25-370"><a href="#cb25-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-371"><a href="#cb25-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-372"><a href="#cb25-372" aria-hidden="true" tabindex="-1"></a>The LSTM without regularization has shown the best performance on the training data, indicating good fitting capabilities. The RNN with regularization appears to have the best performance on the test data, indicating better generalization and predictive power. </span>
<span id="cb25-373"><a href="#cb25-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-374"><a href="#cb25-374" aria-hidden="true" tabindex="-1"></a>Regularization improved the predictive power of the RNN model, as indicated by the decrease in test RMSE. For the GRU and LSTM models, the impact of regularization on the test RMSE is minimal, with a slight decrease for the GRU and a slight increase for the LSTM. This suggests that the benefit of regularization in these cases is less clear, and it may not be necessary or the parameters might need adjustment.</span>
<span id="cb25-375"><a href="#cb25-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-376"><a href="#cb25-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning for Daily New Confirmed Deaths in the US</span></span>
<span id="cb25-377"><a href="#cb25-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-378"><a href="#cb25-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{python prepare_2}</span></span>
<span id="cb25-379"><a href="#cb25-379" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> df_2[[<span class="st">'new_deaths'</span>]].values</span>
<span id="cb25-380"><a href="#cb25-380" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12345</span>)</span>
<span id="cb25-381"><a href="#cb25-381" aria-hidden="true" tabindex="-1"></a>nrows <span class="op">=</span> ts.shape[<span class="dv">0</span>]</span>
<span id="cb25-382"><a href="#cb25-382" aria-hidden="true" tabindex="-1"></a>test_share <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb25-383"><a href="#cb25-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Spliting into train and test sets</span></span>
<span id="cb25-384"><a href="#cb25-384" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> ts[<span class="dv">0</span>:<span class="bu">int</span>(nrows <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-385"><a href="#cb25-385" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> ts[<span class="bu">int</span>(nrows <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-386"><a href="#cb25-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-387"><a href="#cb25-387" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaling the data</span></span>
<span id="cb25-388"><a href="#cb25-388" aria-hidden="true" tabindex="-1"></a>train_mean <span class="op">=</span> train.mean()</span>
<span id="cb25-389"><a href="#cb25-389" aria-hidden="true" tabindex="-1"></a>train_std <span class="op">=</span> train.std()</span>
<span id="cb25-390"><a href="#cb25-390" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> (train <span class="op">-</span> train_mean) <span class="op">/</span> train_std</span>
<span id="cb25-391"><a href="#cb25-391" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> (test <span class="op">-</span> train_mean) <span class="op">/</span> train_std</span>
<span id="cb25-392"><a href="#cb25-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-393"><a href="#cb25-393" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the final scaled frame</span></span>
<span id="cb25-394"><a href="#cb25-394" aria-hidden="true" tabindex="-1"></a>ts_s <span class="op">=</span> np.concatenate([train, test])</span>
<span id="cb25-395"><a href="#cb25-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-396"><a href="#cb25-396" aria-hidden="true" tabindex="-1"></a>lag <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb25-397"><a href="#cb25-397" aria-hidden="true" tabindex="-1"></a>ahead <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb25-398"><a href="#cb25-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-399"><a href="#cb25-399" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the X and Y for training</span></span>
<span id="cb25-400"><a href="#cb25-400" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> create_X_Y(ts_s, lag<span class="op">=</span>lag, n_ahead<span class="op">=</span>ahead)</span>
<span id="cb25-401"><a href="#cb25-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-402"><a href="#cb25-402" aria-hidden="true" tabindex="-1"></a>Xtrain <span class="op">=</span> X[<span class="dv">0</span>:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-403"><a href="#cb25-403" aria-hidden="true" tabindex="-1"></a>Ytrain <span class="op">=</span> Y[<span class="dv">0</span>:<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share))]</span>
<span id="cb25-404"><a href="#cb25-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-405"><a href="#cb25-405" aria-hidden="true" tabindex="-1"></a>Xval <span class="op">=</span> X[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-406"><a href="#cb25-406" aria-hidden="true" tabindex="-1"></a>Yval <span class="op">=</span> Y[<span class="bu">int</span>(X.shape[<span class="dv">0</span>] <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> test_share)):]</span>
<span id="cb25-407"><a href="#cb25-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-408"><a href="#cb25-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-409"><a href="#cb25-409" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recurrent Neural Network</span></span>
<span id="cb25-410"><a href="#cb25-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-411"><a href="#cb25-411" aria-hidden="true" tabindex="-1"></a>Based on the RMSE values, the RNN with regularization performs better on both the training and test datasets, with the test RMSE decreasing from 0.533 to 0.495, which is a significant improvement in predictive accuracy.</span>
<span id="cb25-412"><a href="#cb25-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-413"><a href="#cb25-413" aria-hidden="true" tabindex="-1"></a>In summary, regularization has improved the RNN model's performance, evidenced by both the loss curves and the RMSE values. The smoother loss curves and reduced RMSE values suggest that the regularized model is more reliable when making predictions on new data.</span>
<span id="cb25-414"><a href="#cb25-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-415"><a href="#cb25-415" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-416"><a href="#cb25-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-417"><a href="#cb25-417" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RNN</span></span>
<span id="cb25-418"><a href="#cb25-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-419"><a href="#cb25-419" aria-hidden="true" tabindex="-1"></a><span class="in">```{python RNN_3}</span></span>
<span id="cb25-420"><a href="#cb25-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-421"><a href="#cb25-421" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a RNN</span></span>
<span id="cb25-422"><a href="#cb25-422" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>,kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-423"><a href="#cb25-423" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-424"><a href="#cb25-424" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple neural network layer</span></span>
<span id="cb25-425"><a href="#cb25-425" aria-hidden="true" tabindex="-1"></a>    model.add(SimpleRNN(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb25-426"><a href="#cb25-426" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb25-427"><a href="#cb25-427" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-428"><a href="#cb25-428" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-429"><a href="#cb25-429" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>],</span>
<span id="cb25-430"><a href="#cb25-430" aria-hidden="true" tabindex="-1"></a>              kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-431"><a href="#cb25-431" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-432"><a href="#cb25-432" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-433"><a href="#cb25-433" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-434"><a href="#cb25-434" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-435"><a href="#cb25-435" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>)</span>
<span id="cb25-436"><a href="#cb25-436" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-437"><a href="#cb25-437" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb25-438"><a href="#cb25-438" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network</span></span>
<span id="cb25-439"><a href="#cb25-439" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-440"><a href="#cb25-440" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-441"><a href="#cb25-441" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>,validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-442"><a href="#cb25-442" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model'</span>)</span>
<span id="cb25-443"><a href="#cb25-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-444"><a href="#cb25-444" aria-hidden="true" tabindex="-1"></a>yhat_d <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-445"><a href="#cb25-445" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [y[<span class="dv">0</span>] <span class="cf">for</span> y <span class="kw">in</span> Yval]</span>
<span id="cb25-446"><a href="#cb25-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-447"><a href="#cb25-447" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-448"><a href="#cb25-448" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-449"><a href="#cb25-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-450"><a href="#cb25-450" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-451"><a href="#cb25-451" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-452"><a href="#cb25-452" aria-hidden="true" tabindex="-1"></a>rmse_table_2 <span class="op">=</span> {</span>
<span id="cb25-453"><a href="#cb25-453" aria-hidden="true" tabindex="-1"></a>    <span class="st">'model'</span>: [<span class="st">'Recurrent Neural Network'</span>],</span>
<span id="cb25-454"><a href="#cb25-454" aria-hidden="true" tabindex="-1"></a>    <span class="st">'training_rmse'</span>: [train_rmse],</span>
<span id="cb25-455"><a href="#cb25-455" aria-hidden="true" tabindex="-1"></a>    <span class="st">'testing_rmse'</span>: [test_rmse]</span>
<span id="cb25-456"><a href="#cb25-456" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-457"><a href="#cb25-457" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-458"><a href="#cb25-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-459"><a href="#cb25-459" aria-hidden="true" tabindex="-1"></a><span class="fu">#### RNN including Regularization</span></span>
<span id="cb25-460"><a href="#cb25-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-461"><a href="#cb25-461" aria-hidden="true" tabindex="-1"></a><span class="in">```{python RNN_4}</span></span>
<span id="cb25-462"><a href="#cb25-462" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-463"><a href="#cb25-463" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a recurrent neural network with regularization</span></span>
<span id="cb25-464"><a href="#cb25-464" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_RNN(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-465"><a href="#cb25-465" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'tanh'</span>], kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>), dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-466"><a href="#cb25-466" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-467"><a href="#cb25-467" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'Recurrent Neural Network Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-468"><a href="#cb25-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-469"><a href="#cb25-469" aria-hidden="true" tabindex="-1"></a>yhat_d_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-470"><a href="#cb25-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-471"><a href="#cb25-471" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-472"><a href="#cb25-472" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-473"><a href="#cb25-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-474"><a href="#cb25-474" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-475"><a href="#cb25-475" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-476"><a href="#cb25-476" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(</span>
<span id="cb25-477"><a href="#cb25-477" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Recurrent Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-478"><a href="#cb25-478" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-479"><a href="#cb25-479" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-480"><a href="#cb25-480" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-481"><a href="#cb25-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-482"><a href="#cb25-482" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-483"><a href="#cb25-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-484"><a href="#cb25-484" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gated Recurrent unit</span></span>
<span id="cb25-485"><a href="#cb25-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-486"><a href="#cb25-486" aria-hidden="true" tabindex="-1"></a>The GRU model with L1L2 regularization shows a better learning trend and reduced RMSE values, suggesting an improvement in model performance. However, the high variance in validation loss and the relatively small improvement in test RMSE indicate that there may still be issues to address, such as further hyperparameter tuning, gathering more training data, or experimenting with different model architectures.</span>
<span id="cb25-487"><a href="#cb25-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-488"><a href="#cb25-488" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-489"><a href="#cb25-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-490"><a href="#cb25-490" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GRU</span></span>
<span id="cb25-491"><a href="#cb25-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-492"><a href="#cb25-492" aria-hidden="true" tabindex="-1"></a><span class="in">```{python GRU_3}</span></span>
<span id="cb25-493"><a href="#cb25-493" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-494"><a href="#cb25-494" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU</span></span>
<span id="cb25-495"><a href="#cb25-495" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate<span class="op">=</span><span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-496"><a href="#cb25-496" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-497"><a href="#cb25-497" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple GRU layer</span></span>
<span id="cb25-498"><a href="#cb25-498" aria-hidden="true" tabindex="-1"></a>    model.add(GRU(hidden_units, input_shape<span class="op">=</span>input_shape,</span>
<span id="cb25-499"><a href="#cb25-499" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>]))</span>
<span id="cb25-500"><a href="#cb25-500" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-501"><a href="#cb25-501" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-502"><a href="#cb25-502" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-503"><a href="#cb25-503" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-504"><a href="#cb25-504" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-505"><a href="#cb25-505" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-506"><a href="#cb25-506" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'mean_squared_error'</span>, optimizer<span class="op">=</span><span class="st">'sgd'</span>)</span>
<span id="cb25-507"><a href="#cb25-507" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-508"><a href="#cb25-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-509"><a href="#cb25-509" aria-hidden="true" tabindex="-1"></a><span class="co"># Training and evaluating a GRU-based model</span></span>
<span id="cb25-510"><a href="#cb25-510" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-511"><a href="#cb25-511" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-512"><a href="#cb25-512" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-513"><a href="#cb25-513" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model'</span>)</span>
<span id="cb25-514"><a href="#cb25-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-515"><a href="#cb25-515" aria-hidden="true" tabindex="-1"></a>yhat_gru <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-516"><a href="#cb25-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-517"><a href="#cb25-517" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-518"><a href="#cb25-518" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-519"><a href="#cb25-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-520"><a href="#cb25-520" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-521"><a href="#cb25-521" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-522"><a href="#cb25-522" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network'</span>)</span>
<span id="cb25-523"><a href="#cb25-523" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-524"><a href="#cb25-524" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-525"><a href="#cb25-525" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-526"><a href="#cb25-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-527"><a href="#cb25-527" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GRU including Regularization</span></span>
<span id="cb25-528"><a href="#cb25-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-529"><a href="#cb25-529" aria-hidden="true" tabindex="-1"></a><span class="in">```{python GRU_4}</span></span>
<span id="cb25-530"><a href="#cb25-530" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-531"><a href="#cb25-531" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a GRU-based model with regularization</span></span>
<span id="cb25-532"><a href="#cb25-532" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_GRU(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-533"><a href="#cb25-533" aria-hidden="true" tabindex="-1"></a>                   activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'relu'</span>],dropout_rate <span class="op">=</span> <span class="fl">0.2</span>,  kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb25-534"><a href="#cb25-534" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-535"><a href="#cb25-535" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'GRU Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-536"><a href="#cb25-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-537"><a href="#cb25-537" aria-hidden="true" tabindex="-1"></a>yhat_gru_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-538"><a href="#cb25-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-539"><a href="#cb25-539" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-540"><a href="#cb25-540" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-541"><a href="#cb25-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-542"><a href="#cb25-542" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-543"><a href="#cb25-543" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-544"><a href="#cb25-544" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'GRU Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-545"><a href="#cb25-545" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-546"><a href="#cb25-546" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-547"><a href="#cb25-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-548"><a href="#cb25-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-549"><a href="#cb25-549" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-550"><a href="#cb25-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-551"><a href="#cb25-551" aria-hidden="true" tabindex="-1"></a><span class="fu">### Long Short Term Memory</span></span>
<span id="cb25-552"><a href="#cb25-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-553"><a href="#cb25-553" aria-hidden="true" tabindex="-1"></a>The LSTM network benefits from the use of L1L2 regularization, albeit modestly. The regularization helps to slightly improve the model's prediction accuracy and stability, as evidenced by the reduced RMSE values on both the training and test datasets. The behavior of the loss curves suggests that the model with regularization might be more reliable when applied to new, unseen data, although the effect of regularization is not dramatically large in this case.</span>
<span id="cb25-554"><a href="#cb25-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-555"><a href="#cb25-555" aria-hidden="true" tabindex="-1"></a>::: panel-tabset</span>
<span id="cb25-556"><a href="#cb25-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-557"><a href="#cb25-557" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LSTM</span></span>
<span id="cb25-558"><a href="#cb25-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-559"><a href="#cb25-559" aria-hidden="true" tabindex="-1"></a><span class="in">```{python LSTM_3}</span></span>
<span id="cb25-560"><a href="#cb25-560" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-561"><a href="#cb25-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM Neural Network</span></span>
<span id="cb25-562"><a href="#cb25-562" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-563"><a href="#cb25-563" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential()</span>
<span id="cb25-564"><a href="#cb25-564" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a simple long short term memory neural network</span></span>
<span id="cb25-565"><a href="#cb25-565" aria-hidden="true" tabindex="-1"></a>    model.add(LSTM(hidden_units,</span>
<span id="cb25-566"><a href="#cb25-566" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">0</span>], input_shape<span class="op">=</span>input_shape))</span>
<span id="cb25-567"><a href="#cb25-567" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a dense layer (only one, more layers would make it a deep neural net)</span></span>
<span id="cb25-568"><a href="#cb25-568" aria-hidden="true" tabindex="-1"></a>    model.add(Dense(units<span class="op">=</span>dense_units,</span>
<span id="cb25-569"><a href="#cb25-569" aria-hidden="true" tabindex="-1"></a>              activation<span class="op">=</span>activation[<span class="dv">1</span>], kernel_regularizer<span class="op">=</span>kernel_regularizer))</span>
<span id="cb25-570"><a href="#cb25-570" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add layer dropout</span></span>
<span id="cb25-571"><a href="#cb25-571" aria-hidden="true" tabindex="-1"></a>    model.add(Dropout(dropout_rate))</span>
<span id="cb25-572"><a href="#cb25-572" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the model and optimize on mean squared error</span></span>
<span id="cb25-573"><a href="#cb25-573" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"RMSprop"</span>, loss<span class="op">=</span><span class="st">'mae'</span>)</span>
<span id="cb25-574"><a href="#cb25-574" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-575"><a href="#cb25-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-576"><a href="#cb25-576" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an LSTM neural network</span></span>
<span id="cb25-577"><a href="#cb25-577" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-578"><a href="#cb25-578" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb25-579"><a href="#cb25-579" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-580"><a href="#cb25-580" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model'</span>)</span>
<span id="cb25-581"><a href="#cb25-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-582"><a href="#cb25-582" aria-hidden="true" tabindex="-1"></a>yhat_lstm <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-583"><a href="#cb25-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-584"><a href="#cb25-584" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-585"><a href="#cb25-585" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-586"><a href="#cb25-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-587"><a href="#cb25-587" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-588"><a href="#cb25-588" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-589"><a href="#cb25-589" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network'</span>)</span>
<span id="cb25-590"><a href="#cb25-590" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-591"><a href="#cb25-591" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-592"><a href="#cb25-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-593"><a href="#cb25-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-594"><a href="#cb25-594" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LSTM including Regularization</span></span>
<span id="cb25-595"><a href="#cb25-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-596"><a href="#cb25-596" aria-hidden="true" tabindex="-1"></a><span class="in">```{python LSTM_4}</span></span>
<span id="cb25-597"><a href="#cb25-597" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb25-598"><a href="#cb25-598" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LSTM neural network with regularization</span></span>
<span id="cb25-599"><a href="#cb25-599" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_LSTM(hidden_units<span class="op">=</span><span class="dv">5</span>, dense_units<span class="op">=</span><span class="dv">1</span>, input_shape<span class="op">=</span>(lag, Xtrain.shape[<span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb25-600"><a href="#cb25-600" aria-hidden="true" tabindex="-1"></a>                    activation<span class="op">=</span>[<span class="st">'tanh'</span>, <span class="st">'linear'</span>], dropout_rate <span class="op">=</span> <span class="fl">0.2</span>, kernel_regularizer<span class="op">=</span>regularizers.L1L2(l1<span class="op">=</span><span class="fl">1e-5</span>, l2<span class="op">=</span><span class="fl">1e-4</span>))</span>
<span id="cb25-601"><a href="#cb25-601" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(Xtrain, Ytrain, epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>, validation_data<span class="op">=</span>(Xval, Yval))</span>
<span id="cb25-602"><a href="#cb25-602" aria-hidden="true" tabindex="-1"></a>plot_model(history, <span class="st">'LSTM Model (with L1L2 Regularization)'</span>)</span>
<span id="cb25-603"><a href="#cb25-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-604"><a href="#cb25-604" aria-hidden="true" tabindex="-1"></a>yhat_lstm_reg <span class="op">=</span> [x[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)]</span>
<span id="cb25-605"><a href="#cb25-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-606"><a href="#cb25-606" aria-hidden="true" tabindex="-1"></a>train_predict <span class="op">=</span> model.predict(Xtrain, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-607"><a href="#cb25-607" aria-hidden="true" tabindex="-1"></a>test_predict <span class="op">=</span> model.predict(Xval, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-608"><a href="#cb25-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-609"><a href="#cb25-609" aria-hidden="true" tabindex="-1"></a><span class="co"># Print error</span></span>
<span id="cb25-610"><a href="#cb25-610" aria-hidden="true" tabindex="-1"></a>train_rmse, test_rmse <span class="op">=</span> print_error(Ytrain, Yval, train_predict, test_predict)</span>
<span id="cb25-611"><a href="#cb25-611" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'model'</span>].append(<span class="st">'LSTM Neural Network (with L1L2 Regularization)'</span>)</span>
<span id="cb25-612"><a href="#cb25-612" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'training_rmse'</span>].append(train_rmse)</span>
<span id="cb25-613"><a href="#cb25-613" aria-hidden="true" tabindex="-1"></a>rmse_table_2[<span class="st">'testing_rmse'</span>].append(test_rmse)</span>
<span id="cb25-614"><a href="#cb25-614" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-615"><a href="#cb25-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-616"><a href="#cb25-616" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb25-617"><a href="#cb25-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-618"><a href="#cb25-618" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing</span></span>
<span id="cb25-619"><a href="#cb25-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-620"><a href="#cb25-620" aria-hidden="true" tabindex="-1"></a><span class="in">```{python compare_1}</span></span>
<span id="cb25-621"><a href="#cb25-621" aria-hidden="true" tabindex="-1"></a>rmse_df <span class="op">=</span> pd.DataFrame(rmse_table_2)</span>
<span id="cb25-622"><a href="#cb25-622" aria-hidden="true" tabindex="-1"></a>rmse_df</span>
<span id="cb25-623"><a href="#cb25-623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-624"><a href="#cb25-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-625"><a href="#cb25-625" aria-hidden="true" tabindex="-1"></a>LSTM shows the best performance both with and without regularization, having the lowest RMSE values across all models. This suggests that the LSTM model has the highest accuracy and predictive power among the three.</span>
<span id="cb25-626"><a href="#cb25-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-627"><a href="#cb25-627" aria-hidden="true" tabindex="-1"></a>RNN benefits significantly from regularization, showing a good reduction in RMSE values when regularization is applied. This indicates an improvement in the model's ability to generalize, although not to the extent of the LSTM model.</span>
<span id="cb25-628"><a href="#cb25-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-629"><a href="#cb25-629" aria-hidden="true" tabindex="-1"></a>GRU shows the most significant decrease in training RMSE after applying regularization, but the improvement in the test RMSE is not as pronounced as with the RNN. Despite this improvement, the GRU's performance is still behind both the LSTM and the regularized RNN in terms of test RMSE.</span>
<span id="cb25-630"><a href="#cb25-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-631"><a href="#cb25-631" aria-hidden="true" tabindex="-1"></a>In conclusion, LSTM appears to be the most accurate and powerful model for the data and tasks presented, with the lowest test RMSE reflecting its strong predictive power. RNN and GRU benefit from regularization but do not reach the performance level of the LSTM.</span>
<span id="cb25-632"><a href="#cb25-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-633"><a href="#cb25-633" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning vs Traditional TS Model</span></span>
<span id="cb25-634"><a href="#cb25-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-635"><a href="#cb25-635" aria-hidden="true" tabindex="-1"></a>The ability of a deep learning model to accurately predict the future depends on a variety of factors, including the complexity of the problem, the quality and quantity of the data, the architecture of the model, and more. Models such as LSTM and GRU can remember long sequences of information, which theoretically allows them to further predict the future. However, the further out the forecast, the more likely it is to be affected by overfitting to past data or the compounding of small forecast errors, especially when we are predicting the number of daily new cases and deaths from COVID-19. When affecting complex data, it is possible that short-term forecasts (e.g., 30 days or less) will be relatively more reliable.</span>
<span id="cb25-636"><a href="#cb25-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-637"><a href="#cb25-637" aria-hidden="true" tabindex="-1"></a>Deep learning models and traditional time-series models like ARMA (Autoregressive Moving Average) and ARIMA (Autoregressive Integrated Moving Average) are fundamentally different in their approaches and capabilities, and each has its advantages and disadvantages depending on the context of the problem.</span>
<span id="cb25-638"><a href="#cb25-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-639"><a href="#cb25-639" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deep learning models are well-suited for handling large datasets with complex patterns and multiple variables (multivariate time series). They can automatically detect and model nonlinear relationships in the data without the need for explicit feature engineering.</span>
<span id="cb25-640"><a href="#cb25-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-641"><a href="#cb25-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Traditional time-series models like ARMA or ARIMA are more data-efficient and can often produce good results with smaller datasets. They are typically used for univariate time series, although extensions like Vector ARIMA (VARIMA) exist for multivariate cases.</span>
<span id="cb25-642"><a href="#cb25-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-643"><a href="#cb25-643" aria-hidden="true" tabindex="-1"></a>Overall, Deep learning models might outperform ARMA/ARIMA when dealing with complex patterns and large multivariate time series. However, for simpler, smaller, or more stable univariate time series, ARMA/ARIMA models can be competitive or even superior.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>